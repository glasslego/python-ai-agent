# flake8: noqa E501

import os

import numpy as np
from dotenv import load_dotenv
from langchain.prompts import PromptTemplate
from langchain_openai import ChatOpenAI
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity

# í™˜ê²½ë³€ìˆ˜ ë¡œë“œ
load_dotenv()


def rag_with_metadata():
    """
    ë©”íƒ€ë°ì´í„°ë¥¼ í¬í•¨í•œ RAG ì˜ˆì œ
    """

    # ë©”íƒ€ë°ì´í„°ê°€ í¬í•¨ëœ ë¬¸ì„œ
    documents_with_metadata = [
        {
            "content": "íŒŒì´ì¬ 3.12ê°€ 2023ë…„ 10ì›”ì— ë¦´ë¦¬ì¦ˆë˜ì—ˆìŠµë‹ˆë‹¤. ì„±ëŠ¥ì´ í¬ê²Œ í–¥ìƒë˜ì—ˆìŠµë‹ˆë‹¤.",
            "source": "Python.org",
            "date": "2023-10-02",
            "category": "Release Notes",
        },
        {
            "content": "Django 5.0ì´ 2023ë…„ 12ì›”ì— ì¶œì‹œë˜ì—ˆìŠµë‹ˆë‹¤. ë§ì€ ìƒˆë¡œìš´ ê¸°ëŠ¥ì´ ì¶”ê°€ë˜ì—ˆìŠµë‹ˆë‹¤.",
            "source": "Django Docs",
            "date": "2023-12-04",
            "category": "Framework",
        },
        {
            "content": "ChatGPTê°€ 2022ë…„ 11ì›”ì— ê³µê°œë˜ì–´ AI í˜ëª…ì„ ì¼ìœ¼ì¼°ìŠµë‹ˆë‹¤.",
            "source": "OpenAI Blog",
            "date": "2022-11-30",
            "category": "AI",
        },
        {
            "content": "FastAPIëŠ” í˜„ëŒ€ì ì´ê³  ë¹ ë¥¸ ì›¹ API í”„ë ˆì„ì›Œí¬ì…ë‹ˆë‹¤. íƒ€ì… íŒíŠ¸ë¥¼ í™œìš©í•©ë‹ˆë‹¤.",
            "source": "FastAPI Docs",
            "date": "2023-08-15",
            "category": "Framework",
        },
        {
            "content": "NumPy 2.0ì´ 2024ë…„ì— ì¶œì‹œ ì˜ˆì •ì…ë‹ˆë‹¤. ì£¼ìš” ì„±ëŠ¥ ê°œì„ ì´ í¬í•¨ë©ë‹ˆë‹¤.",
            "source": "NumPy News",
            "date": "2023-11-20",
            "category": "Library",
        },
    ]

    # ì¹´í…Œê³ ë¦¬ë³„ í•„í„°ë§ í•¨ìˆ˜
    def filter_by_category(docs, category):
        return [doc for doc in docs if doc["category"] == category]

    # ë‚ ì§œë³„ ì •ë ¬ í•¨ìˆ˜
    def sort_by_date(docs):
        return sorted(docs, key=lambda x: x["date"], reverse=True)

    # ë‚ ì§œ ë²”ìœ„ í•„í„°ë§ í•¨ìˆ˜
    def filter_by_date_range(docs, start_date, end_date):
        return [doc for doc in docs if start_date <= doc["date"] <= end_date]

    # í‚¤ì›Œë“œ ê²€ìƒ‰ í•¨ìˆ˜
    def search_by_keyword(docs, keyword):
        keyword_lower = keyword.lower()
        return [doc for doc in docs if keyword_lower in doc["content"].lower()]

    # RAG í”„ë¡¬í”„íŠ¸
    rag_template = """
    ì°¸ê³  ë¬¸ì„œ:
    {context}

    ì§ˆë¬¸: {question}

    ë‹µë³€ ì‹œ ì¶œì²˜ì™€ ë‚ ì§œë¥¼ ëª…ì‹œí•´ì£¼ì„¸ìš”.
    """

    prompt = PromptTemplate(
        input_variables=["context", "question"], template=rag_template
    )

    # LLM ì´ˆê¸°í™”
    llm = ChatOpenAI(
        model="gpt-3.5-turbo", temperature=0, openai_api_key=os.getenv("OPENAI_API_KEY")
    )

    # ë‹¤ì–‘í•œ ê²€ìƒ‰ ì‹œë‚˜ë¦¬ì˜¤
    scenarios = [
        {
            "question": "ìµœê·¼ AI ê´€ë ¨ ì†Œì‹ì€ ë¬´ì—‡ì¸ê°€ìš”?",
            "filter_type": "category",
            "filter_value": "AI",
        },
        {
            "question": "í”„ë ˆì„ì›Œí¬ ê´€ë ¨ ì—…ë°ì´íŠ¸ë¥¼ ì•Œë ¤ì£¼ì„¸ìš”",
            "filter_type": "category",
            "filter_value": "Framework",
        },
        {
            "question": "2023ë…„ì— ìˆì—ˆë˜ ì£¼ìš” ë¦´ë¦¬ì¦ˆëŠ”?",
            "filter_type": "date_range",
            "filter_value": ("2023-01-01", "2023-12-31"),
        },
        {
            "question": "íŒŒì´ì¬ê³¼ ê´€ë ¨ëœ ì†Œì‹ì„ ì°¾ì•„ì£¼ì„¸ìš”",
            "filter_type": "keyword",
            "filter_value": "íŒŒì´ì¬",
        },
    ]

    for scenario in scenarios:
        print(f"\n{'=' * 60}")
        print(f"â“ ì§ˆë¬¸: {scenario['question']}")

        # í•„í„°ë§ ì ìš©
        filtered_docs = documents_with_metadata.copy()

        if scenario["filter_type"] == "category":
            filtered_docs = filter_by_category(filtered_docs, scenario["filter_value"])
            print(f"ğŸ·ï¸  í•„í„°: {scenario['filter_value']} ì¹´í…Œê³ ë¦¬")

        elif scenario["filter_type"] == "date_range":
            start, end = scenario["filter_value"]
            filtered_docs = filter_by_date_range(filtered_docs, start, end)
            print(f"ğŸ“… í•„í„°: {start} ~ {end}")

        elif scenario["filter_type"] == "keyword":
            filtered_docs = search_by_keyword(filtered_docs, scenario["filter_value"])
            print(f"ğŸ” í‚¤ì›Œë“œ: {scenario['filter_value']}")

        # ë‚ ì§œìˆœ ì •ë ¬
        sorted_docs = sort_by_date(filtered_docs)

        if sorted_docs:
            print(f"ğŸ“š ê²€ìƒ‰ëœ ë¬¸ì„œ: {len(sorted_docs)}ê°œ")

            # ì»¨í…ìŠ¤íŠ¸ ìƒì„± (ë©”íƒ€ë°ì´í„° í¬í•¨)
            context_parts = []
            for i, doc in enumerate(sorted_docs, 1):
                context_part = f"""
ë¬¸ì„œ {i}:
- ì¶œì²˜: {doc["source"]}
- ë‚ ì§œ: {doc["date"]}
- ì¹´í…Œê³ ë¦¬: {doc["category"]}
- ë‚´ìš©: {doc["content"]}
"""
                context_parts.append(context_part)
                print(
                    f"  ğŸ“„ [{doc['date']}] {doc['source']} - {doc['content'][:50]}..."
                )

            context = "\n".join(context_parts)

            # LLMìœ¼ë¡œ ë‹µë³€ ìƒì„±
            formatted_prompt = prompt.format(
                context=context, question=scenario["question"]
            )

            response = llm.invoke(formatted_prompt)
            print(f"\nğŸ“ ë‹µë³€: {response.content}")
        else:
            print("ğŸ“ í•´ë‹¹í•˜ëŠ” ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

    # ì¶”ê°€: ë³µí•© í•„í„°ë§ ì˜ˆì œ
    print(f"\n{'=' * 60}")
    print("ğŸ”§ ë³µí•© í•„í„°ë§ ì˜ˆì œ")
    print("='*60")

    # Framework ì¹´í…Œê³ ë¦¬ ì¤‘ 2023ë…„ ë¬¸ì„œë§Œ
    framework_docs = filter_by_category(documents_with_metadata, "Framework")
    recent_framework_docs = filter_by_date_range(
        framework_docs, "2023-01-01", "2023-12-31"
    )

    if recent_framework_docs:
        print("ì¡°ê±´: Framework ì¹´í…Œê³ ë¦¬ AND 2023ë…„")
        print(f"ê²°ê³¼: {len(recent_framework_docs)}ê°œ ë¬¸ì„œ")
        for doc in recent_framework_docs:
            print(f"  - [{doc['date']}] {doc['source']}: {doc['content'][:50]}...")

    return documents_with_metadata


# ì¶”ê°€: RAG ì„±ëŠ¥ ê°œì„ ì„ ìœ„í•œ ê³ ê¸‰ ì˜ˆì œ
def rag_with_reranking():
    """
    ì¬ìˆœìœ„í™”(Reranking)ë¥¼ í¬í•¨í•œ ê³ ê¸‰ RAG ì˜ˆì œ
    """

    # í™•ì¥ëœ ë¬¸ì„œ ë°ì´í„°ë² ì´ìŠ¤
    documents = [
        "íŒŒì´ì¬ì€ ê·€ë„ ë°˜ ë¡œì„¬ì´ 1991ë…„ ê°œë°œí•œ ê³ ê¸‰ í”„ë¡œê·¸ë˜ë° ì–¸ì–´ì…ë‹ˆë‹¤.",
        "íŒŒì´ì¬ì€ ê°„ê²°í•œ ë¬¸ë²•ê³¼ ë†’ì€ ê°€ë…ì„±ìœ¼ë¡œ ì´ˆë³´ìì—ê²Œ ì¸ê¸°ê°€ ë§ìŠµë‹ˆë‹¤.",
        "íŒŒì´ì¬ì€ ë°ì´í„° ê³¼í•™, ì›¹ ê°œë°œ, ìë™í™” ë“± ë‹¤ì–‘í•œ ë¶„ì•¼ì—ì„œ ì‚¬ìš©ë©ë‹ˆë‹¤.",
        "ìë°”ìŠ¤í¬ë¦½íŠ¸ëŠ” ë¸Œë Œë˜ ì•„ì´í¬ê°€ 1995ë…„ ê°œë°œí•œ ì›¹ í”„ë¡œê·¸ë˜ë° ì–¸ì–´ì…ë‹ˆë‹¤.",
        "ëŸ¬ìŠ¤íŠ¸ëŠ” ë©”ëª¨ë¦¬ ì•ˆì „ì„±ì„ ë³´ì¥í•˜ëŠ” ì‹œìŠ¤í…œ í”„ë¡œê·¸ë˜ë° ì–¸ì–´ì…ë‹ˆë‹¤.",
        "Go ì–¸ì–´ëŠ” êµ¬ê¸€ì—ì„œ ê°œë°œí•œ ë™ì‹œì„± ì²˜ë¦¬ì— ê°•í•œ ì–¸ì–´ì…ë‹ˆë‹¤.",
        "íŒŒì´ì¬ì˜ ì£¼ìš” ë¼ì´ë¸ŒëŸ¬ë¦¬ë¡œëŠ” NumPy, Pandas, TensorFlowê°€ ìˆìŠµë‹ˆë‹¤.",
        "íŒŒì´ì¬ 3.x ë²„ì „ì€ 2.xì™€ í˜¸í™˜ë˜ì§€ ì•ŠëŠ” ë³€ê²½ì‚¬í•­ì´ ìˆìŠµë‹ˆë‹¤.",
    ]

    # TF-IDF ë²¡í„°í™”
    vectorizer = TfidfVectorizer()
    doc_vectors = vectorizer.fit_transform(documents)

    def initial_retrieval(query, top_k=5):
        """ì´ˆê¸° ê²€ìƒ‰: TF-IDF ê¸°ë°˜"""
        query_vector = vectorizer.transform([query])
        similarities = cosine_similarity(query_vector, doc_vectors)[0]
        top_indices = np.argsort(similarities)[-top_k:][::-1]

        results = []
        for idx in top_indices:
            if similarities[idx] > 0:  # ê´€ë ¨ì„±ì´ ìˆëŠ” ë¬¸ì„œë§Œ
                results.append(
                    {
                        "document": documents[idx],
                        "initial_score": similarities[idx],
                        "index": idx,
                    }
                )
        return results

    def rerank_documents(query, initial_results, llm):
        """LLMì„ ì‚¬ìš©í•œ ì¬ìˆœìœ„í™”"""
        rerank_template = """
        ì§ˆë¬¸: {query}
        ë¬¸ì„œ: {document}

        ìœ„ ë¬¸ì„œê°€ ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ì— ì–¼ë§ˆë‚˜ ê´€ë ¨ì´ ìˆëŠ”ì§€ 0-10 ì ìˆ˜ë¡œ í‰ê°€í•˜ì„¸ìš”.
        ì ìˆ˜ë§Œ ìˆ«ìë¡œ ë‹µí•˜ì„¸ìš”.
        """

        prompt = PromptTemplate(
            input_variables=["query", "document"], template=rerank_template
        )

        reranked_results = []
        for result in initial_results:
            formatted_prompt = prompt.format(query=query, document=result["document"])

            try:
                response = llm.invoke(formatted_prompt)
                relevance_score = float(response.content.strip())
                result["rerank_score"] = relevance_score
                reranked_results.append(result)
            except Exception as e:
                print(f"ì¬ìˆœìœ„í™” ì˜¤ë¥˜: {e}")
                result["rerank_score"] = 0
                reranked_results.append(result)

        # ì¬ìˆœìœ„í™” ì ìˆ˜ë¡œ ì •ë ¬
        reranked_results.sort(key=lambda x: x["rerank_score"], reverse=True)
        return reranked_results

    # LLM ì´ˆê¸°í™”
    llm = ChatOpenAI(
        model="gpt-3.5-turbo", temperature=0, openai_api_key=os.getenv("OPENAI_API_KEY")
    )

    # ì§ˆë¬¸ ì²˜ë¦¬
    questions = [
        "íŒŒì´ì¬ì˜ ì°½ì‹œìì™€ ê°œë°œ ì—°ë„ë¥¼ ì•Œë ¤ì£¼ì„¸ìš”",
        "íŒŒì´ì¬ì´ ì¸ê¸° ìˆëŠ” ì´ìœ ëŠ” ë¬´ì—‡ì¸ê°€ìš”?",
        "ë©”ëª¨ë¦¬ ì•ˆì „ì„±ì´ ì¤‘ìš”í•œ ì–¸ì–´ëŠ”?",
    ]

    for question in questions:
        print(f"\n{'=' * 60}")
        print(f"â“ ì§ˆë¬¸: {question}")

        # 1ë‹¨ê³„: ì´ˆê¸° ê²€ìƒ‰
        initial_results = initial_retrieval(question, top_k=5)
        print("\nğŸ“Š ì´ˆê¸° ê²€ìƒ‰ ê²°ê³¼ (TF-IDF):")
        for i, result in enumerate(initial_results[:3], 1):
            print(
                f"  {i}. [ì ìˆ˜: {result['initial_score']:.3f}] {result['document'][:50]}..."
            )

        # 2ë‹¨ê³„: ì¬ìˆœìœ„í™”
        reranked_results = rerank_documents(question, initial_results, llm)
        print("\nğŸ“Š ì¬ìˆœìœ„í™” ê²°ê³¼ (LLM í‰ê°€):")
        for i, result in enumerate(reranked_results[:3], 1):
            print(
                f"  {i}. [ì ìˆ˜: {result['rerank_score']:.1f}] {result['document'][:50]}..."
            )

        # 3ë‹¨ê³„: ìµœì¢… ë‹µë³€ ìƒì„±
        top_docs = [r["document"] for r in reranked_results[:2]]
        context = "\n".join(top_docs)

        answer_template = """
        ì°¸ê³  ë¬¸ì„œ:
        {context}

        ì§ˆë¬¸: {question}

        ìœ„ ë¬¸ì„œë¥¼ ë°”íƒ•ìœ¼ë¡œ ê°„ê²°í•˜ê²Œ ë‹µë³€í•˜ì„¸ìš”.
        """

        answer_prompt = PromptTemplate(
            input_variables=["context", "question"], template=answer_template
        )

        formatted_prompt = answer_prompt.format(context=context, question=question)

        response = llm.invoke(formatted_prompt)
        print(f"\nğŸ“ ìµœì¢… ë‹µë³€: {response.content}")

    return vectorizer


# í…ŒìŠ¤íŠ¸ í•¨ìˆ˜
if __name__ == "__main__":
    print("RAG with Metadata ì˜ˆì œ ì‹¤í–‰")
    print("=" * 60)
    rag_with_metadata()

    print("\n\nRAG with Reranking ì˜ˆì œ ì‹¤í–‰")
    print("=" * 60)
    rag_with_reranking()
