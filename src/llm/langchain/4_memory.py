import uuid

from dotenv import load_dotenv
from langchain_community.chat_message_histories import ChatMessageHistory
from langchain_core.chat_history import BaseChatMessageHistory
from langchain_core.messages import AIMessage, HumanMessage
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.runnables.history import RunnableWithMessageHistory
from langchain_openai import ChatOpenAI

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

# ì„¸ì…˜ë³„ ë©”ëª¨ë¦¬ ì €ì¥ì†Œ
store = {}


def get_session_history(session_id: str) -> BaseChatMessageHistory:
    """ì„¸ì…˜ IDë³„ë¡œ ë©”ëª¨ë¦¬ íˆìŠ¤í† ë¦¬ë¥¼ ê´€ë¦¬"""
    if session_id not in store:
        store[session_id] = ChatMessageHistory()
    return store[session_id]


# ============================================================
# 4. ë©”ëª¨ë¦¬ (Memory) - ìµœì‹  ë°©ì‹
# ============================================================


def memory_conversation_buffer():
    """
    ëŒ€í™” ë²„í¼ ë©”ëª¨ë¦¬ ì˜ˆì œ - ì „ì²´ ëŒ€í™” ë‚´ìš© ì €ì¥ (ìµœì‹  ë°©ì‹)
    """

    # LLM ì´ˆê¸°í™”
    llm = ChatOpenAI(
        model="gpt-3.5-turbo",
        temperature=0.7,
    )

    # í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ìƒì„±
    prompt = ChatPromptTemplate.from_messages(
        [
            (
                "system",
                "ë‹¹ì‹ ì€ ë„ì›€ì´ ë˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. ì´ì „ ëŒ€í™” ë‚´ìš©ì„ ê¸°ì–µí•˜ê³  ë§¥ë½ì— ë§ê²Œ ì‘ë‹µí•˜ì„¸ìš”.",
            ),
            MessagesPlaceholder(variable_name="history"),
            ("human", "{input}"),
        ]
    )

    # ì²´ì¸ ìƒì„±
    chain = prompt | llm

    # ë©”ëª¨ë¦¬ê°€ ìˆëŠ” ì‹¤í–‰ ê°€ëŠ¥í•œ ì²´ì¸ ìƒì„±
    chain_with_history = RunnableWithMessageHistory(
        chain,
        get_session_history,
        input_messages_key="input",
        history_messages_key="history",
    )

    # ì„¸ì…˜ ID
    session_id = str(uuid.uuid4())

    # ëŒ€í™” ì‹œë®¬ë ˆì´ì…˜
    conversations = [
        "ì•ˆë…•í•˜ì„¸ìš”! ì €ëŠ” íŒŒì´ì¬ì„ ë°°ìš°ê³  ìˆëŠ” í•™ìƒì…ë‹ˆë‹¤.",
        "ì œê°€ ë­˜ ë°°ìš°ê³  ìˆë‹¤ê³  í–ˆì£ ?",
        "ì´ˆë³´ìì—ê²Œ ì¶”ì²œí•˜ëŠ” í”„ë¡œì íŠ¸ê°€ ìˆë‚˜ìš”?",
        "ì•ì„œ ì œê°€ ëˆ„êµ¬ë¼ê³  ì†Œê°œí–ˆë‚˜ìš”?",
    ]

    print("=== ëŒ€í™” ë²„í¼ ë©”ëª¨ë¦¬ ì˜ˆì œ ===")
    for user_input in conversations:
        print(f"\nğŸ‘¤ ì‚¬ìš©ì: {user_input}")

        response = chain_with_history.invoke(
            {"input": user_input}, config={"configurable": {"session_id": session_id}}
        )

        print(f"ğŸ¤– AI: {response.content}")

    # ë©”ëª¨ë¦¬ ë‚´ìš© í™•ì¸
    print("\nğŸ“ ì €ì¥ëœ ëŒ€í™” ë‚´ìš©:")
    history = get_session_history(session_id)
    for message in history.messages:
        if isinstance(message, HumanMessage):
            print(f"ğŸ‘¤: {message.content}")
        elif isinstance(message, AIMessage):
            print(f"ğŸ¤–: {message.content[:100]}...")

    return chain_with_history


def memory_conversation_summary():
    """
    ëŒ€í™” ìš”ì•½ ë©”ëª¨ë¦¬ ì˜ˆì œ - ëŒ€í™”ë¥¼ ìš”ì•½í•´ì„œ ì €ì¥ (ìˆ˜ë™ êµ¬í˜„)
    """

    llm = ChatOpenAI(
        model="gpt-3.5-turbo",
        temperature=0.7,
    )

    # ìš”ì•½ì„ ìœ„í•œ ë³„ë„ ì²´ì¸
    summary_prompt = ChatPromptTemplate.from_messages(
        [
            (
                "system",
                "ë‹¤ìŒ ëŒ€í™” ë‚´ìš©ì„ ê°„ê²°í•˜ê²Œ ìš”ì•½í•´ì£¼ì„¸ìš”. ì¤‘ìš”í•œ ì •ë³´ë§Œ í¬í•¨í•˜ì„¸ìš”.",
            ),
            ("human", "{conversation}"),
        ]
    )
    summary_chain = summary_prompt | llm

    # ë©”ì¸ ëŒ€í™” í”„ë¡¬í”„íŠ¸
    conversation_prompt = ChatPromptTemplate.from_messages(
        [
            (
                "system",
                "ë‹¹ì‹ ì€ ë„ì›€ì´ ë˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. ë‹¤ìŒì€ ì´ì „ ëŒ€í™”ì˜ ìš”ì•½ì…ë‹ˆë‹¤: {summary}",
            ),
            ("human", "{input}"),
        ]
    )
    conversation_chain = conversation_prompt | llm

    # ëŒ€í™” íˆìŠ¤í† ë¦¬ì™€ ìš”ì•½ ì €ì¥
    conversation_history = []
    summary = "ì•„ì§ ëŒ€í™”ê°€ ì—†ìŠµë‹ˆë‹¤."

    # ê¸´ ëŒ€í™” ì§„í–‰
    long_conversation = [
        "ì €ëŠ” ì›¹ ê°œë°œìì´ê³ , ìµœê·¼ì— Reactë¥¼ ê³µë¶€í•˜ê¸° ì‹œì‘í–ˆìŠµë‹ˆë‹¤.",
        "Reactì—ì„œ ìƒíƒœ ê´€ë¦¬ëŠ” ì–´ë–»ê²Œ í•˜ë‚˜ìš”?",
        "Reduxì™€ Context APIì˜ ì°¨ì´ì ì€ ë¬´ì—‡ì¸ê°€ìš”?",
        "ì œê°€ ì²˜ìŒì— ë¬´ì—‡ì„ ê³µë¶€í•œë‹¤ê³  í–ˆì—ˆë‚˜ìš”?",
    ]

    print("=== ëŒ€í™” ìš”ì•½ ë©”ëª¨ë¦¬ ì˜ˆì œ ===")
    for i, user_input in enumerate(long_conversation):
        print(f"\nğŸ‘¤ ì‚¬ìš©ì: {user_input}")

        # ëŒ€í™” ì‹¤í–‰
        response = conversation_chain.invoke({"input": user_input, "summary": summary})

        print(f"ğŸ¤– AI: {response.content[:200]}...")

        # ëŒ€í™” íˆìŠ¤í† ë¦¬ì— ì¶”ê°€
        conversation_history.append(f"Human: {user_input}")
        conversation_history.append(f"AI: {response.content}")

        # 3ë²ˆì§¸ ëŒ€í™”ë¶€í„° ìš”ì•½ ì‹œì‘
        if i >= 2:
            full_conversation = "\n".join(conversation_history)
            summary_response = summary_chain.invoke({"conversation": full_conversation})
            summary = summary_response.content
            print(f"ğŸ“„ í˜„ì¬ ìš”ì•½: {summary[:100]}...")

    return conversation_chain


def memory_window_buffer():
    """
    ìœˆë„ìš° ë²„í¼ ë©”ëª¨ë¦¬ ì˜ˆì œ - ìµœê·¼ Kê°œì˜ ëŒ€í™”ë§Œ ê¸°ì–µ (ìˆ˜ë™ êµ¬í˜„)
    """

    llm = ChatOpenAI(
        model="gpt-3.5-turbo",
        temperature=0.7,
    )

    # í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿
    prompt = ChatPromptTemplate.from_messages(
        [
            (
                "system",
                "ë‹¹ì‹ ì€ ë„ì›€ì´ ë˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. ë‹¤ìŒì€ ìµœê·¼ ëŒ€í™” ë‚´ìš©ì…ë‹ˆë‹¤: {recent_history}",
            ),
            ("human", "{input}"),
        ]
    )

    chain = prompt | llm

    # ìµœê·¼ ëŒ€í™”ë§Œ ì €ì¥ (k=2, ì¦‰ ìµœê·¼ 2ë²ˆì˜ ëŒ€í™”ìŒ)
    k = 2
    recent_messages = []

    # ì—¬ëŸ¬ ëŒ€í™” ì§„í–‰
    conversations = [
        "ì œ ì´ë¦„ì€ ê¹€ì² ìˆ˜ì…ë‹ˆë‹¤.",
        "ì €ëŠ” ì„œìš¸ì— ì‚´ê³  ìˆì–´ìš”.",
        "íŒŒì´ì¬ì„ ë°°ìš°ê³  ìˆìŠµë‹ˆë‹¤.",
        "ì œ ì´ë¦„ì´ ë­ë¼ê³  í–ˆì£ ?",  # ì´ë¯¸ ë©”ëª¨ë¦¬ì—ì„œ ì‚¬ë¼ì§
        "ì œê°€ ì–´ë”” ì‚°ë‹¤ê³  í–ˆì£ ?",  # ì•„ì§ ë©”ëª¨ë¦¬ì— ìˆìŒ
    ]

    print("=== ìœˆë„ìš° ë²„í¼ ë©”ëª¨ë¦¬ ì˜ˆì œ (ìµœê·¼ 2ê°œ ëŒ€í™”ë§Œ ê¸°ì–µ) ===")
    for i, user_input in enumerate(conversations, 1):
        print(f"\nëŒ€í™” {i}")
        print(f"ğŸ‘¤ ì‚¬ìš©ì: {user_input}")

        # ìµœê·¼ ëŒ€í™” íˆìŠ¤í† ë¦¬ êµ¬ì„±
        recent_history = (
            "\n".join(recent_messages[-k * 2 :])
            if recent_messages
            else "ì´ì „ ëŒ€í™”ê°€ ì—†ìŠµë‹ˆë‹¤."
        )

        response = chain.invoke({"input": user_input, "recent_history": recent_history})

        print(f"ğŸ¤– AI: {response.content[:150]}...")

        # ë©”ì‹œì§€ íˆìŠ¤í† ë¦¬ì— ì¶”ê°€
        recent_messages.append(f"Human: {user_input}")
        recent_messages.append(f"AI: {response.content}")

        # ìœˆë„ìš° í¬ê¸°ë§Œí¼ë§Œ ìœ ì§€ (k=2ì´ë¯€ë¡œ 4ê°œ ë©”ì‹œì§€: 2ê°œ ëŒ€í™”ìŒ)
        if len(recent_messages) > k * 2:
            recent_messages = recent_messages[-k * 2 :]

        print(f"ğŸ’¾ í˜„ì¬ ë©”ëª¨ë¦¬: {len(recent_messages) // 2}ê°œ ëŒ€í™”ìŒ ì €ì¥ë¨")
        print(f"ğŸ“ ì €ì¥ëœ ë‚´ìš©: {[msg[:50] + '...' for msg in recent_messages[-4:]]}")

    return chain


def memory_custom_session():
    """
    ì»¤ìŠ¤í…€ ì„¸ì…˜ ê´€ë¦¬ ì˜ˆì œ - ì—¬ëŸ¬ ì‚¬ìš©ìì˜ ëŒ€í™”ë¥¼ ë³„ë„ë¡œ ê´€ë¦¬
    """

    llm = ChatOpenAI(
        model="gpt-3.5-turbo",
        temperature=0.7,
    )

    prompt = ChatPromptTemplate.from_messages(
        [
            ("system", "ë‹¹ì‹ ì€ ë„ì›€ì´ ë˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤."),
            MessagesPlaceholder(variable_name="history"),
            ("human", "{input}"),
        ]
    )

    chain = prompt | llm

    chain_with_history = RunnableWithMessageHistory(
        chain,
        get_session_history,
        input_messages_key="input",
        history_messages_key="history",
    )

    # ì„œë¡œ ë‹¤ë¥¸ ì‚¬ìš©ì ì‹œë®¬ë ˆì´ì…˜
    users = {
        "user1": ["ì•ˆë…•í•˜ì„¸ìš”, ì €ëŠ” ê¹€ì² ìˆ˜ì…ë‹ˆë‹¤.", "ì œ ì´ë¦„ì„ ê¸°ì–µí•˜ê³  ê³„ì‹ ê°€ìš”?"],
        "user2": ["ì•ˆë…•í•˜ì„¸ìš”, ì €ëŠ” ì´ì˜í¬ì…ë‹ˆë‹¤.", "ì œ ì´ë¦„ì€ ë¬´ì—‡ì¸ê°€ìš”?"],
    }

    print("=== ë©€í‹° ì„¸ì…˜ ë©”ëª¨ë¦¬ ì˜ˆì œ ===")
    for user_id, messages in users.items():
        print(f"\n--- {user_id} ëŒ€í™” ---")
        for message in messages:
            print(f"ğŸ‘¤ {user_id}: {message}")

            response = chain_with_history.invoke(
                {"input": message}, config={"configurable": {"session_id": user_id}}
            )

            print(f"ğŸ¤– AI: {response.content}")

    # ê° ì‚¬ìš©ìë³„ ë©”ëª¨ë¦¬ í™•ì¸
    print("\nğŸ“ ì €ì¥ëœ ì„¸ì…˜ë“¤:")
    for session_id in store.keys():
        print(f"{session_id}: {len(store[session_id].messages)}ê°œ ë©”ì‹œì§€")

    return chain_with_history


if __name__ == "__main__":
    memory_conversation_buffer()
    print("\n" + "=" * 50 + "\n")
    memory_conversation_summary()
    print("\n" + "=" * 50 + "\n")
    memory_window_buffer()
    print("\n" + "=" * 50 + "\n")
    memory_custom_session()
