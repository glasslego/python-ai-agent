"""
ë¬¸ì„œ ìœ ì‚¬ë„ ë¶„ì„ ëª¨ë“ˆ
"""

import warnings
from typing import Any, Dict, List, Optional

import matplotlib.pyplot as plt
import networkx as nx
import numpy as np
import pandas as pd
import seaborn as sns
from loguru import logger
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity

warnings.filterwarnings(
    "ignore", category=UserWarning, module="matplotlib.font_manager"
)

# í°íŠ¸ ì„¤ì •
plt.rcParams["font.family"] = [
    "DejaVu Sans",
    "AppleGothic",
    "Malgun Gothic",
    "gulim",
    "Arial Unicode MS",
]
plt.rcParams["axes.unicode_minus"] = False


class SimilarityAnalyzer:
    """ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê¸°ë°˜ ë¬¸ì„œ ìœ ì‚¬ë„ ë¶„ì„ í´ë˜ìŠ¤"""

    def __init__(self, tfidf_matrix=None):
        """
        Args:
            tfidf_matrix: TF-IDF í–‰ë ¬ (ì„ íƒì )
        """
        self.tfidf_matrix = tfidf_matrix
        self.similarity_matrix = None
        self.is_computed = False

    def set_tfidf_matrix(self, tfidf_matrix):
        """TF-IDF í–‰ë ¬ ì„¤ì •"""
        self.tfidf_matrix = tfidf_matrix
        self.similarity_matrix = None
        self.is_computed = False

    def compute_similarity_matrix(self):
        """
        ì½”ì‚¬ì¸ ìœ ì‚¬ë„ í–‰ë ¬ ê³„ì‚°

        Returns:
            ì½”ì‚¬ì¸ ìœ ì‚¬ë„ í–‰ë ¬
        """
        if self.tfidf_matrix is None:
            raise ValueError("TF-IDF í–‰ë ¬ì´ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

        self.similarity_matrix = cosine_similarity(self.tfidf_matrix)
        self.is_computed = True

        logger.success(f" ìœ ì‚¬ë„ í–‰ë ¬ ê³„ì‚° ì™„ë£Œ ({self.similarity_matrix.shape})")
        logger.info(f"   í‰ê·  ìœ ì‚¬ë„: {self._get_avg_similarity():.3f}")

        return self.similarity_matrix

    def _get_avg_similarity(self) -> float:
        """í‰ê·  ìœ ì‚¬ë„ ê³„ì‚° (ëŒ€ê°ì„  ì œì™¸)"""
        if self.similarity_matrix is None:
            return 0.0

        mask = np.triu(np.ones_like(self.similarity_matrix, dtype=bool), k=1)
        similarities = self.similarity_matrix[mask]
        return similarities.mean()

    def find_similar_documents(
        self, doc_index: int, df: pd.DataFrame, n_similar: int = 5
    ) -> List[Dict[str, Any]]:
        """
        íŠ¹ì • ë¬¸ì„œì™€ ìœ ì‚¬í•œ ë¬¸ì„œë“¤ ì°¾ê¸°

        Args:
            doc_index: ê¸°ì¤€ ë¬¸ì„œì˜ ì¸ë±ìŠ¤
            df: ì›ë³¸ DataFrame
            n_similar: ì°¾ì„ ìœ ì‚¬ ë¬¸ì„œ ìˆ˜

        Returns:
            ìœ ì‚¬ ë¬¸ì„œ ì •ë³´ ë¦¬ìŠ¤íŠ¸
        """
        if not self.is_computed:
            self.compute_similarity_matrix()

        if doc_index >= len(df) or doc_index < 0:
            raise ValueError(f"ìœ íš¨í•˜ì§€ ì•Šì€ ë¬¸ì„œ ì¸ë±ìŠ¤: {doc_index}")

        # ìê¸° ìì‹ ì„ ì œì™¸í•œ ìœ ì‚¬ë„ ì ìˆ˜
        similarities = self.similarity_matrix[doc_index].copy()
        similarities[doc_index] = -1  # ìê¸° ìì‹  ì œì™¸

        # ìƒìœ„ ìœ ì‚¬ ë¬¸ì„œ ì¸ë±ìŠ¤
        similar_indices = similarities.argsort()[-n_similar:][::-1]

        reference_title = df.iloc[doc_index].get("Title", f"ë¬¸ì„œ {doc_index}")
        logger.info(
            f"\nğŸ”— '{reference_title[:50]}...'ì™€ ìœ ì‚¬í•œ ìƒìœ„ {n_similar}ê°œ ë¬¸ì„œ:"
        )

        similar_documents = []
        for i, idx in enumerate(similar_indices, 1):
            if similarities[idx] > 0:  # ìœ íš¨í•œ ìœ ì‚¬ë„ë§Œ
                title = df.iloc[idx].get("Title", f"ë¬¸ì„œ {idx}")
                similarity_score = similarities[idx]
                logger.info(f"   {i}. {title[:60]}...")
                logger.info(f"      ìœ ì‚¬ë„: {similarity_score:.3f}")

                similar_documents.append(
                    {
                        "index": int(idx),
                        "title": title,
                        "similarity": float(similarity_score),
                    }
                )

        return similar_documents

    def get_similarity_statistics(self) -> Dict[str, float]:
        """
        ì „ì²´ ìœ ì‚¬ë„ í†µê³„ ë°˜í™˜

        Returns:
            ìœ ì‚¬ë„ í†µê³„ ë”•ì…”ë„ˆë¦¬
        """
        if not self.is_computed:
            self.compute_similarity_matrix()

        # ëŒ€ê°ì„  ì œì™¸ (ìê¸° ìì‹ ê³¼ì˜ ìœ ì‚¬ë„ ì œì™¸)
        mask = np.triu(np.ones_like(self.similarity_matrix, dtype=bool), k=1)
        similarities = self.similarity_matrix[mask]

        return {
            "mean_similarity": float(similarities.mean()),
            "max_similarity": float(similarities.max()),
            "min_similarity": float(similarities.min()),
            "std_similarity": float(similarities.std()),
            "median_similarity": float(np.median(similarities)),
        }

    def find_most_similar_pairs(
        self, df: pd.DataFrame, top_k: int = 5
    ) -> List[Dict[str, Any]]:
        """
        ê°€ì¥ ìœ ì‚¬í•œ ë¬¸ì„œ ìŒë“¤ ì°¾ê¸°

        Args:
            df: ì›ë³¸ DataFrame
            top_k: ì°¾ì„ ìƒìœ„ ìŒ ìˆ˜

        Returns:
            ìœ ì‚¬í•œ ë¬¸ì„œ ìŒ ë¦¬ìŠ¤íŠ¸
        """
        if not self.is_computed:
            self.compute_similarity_matrix()

        # ìƒì‚¼ê° í–‰ë ¬ì—ì„œ ìœ ì‚¬ë„ ì¶”ì¶œ (ì¤‘ë³µ ë° ìê¸° ìì‹  ì œê±°)
        mask = np.triu(np.ones_like(self.similarity_matrix, dtype=bool), k=1)
        similarities = self.similarity_matrix[mask]

        # ìƒìœ„ kê°œ ìœ ì‚¬ë„ ì¸ë±ìŠ¤ ì°¾ê¸°
        top_indices = similarities.argsort()[-top_k:][::-1]

        # 2D ì¸ë±ìŠ¤ë¡œ ë³€í™˜
        mask_indices = np.where(mask)
        similar_pairs = []

        logger.info(f"\nğŸ† ê°€ì¥ ìœ ì‚¬í•œ ìƒìœ„ {top_k}ê°œ ë¬¸ì„œ ìŒ:")

        for rank, idx in enumerate(top_indices, 1):
            i, j = mask_indices[0][idx], mask_indices[1][idx]
            similarity = self.similarity_matrix[i, j]

            doc1_title = df.iloc[i].get("Title", f"ë¬¸ì„œ {i}")
            doc2_title = df.iloc[j].get("Title", f"ë¬¸ì„œ {j}")

            logger.info(f"   {rank}. ìœ ì‚¬ë„: {similarity:.3f}")
            logger.info(f"      â€¢ {doc1_title[:50]}...")
            logger.info(f"      â€¢ {doc2_title[:50]}...")

            similar_pairs.append(
                {
                    "rank": rank,
                    "doc1_index": int(i),
                    "doc1_title": doc1_title,
                    "doc2_index": int(j),
                    "doc2_title": doc2_title,
                    "similarity": float(similarity),
                }
            )

        return similar_pairs

    def find_outliers(
        self, df: pd.DataFrame, threshold: float = 0.1
    ) -> List[Dict[str, Any]]:
        """
        í‰ê·  ìœ ì‚¬ë„ê°€ ë‚®ì€ ì´ìƒì¹˜ ë¬¸ì„œ ì°¾ê¸°

        Args:
            df: ì›ë³¸ DataFrame
            threshold: ì´ìƒì¹˜ íŒë‹¨ ì„ê³„ê°’

        Returns:
            ì´ìƒì¹˜ ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸
        """
        if not self.is_computed:
            self.compute_similarity_matrix()

        logger.info(f"\nğŸ¯ ì´ìƒì¹˜ ë¬¸ì„œ íƒì§€ (ì„ê³„ê°’: {threshold})")

        outliers = []

        for i in range(len(self.similarity_matrix)):
            # ìê¸° ìì‹ ì„ ì œì™¸í•œ í‰ê·  ìœ ì‚¬ë„ ê³„ì‚°
            similarities = self.similarity_matrix[i].copy()
            similarities[i] = 0  # ìê¸° ìì‹  ì œì™¸
            avg_similarity = similarities.mean()

            if avg_similarity < threshold:
                title = df.iloc[i].get("Title", f"ë¬¸ì„œ {i}")
                outliers.append(
                    {
                        "index": int(i),
                        "title": title,
                        "avg_similarity": float(avg_similarity),
                    }
                )

        if outliers:
            logger.info(f"   ğŸ“‹ ë°œê²¬ëœ ì´ìƒì¹˜ ë¬¸ì„œ: {len(outliers)}ê°œ")
            for outlier in outliers:
                logger.info(f"   â€¢ {outlier['title'][:60]}...")
                logger.info(f"     í‰ê·  ìœ ì‚¬ë„: {outlier['avg_similarity']:.3f}")
        else:
            logger.info("   âœ… ì„ê³„ê°’ ì´í•˜ì˜ ì´ìƒì¹˜ ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤.")

        return outliers

    def visualize_similarity_matrix(
        self,
        df: pd.DataFrame,
        top_n: Optional[int] = None,
        save_path: str = "similarity_matrix_heatmap.png",
    ) -> None:
        """
        ìœ ì‚¬ë„ í–‰ë ¬ íˆíŠ¸ë§µ ì‹œê°í™”

        Args:
            df: ì›ë³¸ DataFrame
            top_n: ì‹œê°í™”í•  ìƒìœ„ ë¬¸ì„œ ìˆ˜ (Noneì´ë©´ ëª¨ë“  ë¬¸ì„œ)
            save_path: ì €ì¥í•  íŒŒì¼ ê²½ë¡œ
        """
        if not self.is_computed:
            self.compute_similarity_matrix()

        logger.info("\nğŸ“Š ìœ ì‚¬ë„ í–‰ë ¬ íˆíŠ¸ë§µ ìƒì„± ì¤‘...")

        # ìƒìœ„ Nê°œ ë¬¸ì„œë§Œ ì‹œê°í™” (ê°€ë…ì„±ì„ ìœ„í•´)
        if top_n is None:
            top_n = min(20, len(df))

        matrix_subset = self.similarity_matrix[:top_n, :top_n]
        titles_subset = [
            title[:25] + "..." if len(title) > 25 else title
            for title in df["Title"].head(top_n)
        ]

        plt.figure(figsize=(12, 10))

        # íˆíŠ¸ë§µ ìƒì„± (í•˜ì‚¼ê° ë§ˆìŠ¤í¬ ì ìš©)
        mask = np.triu(np.ones_like(matrix_subset, dtype=bool), k=1)

        sns.heatmap(
            matrix_subset,
            mask=mask,
            xticklabels=titles_subset,
            yticklabels=titles_subset,
            annot=True if top_n <= 10 else False,
            fmt=".2f",
            cmap="RdYlBu_r",
            center=0.5,
            square=True,
            cbar_kws={"label": "Cosine Similarity"},
            linewidths=0.5,
        )

        plt.title(
            f"Document Similarity Matrix (Top {top_n})", fontsize=16, fontweight="bold"
        )
        plt.xticks(rotation=45, ha="right")
        plt.yticks(rotation=0)
        plt.tight_layout()
        plt.savefig(save_path, dpi=300, bbox_inches="tight")
        plt.show()

        logger.info(f"   ğŸ“ íˆíŠ¸ë§µì´ '{save_path}'ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")

    def visualize_similarity_distribution(
        self, save_path: str = "similarity_distribution.png"
    ) -> None:
        """
        ìœ ì‚¬ë„ ë¶„í¬ ì‹œê°í™”

        Args:
            save_path: ì €ì¥í•  íŒŒì¼ ê²½ë¡œ
        """
        if not self.is_computed:
            self.compute_similarity_matrix()

        logger.info("\nğŸ“ˆ ìœ ì‚¬ë„ ë¶„í¬ ë¶„ì„ ì¤‘...")

        # ëŒ€ê°ì„  ì œì™¸í•œ ìœ ì‚¬ë„ ê°’ë“¤
        mask = np.triu(np.ones_like(self.similarity_matrix, dtype=bool), k=1)
        similarities = self.similarity_matrix[mask]

        fig, axes = plt.subplots(1, 3, figsize=(18, 5))

        # 1. íˆìŠ¤í† ê·¸ë¨
        axes[0].hist(
            similarities,
            bins=30,
            alpha=0.7,
            color="skyblue",
            edgecolor="black",
            density=True,
        )
        axes[0].axvline(
            similarities.mean(),
            color="red",
            linestyle="--",
            label=f"Mean: {similarities.mean():.3f}",
        )
        axes[0].axvline(
            np.median(similarities),
            color="green",
            linestyle="--",
            label=f"Median: {np.median(similarities):.3f}",
        )
        axes[0].set_title("Similarity Distribution", fontweight="bold")
        axes[0].set_xlabel("Cosine Similarity")
        axes[0].set_ylabel("Density")
        axes[0].legend()
        axes[0].grid(True, alpha=0.3)

        # 2. ë°•ìŠ¤í”Œë¡¯
        box_plot = axes[1].boxplot(similarities, patch_artist=True)
        box_plot["boxes"][0].set_facecolor("lightblue")
        box_plot["boxes"][0].set_alpha(0.7)
        axes[1].set_title("Similarity Box Plot", fontweight="bold")
        axes[1].set_ylabel("Cosine Similarity")
        axes[1].grid(True, alpha=0.3)

        # 3. ëˆ„ì  ë¶„í¬ í•¨ìˆ˜
        sorted_sim = np.sort(similarities)
        cumulative = np.arange(1, len(sorted_sim) + 1) / len(sorted_sim)
        axes[2].plot(sorted_sim, cumulative, linewidth=2, color="darkblue")
        axes[2].set_title("Cumulative Distribution Function", fontweight="bold")
        axes[2].set_xlabel("Cosine Similarity")
        axes[2].set_ylabel("Cumulative Probability")
        axes[2].grid(True, alpha=0.3)

        plt.tight_layout()
        plt.savefig(save_path, dpi=300, bbox_inches="tight")
        plt.show()

        logger.info(f"   ğŸ“ ë¶„í¬ ë¶„ì„ì´ '{save_path}'ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")

        # í†µê³„ ì •ë³´ ì¶œë ¥
        stats = self.get_similarity_statistics()
        logger.info("   ğŸ“Š ìœ ì‚¬ë„ í†µê³„:")
        logger.info(f"      â€¢ í‰ê· : {stats['mean_similarity']:.3f}")
        logger.info(f"      â€¢ ì¤‘ì•™ê°’: {stats['median_similarity']:.3f}")
        logger.info(f"      â€¢ í‘œì¤€í¸ì°¨: {stats['std_similarity']:.3f}")
        logger.info(f"      â€¢ ìµœëŒ€ê°’: {stats['max_similarity']:.3f}")
        logger.info(f"      â€¢ ìµœì†Œê°’: {stats['min_similarity']:.3f}")

    def create_similarity_network(
        self,
        df: pd.DataFrame,
        threshold: float = 0.3,
        top_n: int = 15,
        save_path: str = "similarity_network.png",
    ) -> None:
        """
        ìœ ì‚¬ë„ ê¸°ë°˜ ë„¤íŠ¸ì›Œí¬ ê·¸ë˜í”„ ìƒì„±

        Args:
            df: ì›ë³¸ DataFrame
            threshold: ì—°ê²° ì„ê³„ê°’
            top_n: í‘œì‹œí•  ìƒìœ„ ë¬¸ì„œ ìˆ˜
            save_path: ì €ì¥í•  íŒŒì¼ ê²½ë¡œ
        """
        if not self.is_computed:
            self.compute_similarity_matrix()

        logger.info(f"\nğŸ•¸ï¸  ìœ ì‚¬ë„ ë„¤íŠ¸ì›Œí¬ ìƒì„± ì¤‘... (ì„ê³„ê°’: {threshold})")

        # ìƒìœ„ Nê°œ ë¬¸ì„œë§Œ ì‚¬ìš©
        matrix_subset = self.similarity_matrix[:top_n, :top_n]
        titles_subset = df["Title"].head(top_n).tolist()

        # ë„¤íŠ¸ì›Œí¬ ê·¸ë˜í”„ ìƒì„±
        G = nx.Graph()

        # ë…¸ë“œ ì¶”ê°€
        for i, title in enumerate(titles_subset):
            short_title = title[:30] + "..." if len(title) > 30 else title
            G.add_node(i, title=short_title, full_title=title)

        # ì—£ì§€ ì¶”ê°€ (ì„ê³„ê°’ ì´ìƒì˜ ìœ ì‚¬ë„)
        edges_added = 0
        for i in range(len(matrix_subset)):
            for j in range(i + 1, len(matrix_subset)):
                similarity = matrix_subset[i, j]
                if similarity > threshold:
                    G.add_edge(i, j, weight=similarity)
                    edges_added += 1

        logger.info(f"   ğŸ”— ì¶”ê°€ëœ ì—£ì§€ ìˆ˜: {edges_added}")

        # ê·¸ë˜í”„ ì‹œê°í™”
        plt.figure(figsize=(14, 10))

        if len(G.edges()) > 0:
            # ë ˆì´ì•„ì›ƒ ì„¤ì •
            pos = nx.spring_layout(G, k=2, iterations=50, seed=42)
        else:
            # ì—£ì§€ê°€ ì—†ìœ¼ë©´ ì›í˜• ë°°ì¹˜
            pos = nx.circular_layout(G)

        # ë…¸ë“œ í¬ê¸° (ì—°ê²° ìˆ˜ì— ë¹„ë¡€)
        node_sizes = [300 + G.degree(node) * 100 for node in G.nodes()]

        # ë…¸ë“œ ìƒ‰ìƒ (ì—°ê²° ìˆ˜ì— ë”°ë¼)
        node_colors = [G.degree(node) for node in G.nodes()]

        # ì—£ì§€ ë‘ê»˜ (ìœ ì‚¬ë„ì— ë¹„ë¡€)
        if len(G.edges()) > 0:
            edge_weights = [G[u][v]["weight"] * 5 for u, v in G.edges()]
        else:
            edge_weights = []

        # ë…¸ë“œ ê·¸ë¦¬ê¸°
        nodes = nx.draw_networkx_nodes(
            G,
            pos,
            node_size=node_sizes,
            node_color=node_colors,
            cmap=plt.cm.viridis,
            alpha=0.8,
        )

        # ì—£ì§€ ê·¸ë¦¬ê¸°
        if len(G.edges()) > 0:
            nx.draw_networkx_edges(
                G, pos, width=edge_weights, alpha=0.6, edge_color="gray"
            )

        # ë…¸ë“œ ë¼ë²¨
        labels = {node: data["title"] for node, data in G.nodes(data=True)}
        nx.draw_networkx_labels(G, pos, labels, font_size=8, font_weight="bold")

        # ì»¬ëŸ¬ë°” ì¶”ê°€
        if nodes is not None:
            plt.colorbar(nodes, label="Number of Connections")

        plt.title(
            f"Document Similarity Network (Threshold: {threshold})",
            fontsize=16,
            fontweight="bold",
        )
        plt.axis("off")
        plt.tight_layout()
        plt.savefig(save_path, dpi=300, bbox_inches="tight")
        plt.show()

        logger.info(f"   ğŸ“ ë„¤íŠ¸ì›Œí¬ ê·¸ë˜í”„ê°€ '{save_path}'ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")

        # ë„¤íŠ¸ì›Œí¬ í†µê³„
        if len(G.nodes()) > 0:
            logger.info("   ğŸ“Š ë„¤íŠ¸ì›Œí¬ í†µê³„:")
            logger.info(f"      â€¢ ë…¸ë“œ ìˆ˜: {len(G.nodes())}")
            logger.info(f"      â€¢ ì—£ì§€ ìˆ˜: {len(G.edges())}")
            if len(G.edges()) > 0:
                logger.info(
                    f"      â€¢ í‰ê·  ì—°ê²°ë„: {sum(dict(G.degree()).values()) / len(G.nodes()):.2f}"  # noqa: E501
                )
                logger.info(f"      â€¢ ë°€ë„: {nx.density(G):.3f}")

    def export_results(
        self, df: pd.DataFrame, output_prefix: str = "similarity_analysis"
    ) -> Dict[str, str]:
        """
        ìœ ì‚¬ë„ ë¶„ì„ ê²°ê³¼ ë‚´ë³´ë‚´ê¸°

        Args:
            df: ì›ë³¸ DataFrame
            output_prefix: ì¶œë ¥ íŒŒì¼ ì ‘ë‘ì‚¬

        Returns:
            ì €ì¥ëœ íŒŒì¼ ê²½ë¡œë“¤
        """
        if not self.is_computed:
            self.compute_similarity_matrix()

        saved_files = {}

        logger.info("\nğŸ’¾ ìœ ì‚¬ë„ ë¶„ì„ ê²°ê³¼ ì €ì¥ ì¤‘...")

        # 1. ìœ ì‚¬ë„ í–‰ë ¬ ì €ì¥
        similarity_df = pd.DataFrame(
            self.similarity_matrix,
            columns=[f"Doc_{i}" for i in range(len(df))],
            index=[f"Doc_{i}" for i in range(len(df))],
        )
        matrix_file = f"{output_prefix}_matrix.csv"
        similarity_df.to_csv(matrix_file, encoding="utf-8-sig")
        saved_files["matrix"] = matrix_file

        # 2. ìœ ì‚¬í•œ ë¬¸ì„œ ìŒ ì €ì¥
        similar_pairs = self.find_most_similar_pairs(df, top_k=10)
        if similar_pairs:
            pairs_df = pd.DataFrame(similar_pairs)
            pairs_file = f"{output_prefix}_pairs.csv"
            pairs_df.to_csv(pairs_file, index=False, encoding="utf-8-sig")
            saved_files["pairs"] = pairs_file

        # 3. ë¬¸ì„œë³„ í‰ê·  ìœ ì‚¬ë„ ì €ì¥
        doc_similarities = []
        for i in range(len(self.similarity_matrix)):
            similarities = self.similarity_matrix[i].copy()
            similarities[i] = 0  # ìê¸° ìì‹  ì œì™¸
            avg_similarity = similarities.mean()
            max_similarity = similarities.max()

            doc_similarities.append(
                {
                    "doc_index": i,
                    "title": df.iloc[i].get("Title", f"ë¬¸ì„œ {i}"),
                    "avg_similarity": avg_similarity,
                    "max_similarity": max_similarity,
                }
            )

        doc_sim_df = pd.DataFrame(doc_similarities)
        doc_sim_df = doc_sim_df.sort_values("avg_similarity", ascending=False)
        doc_file = f"{output_prefix}_document_stats.csv"
        doc_sim_df.to_csv(doc_file, index=False, encoding="utf-8-sig")
        saved_files["document_stats"] = doc_file

        # 4. ì´ìƒì¹˜ ë¬¸ì„œ ì €ì¥
        outliers = self.find_outliers(df, threshold=0.1)
        if outliers:
            outliers_df = pd.DataFrame(outliers)
            outliers_file = f"{output_prefix}_outliers.csv"
            outliers_df.to_csv(outliers_file, index=False, encoding="utf-8-sig")
            saved_files["outliers"] = outliers_file

        # 5. í†µê³„ ìš”ì•½ ì €ì¥
        stats = self.get_similarity_statistics()
        stats_df = pd.DataFrame([stats])
        stats_file = f"{output_prefix}_statistics.csv"
        stats_df.to_csv(stats_file, index=False, encoding="utf-8-sig")
        saved_files["statistics"] = stats_file

        logger.info(f"   ğŸ“Š ìœ ì‚¬ë„ í–‰ë ¬: {matrix_file}")
        logger.info(f"   ğŸ”— ìœ ì‚¬ ë¬¸ì„œ ìŒ: {pairs_file}")
        logger.info(f"   ğŸ“„ ë¬¸ì„œë³„ í†µê³„: {doc_file}")
        logger.info(f"   ğŸ“ˆ ì „ì²´ í†µê³„: {stats_file}")
        if outliers:
            logger.info(f"   ğŸ¯ ì´ìƒì¹˜ ë¬¸ì„œ: {outliers_file}")

        return saved_files


if __name__ == "__main__":
    logger.info("ğŸ” ë¬¸ì„œ ìœ ì‚¬ë„ ë¶„ì„ ëª¨ë“ˆ í…ŒìŠ¤íŠ¸")

    df = pd.read_csv("processed_news.csv")

    vectorizer = TfidfVectorizer(
        max_features=1000,  # ìµœëŒ€ íŠ¹ì„± ìˆ˜
        stop_words="english",  # ì˜ì–´ ë¶ˆìš©ì–´ ì œê±°
        ngram_range=(1, 2),  # 1-gramê³¼ 2-gram ì‚¬ìš©
        min_df=2,  # ìµœì†Œ ë¬¸ì„œ ë¹ˆë„
        max_df=0.8,  # ìµœëŒ€ ë¬¸ì„œ ë¹ˆë„
    )

    tfidf_matrix = vectorizer.fit_transform(df["processed_text"])
    feature_names = vectorizer.get_feature_names_out()

    # ìœ ì‚¬ë„ ë¶„ì„ ìˆ˜í–‰
    analyzer = SimilarityAnalyzer(tfidf_matrix)
    analyzer.compute_similarity_matrix()

    # í†µê³„ ì¶œë ¥
    stats = analyzer.get_similarity_statistics()
    logger.info(f"\nìœ ì‚¬ë„ í†µê³„: {stats}")

    # ìœ ì‚¬í•œ ë¬¸ì„œ ì°¾ê¸°
    similar_docs = analyzer.find_similar_documents(0, df, 3)
    logger.info(f"\nìœ ì‚¬í•œ ë¬¸ì„œ: {similar_docs}")
