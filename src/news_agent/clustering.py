"""
K-Means í´ëŸ¬ìŠ¤í„°ë§ ê¸°ë°˜ ë‰´ìŠ¤ ë¶„ë¥˜ ëª¨ë“ˆ (auto_optimize + ë‚´ë¶€ tfidf_matrix ë³´ê´€)
"""

import json
import warnings
from typing import Any, Dict, List, Optional, Tuple

import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
from loguru import logger
from sklearn.cluster import KMeans
from sklearn.decomposition import PCA
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics import silhouette_score

warnings.filterwarnings("ignore")


class NewsClustering:
    """ë‰´ìŠ¤ ê¸°ì‚¬ K-Means í´ëŸ¬ìŠ¤í„°ë§ í´ë˜ìŠ¤"""

    def __init__(
        self,
        n_clusters: int = 3,
        random_state: int = 42,
        auto_optimize: bool = False,
        min_clusters: int = 2,
        max_clusters: int = 8,
        tfidf_matrix: Optional[Any] = None,
        feature_names: Optional[np.ndarray] = None,
    ):
        """
        Args:
            n_clusters: ê¸°ë³¸ í´ëŸ¬ìŠ¤í„° ê°œìˆ˜
            random_state: ëœë¤ ì‹œë“œ
            auto_optimize: fit ì‹œ find_optimal_clusters ì‹¤í–‰ ì—¬ë¶€
            min_clusters: ìë™ ìµœì í™” ì‹œ ìµœì†Œ í´ëŸ¬ìŠ¤í„° ìˆ˜
            max_clusters: ìë™ ìµœì í™” ì‹œ ìµœëŒ€ í´ëŸ¬ìŠ¤í„° ìˆ˜
            tfidf_matrix: ì‚¬ì „ ê³„ì‚°ëœ TF-IDF í–‰ë ¬ (CSR ë“± í¬ì†Œí–‰ë ¬)
            feature_names: TF-IDF íŠ¹ì„± ì´ë¦„ ë°°ì—´ (vectorizer.get_feature_names_out())
        """
        self.tfidf_matrix = tfidf_matrix
        self.feature_names = feature_names

        self.n_clusters = max(2, n_clusters)  # ì‹¤ë£¨ì—£ ê³„ì‚°ì„ ìœ„í•´ ìµœì†Œ 2
        self.random_state = random_state

        # ìµœì í™” ì„¤ì •
        self.auto_optimize = auto_optimize
        self.min_clusters = max(2, min_clusters)
        self.max_clusters = max(self.min_clusters, max_clusters)

        self.kmeans = KMeans(n_clusters=self.n_clusters, random_state=self.random_state)
        self.labels: Optional[np.ndarray] = None
        self.is_fitted = False
        self.cluster_info: Dict[int, Any] = {}

    # ---------- ìœ í‹¸ ----------
    def set_features(
        self,
        tfidf_matrix: Any,
        feature_names: Optional[np.ndarray] = None,
    ) -> None:
        """ì™¸ë¶€ì—ì„œ TF-IDF í–‰ë ¬(ë° íŠ¹ì„±ëª…)ì„ êµì²´/ì„¤ì •"""
        self.tfidf_matrix = tfidf_matrix
        if feature_names is not None:
            self.feature_names = feature_names
        # í–‰ë ¬ì´ ë°”ë€Œë©´ ê¸°ì¡´ í•™ìŠµ ìƒíƒœëŠ” ë¬´íš¨í™”
        self.is_fitted = False
        self.labels = None

    def _require_matrix(self) -> None:
        if self.tfidf_matrix is None:
            raise ValueError(
                "tfidf_matrixê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ìƒì„±ìì— ì „ë‹¬í•˜ê±°ë‚˜ set_features()ë¥¼ í˜¸ì¶œí•˜ì„¸ìš”."
            )

    # ---------- ëª¨ë¸ ----------
    def fit_predict(self) -> np.ndarray:
        """
        í´ëŸ¬ìŠ¤í„°ë§ ìˆ˜í–‰ (auto_optimize=Trueë©´ ìµœì  k íƒìƒ‰ í›„ ì ìš©)
        Returns:
            í´ëŸ¬ìŠ¤í„° ë¼ë²¨ ë°°ì—´
        """
        self._require_matrix()

        # âœ… ìë™ ìµœì í™”: í˜„ì¬ ë°ì´í„° ê¸°ì¤€ ìµœì  k ì‚°ì¶œ í›„ ì ìš©
        if self.auto_optimize:
            optimal_k, _ = self.find_optimal_clusters(
                max_clusters=self.max_clusters,
                min_clusters=self.min_clusters,
            )
            if optimal_k != self.n_clusters:
                logger.info(
                    f"ìµœì  k={optimal_k} ì ìš© (ê¸°ì¡´ {self.n_clusters} â†’ {optimal_k})"
                )
                self.n_clusters = int(optimal_k)
                self.kmeans = KMeans(
                    n_clusters=self.n_clusters, random_state=self.random_state
                )

        self.labels = self.kmeans.fit_predict(self.tfidf_matrix)
        self.is_fitted = True

        logger.success(f"K-Means í´ëŸ¬ìŠ¤í„°ë§ ì™„ë£Œ ({self.n_clusters}ê°œ í´ëŸ¬ìŠ¤í„°)")
        try:
            logger.info(f"ì‹¤ë£¨ì—£ ì ìˆ˜: {self.get_silhouette_score():.3f}")
        except Exception as e:
            logger.warning(f"ì‹¤ë£¨ì—£ ì ìˆ˜ ê³„ì‚° ì‹¤íŒ¨: {e}")

        return self.labels

    def predict(self) -> np.ndarray:
        """ìƒˆë¡œìš´ ë°ì´í„°ì— ëŒ€í•´ í´ëŸ¬ìŠ¤í„° ì˜ˆì¸¡ (ì‚¬ì „ í•™ìŠµ í•„ìš”)"""
        if not self.is_fitted:
            raise ValueError(
                "ëª¨ë¸ì´ í•™ìŠµë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. fit_predict()ë¥¼ ë¨¼ì € í˜¸ì¶œí•˜ì„¸ìš”."
            )
        self._require_matrix()
        return self.kmeans.predict(self.tfidf_matrix)

    def get_silhouette_score(self) -> float:
        """ì‹¤ë£¨ì—£ ì ìˆ˜ ê³„ì‚°"""
        self._require_matrix()
        if self.labels is None or len(set(self.labels)) < 2:
            return 0.0
        return silhouette_score(self.tfidf_matrix, self.labels)

    # ---------- ë¶„ì„/ì‹œê°í™” ----------
    def analyze_clusters(
        self,
        df: pd.DataFrame,
        top_keywords: int = 10,
    ) -> pd.DataFrame:
        """
        í´ëŸ¬ìŠ¤í„° ë¶„ì„: dfì— 'cluster' ì»¬ëŸ¼ ì¶”ê°€ ë° í‚¤ì›Œë“œ/ìƒ˜í”Œ ë¡œê¹…
        """
        if not self.is_fitted:
            raise ValueError("í´ëŸ¬ìŠ¤í„°ë§ì´ ìˆ˜í–‰ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        self._require_matrix()

        df_with_clusters = df.copy()
        df_with_clusters["cluster"] = self.labels

        logger.info("í´ëŸ¬ìŠ¤í„° ë¶„ì„ ê²°ê³¼")
        cluster_info: Dict[int, Any] = {}

        for cluster_id in range(self.n_clusters):
            cluster_docs = df_with_clusters[df_with_clusters["cluster"] == cluster_id]
            cluster_size = len(cluster_docs)
            logger.info(f"\nğŸ“Š í´ëŸ¬ìŠ¤í„° {cluster_id} ({cluster_size}ê°œ ê¸°ì‚¬):")

            # ìƒ˜í”Œ ì œëª©
            sample_titles = (
                cluster_docs["Title"].head(3).tolist()
                if "Title" in cluster_docs.columns
                else []
            )
            for title in sample_titles:
                logger.info(f"   â€¢ {title[:60]}...")

            # í´ëŸ¬ìŠ¤í„° ì¤‘ì‹¬ í‚¤ì›Œë“œ
            keywords = []
            if (
                hasattr(self.kmeans, "cluster_centers_")
                and self.feature_names is not None
            ):
                cluster_center = self.kmeans.cluster_centers_[cluster_id]
                top_indices = cluster_center.argsort()[-top_keywords:][::-1]

                logger.info("   ğŸ”‘ ì£¼ìš” í‚¤ì›Œë“œ:")
                for i in top_indices:
                    if cluster_center[i] > 0.01:
                        keyword = self.feature_names[i]
                        score = cluster_center[i]
                        keywords.append({"keyword": keyword, "score": score})
                        logger.info(f"      - {keyword} ({score:.3f})")
            elif self.feature_names is None:
                logger.warning(
                    "feature_namesê°€ ì—†ì–´ í‚¤ì›Œë“œ ë¡œê¹…ì„ ê±´ë„ˆëœë‹ˆë‹¤. (vectorizer.get_feature_names_out() ì „ë‹¬ í•„ìš”)"  # noqa: E501
                )

            cluster_info[cluster_id] = {
                "size": cluster_size,
                "sample_titles": sample_titles,
                "keywords": keywords,
            }

        self.cluster_info = cluster_info
        return df_with_clusters

    def find_optimal_clusters(
        self, max_clusters: int = 8, min_clusters: int = 2
    ) -> Tuple[int, List[float]]:
        """ìµœì  í´ëŸ¬ìŠ¤í„° ìˆ˜ ì°¾ê¸° (ì‹¤ë£¨ì—£ ê¸°ì¤€)"""
        self._require_matrix()

        n_samples = self.tfidf_matrix.shape[0]
        # í‘œë³¸ ìˆ˜ì— ë”°ë¥¸ ìƒí•œ ë³´ì •
        max_clusters = min(max_clusters, max(2, n_samples // 2))
        min_clusters = max(2, min_clusters)

        if max_clusters < min_clusters:
            logger.warning("í‘œë³¸ ìˆ˜ê°€ ì ì–´ ìµœì í™” ë²”ìœ„ë¥¼ ì¶•ì†Œí–ˆìŠµë‹ˆë‹¤.")
            return min_clusters, []

        logger.info(
            f"\nğŸ” ìµœì  í´ëŸ¬ìŠ¤í„° ìˆ˜ íƒìƒ‰ ì¤‘... (ë²”ìœ„: {min_clusters}-{max_clusters})"
        )

        silhouette_scores: List[float] = []
        cluster_range = list(range(min_clusters, max_clusters + 1))

        for n_clusters in cluster_range:
            try:
                kmeans_temp = KMeans(
                    n_clusters=n_clusters, random_state=self.random_state
                )
                temp_labels = kmeans_temp.fit_predict(self.tfidf_matrix)
                # ì‹¤ë£¨ì—£ì€ ìµœì†Œ 2ê°œ í´ëŸ¬ìŠ¤í„° í•„ìš”
                if len(set(temp_labels)) < 2:
                    silhouette_avg = -1.0
                else:
                    silhouette_avg = silhouette_score(self.tfidf_matrix, temp_labels)
                silhouette_scores.append(silhouette_avg)
                logger.info(
                    f"   í´ëŸ¬ìŠ¤í„° ìˆ˜ {n_clusters}: ì‹¤ë£¨ì—£ ì ìˆ˜ = {silhouette_avg:.3f}"
                )
            except Exception as e:
                logger.warning(f"   í´ëŸ¬ìŠ¤í„° ìˆ˜ {n_clusters}: ê³„ì‚° ì‹¤íŒ¨({e})")
                silhouette_scores.append(-1.0)

        optimal_idx = int(np.argmax(silhouette_scores))
        optimal_clusters = cluster_range[optimal_idx]
        logger.info(
            f"\nâœ… ìµœì  í´ëŸ¬ìŠ¤í„° ìˆ˜: {optimal_clusters} (ì‹¤ë£¨ì—£ ì ìˆ˜: {silhouette_scores[optimal_idx]:.3f})"  # noqa: E501
        )

        return optimal_clusters, silhouette_scores

    def visualize_clusters(
        self,
        df_with_clusters: pd.DataFrame,
        save_path: str = "news_clustering_analysis.png",
    ) -> None:
        """í´ëŸ¬ìŠ¤í„° ì‹œê°í™”"""
        if not self.is_fitted:
            raise ValueError("í´ëŸ¬ìŠ¤í„°ë§ì´ ìˆ˜í–‰ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        self._require_matrix()

        logger.info("\nğŸ“ˆ í´ëŸ¬ìŠ¤í„° ì‹œê°í™” ìƒì„± ì¤‘...")

        # PCA 2D
        pca = PCA(n_components=2, random_state=self.random_state)
        coords_2d = pca.fit_transform(self.tfidf_matrix.toarray())

        colors = [
            "red",
            "blue",
            "green",
            "purple",
            "orange",
            "brown",
            "pink",
            "gray",
            "cyan",
            "magenta",
        ]
        fig, axes = plt.subplots(1, 3, figsize=(18, 6))

        # 1) ì‚°ì ë„
        ax1 = axes[0]
        for i in range(self.n_clusters):
            cluster_points = coords_2d[self.labels == i]
            ax1.scatter(
                cluster_points[:, 0],
                cluster_points[:, 1],
                c=colors[i % len(colors)],
                label=f"Cluster {i}",
                alpha=0.7,
                s=50,
            )
        ax1.set_title("News Clusters (PCA 2D)", fontsize=14)
        ax1.set_xlabel(f"PC1 ({pca.explained_variance_ratio_[0]:.1%} variance)")
        ax1.set_ylabel(f"PC2 ({pca.explained_variance_ratio_[1]:.1%} variance)")
        ax1.legend()
        ax1.grid(True, alpha=0.3)

        # 2) í¬ê¸° ë¶„í¬
        ax2 = axes[1]
        cluster_counts = df_with_clusters["cluster"].value_counts().sort_index()
        bars = ax2.bar(
            range(self.n_clusters),
            cluster_counts.values,
            color=[colors[i % len(colors)] for i in range(self.n_clusters)],
            alpha=0.7,
        )
        ax2.set_title("Cluster Size Distribution", fontsize=14)
        ax2.set_xlabel("Cluster ID")
        ax2.set_ylabel("Number of Articles")
        ax2.grid(True, alpha=0.3)
        for bar, count in zip(bars, cluster_counts.values):
            ax2.text(
                bar.get_x() + bar.get_width() / 2,
                bar.get_height() + 0.5,
                str(count),
                ha="center",
                va="bottom",
            )

        # 3) ì‹¤ë£¨ì—£ ë¹„êµ
        ax3 = axes[2]
        silhouette_current = self.get_silhouette_score()
        cluster_range = range(2, min(9, len(df_with_clusters) // 2 + 1))
        silhouette_scores = []
        for n in cluster_range:
            try:
                kmeans_temp = KMeans(n_clusters=n, random_state=self.random_state)
                temp_labels = kmeans_temp.fit_predict(self.tfidf_matrix)
                score = (
                    silhouette_score(self.tfidf_matrix, temp_labels)
                    if len(set(temp_labels)) > 1
                    else -1.0
                )
                silhouette_scores.append(score)
            except Exception:
                silhouette_scores.append(-1.0)

        if silhouette_scores:
            ax3.plot(
                list(cluster_range), silhouette_scores, "bo-", linewidth=2, markersize=6
            )
            ax3.axvline(
                x=self.n_clusters,
                color="red",
                linestyle="--",
                linewidth=2,
                label=f"Current ({self.n_clusters} clusters)",
            )
            ax3.axhline(
                y=silhouette_current,
                color="red",
                linestyle=":",
                alpha=0.7,
                label=f"Score: {silhouette_current:.3f}",
            )

        ax3.set_title("Silhouette Score by Cluster Count", fontsize=14)
        ax3.set_xlabel("Number of Clusters")
        ax3.set_ylabel("Silhouette Score")
        ax3.legend()
        ax3.grid(True, alpha=0.3)

        plt.tight_layout()
        plt.savefig(save_path, dpi=300, bbox_inches="tight")
        plt.show()

        logger.info(f"   í˜„ì¬ í´ëŸ¬ìŠ¤í„°ë§ ì‹¤ë£¨ì—£ ì ìˆ˜: {silhouette_current:.3f}")
        logger.info(f"   ğŸ“ ì‹œê°í™” ê²°ê³¼ê°€ '{save_path}'ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")

    # ---------- ê²°ê³¼ ì €ì¥/ìš”ì•½ ----------
    def export_results(
        self,
        df_with_clusters: pd.DataFrame,
        output_prefix: str = "clustering",
    ) -> Dict[str, str]:
        """í´ëŸ¬ìŠ¤í„°ë§ ê²°ê³¼ ë‚´ë³´ë‚´ê¸°"""
        if not self.is_fitted:
            raise ValueError("í´ëŸ¬ìŠ¤í„°ë§ì´ ìˆ˜í–‰ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

        saved_files: Dict[str, str] = {}
        logger.info("\nğŸ’¾ í´ëŸ¬ìŠ¤í„°ë§ ê²°ê³¼ ì €ì¥ ì¤‘...")

        # 1) ê²°ê³¼ CSV
        results_file = f"{output_prefix}_results.csv"
        df_with_clusters.to_csv(results_file, index=False, encoding="utf-8-sig")
        saved_files["results"] = results_file

        # 2) í´ëŸ¬ìŠ¤í„°ë³„ í‚¤ì›Œë“œ JSON
        cluster_keywords: Dict[str, Any] = {}
        if hasattr(self.kmeans, "cluster_centers_") and self.feature_names is not None:
            for cluster_id in range(self.n_clusters):
                cluster_center = self.kmeans.cluster_centers_[cluster_id]
                top_indices = cluster_center.argsort()[-10:][::-1]
                keywords = []
                for i in top_indices:
                    if cluster_center[i] > 0.01:
                        keywords.append(
                            {
                                "keyword": self.feature_names[i],
                                "score": float(cluster_center[i]),
                            }
                        )
                cluster_keywords[f"cluster_{cluster_id}"] = keywords
        else:
            logger.warning("feature_namesê°€ ì—†ì–´ í‚¤ì›Œë“œ JSONì´ ë¹„ì–´ ìˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")

        keywords_file = f"{output_prefix}_keywords.json"
        with open(keywords_file, "w", encoding="utf-8") as f:
            json.dump(cluster_keywords, f, ensure_ascii=False, indent=2)
        saved_files["keywords"] = keywords_file

        # 3) ìš”ì•½ CSV
        summary_data = []
        for cluster_id in range(self.n_clusters):
            cluster_docs = df_with_clusters[df_with_clusters["cluster"] == cluster_id]
            summary = {
                "cluster_id": cluster_id,
                "article_count": len(cluster_docs),
                "percentage": len(cluster_docs) / len(df_with_clusters) * 100,
                "sample_titles": (
                    cluster_docs["Title"].head(3).tolist()
                    if "Title" in cluster_docs.columns
                    else []
                ),
                "top_keywords": (
                    [
                        kw["keyword"]
                        for kw in cluster_keywords.get(f"cluster_{cluster_id}", [])[:5]
                    ]
                    if cluster_keywords
                    else []
                ),
            }
            summary_data.append(summary)

        summary_file = f"{output_prefix}_summary.csv"
        pd.DataFrame(summary_data).to_csv(
            summary_file, index=False, encoding="utf-8-sig"
        )
        saved_files["summary"] = summary_file

        logger.info(f"   ğŸ“„ í´ëŸ¬ìŠ¤í„°ë§ ê²°ê³¼: {results_file}")
        logger.info(f"   ğŸ”‘ í´ëŸ¬ìŠ¤í„° í‚¤ì›Œë“œ: {keywords_file}")
        logger.info(f"   ğŸ“Š í´ëŸ¬ìŠ¤í„° ìš”ì•½: {summary_file}")

        return saved_files

    def get_cluster_summary(self) -> Dict[str, Any]:
        """í´ëŸ¬ìŠ¤í„°ë§ ìš”ì•½ ì •ë³´ ë°˜í™˜"""
        if not self.is_fitted:
            return {}
        cluster_counts = np.bincount(self.labels)
        return {
            "n_clusters": self.n_clusters,
            "total_documents": len(self.labels),
            "cluster_sizes": cluster_counts.tolist(),
            "largest_cluster": int(np.argmax(cluster_counts)),
            "smallest_cluster": int(np.argmin(cluster_counts)),
            "size_std": float(np.std(cluster_counts)),
        }


if __name__ == "__main__":
    logger.info("ğŸ“° ë‰´ìŠ¤ í´ëŸ¬ìŠ¤í„°ë§ ëª¨ë“ˆ í…ŒìŠ¤íŠ¸")

    df = pd.read_csv("processed_news.csv")

    vectorizer = TfidfVectorizer(
        max_features=1000,  # ìµœëŒ€ íŠ¹ì„± ìˆ˜
        stop_words="english",  # ì˜ì–´ ë¶ˆìš©ì–´ ì œê±°
        ngram_range=(1, 2),  # 1-gramê³¼ 2-gram ì‚¬ìš©
        min_df=2,  # ìµœì†Œ ë¬¸ì„œ ë¹ˆë„
        max_df=0.8,  # ìµœëŒ€ ë¬¸ì„œ ë¹ˆë„
    )

    tfidf_matrix = vectorizer.fit_transform(df["processed_text"])
    feature_names = vectorizer.get_feature_names_out()

    # âœ… ë‚´ë¶€ì— tfidf_matrix/feature_namesë¥¼ ë³´ê´€í•˜ì—¬ ì‚¬ìš©
    clustering = NewsClustering(
        n_clusters=3,
        auto_optimize=True,
        min_clusters=2,
        max_clusters=6,
        tfidf_matrix=tfidf_matrix,
        feature_names=feature_names,
    )

    labels = clustering.fit_predict()

    # ë¶„ì„ ê²°ê³¼
    df_with_clusters = clustering.analyze_clusters(df, top_keywords=10)

    # ì‹œê°í™” (ì˜µì…˜)
    clustering.visualize_clusters(df_with_clusters)

    # ìš”ì•½ ì •ë³´
    summary = clustering.get_cluster_summary()
    logger.info(f"\ní´ëŸ¬ìŠ¤í„°ë§ ìš”ì•½: {summary}")
