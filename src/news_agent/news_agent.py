"""
ë‰´ìŠ¤ ì—ì´ì „íŠ¸ ë©”ì¸ ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´í„°
ëª¨ë“  ë¶„ì„ ëª¨ë“ˆì„ í†µí•©í•˜ì—¬ ìì—°ì–´ ì¿¼ë¦¬ ê¸°ë°˜ ë‰´ìŠ¤ ë¶„ì„ ì‹œìŠ¤í…œ ì œê³µ
"""

import os
from datetime import datetime
from typing import Any, Dict

import pandas as pd
from loguru import logger

from src.news_agent.clustering import NewsClustering
from src.news_agent.data_collector import NewsDataCollector
from src.news_agent.llm_interface import LLMInterface
from src.news_agent.pre_processor import NewsPreprocessor
from src.news_agent.sentiment_analyzer import SentimentAnalyzer
from src.news_agent.similarity_analyzer import SimilarityAnalyzer
from src.news_agent.text_analyzer import TextAnalyzer
from src.news_agent.topic_modeler import TopicModeler


class NewsAgent:
    """í†µí•© ë‰´ìŠ¤ ë¶„ì„ ì—ì´ì „íŠ¸"""

    def __init__(
        self,
        api_key: str = "9268c1b56a644f2fa03cc5979fbfcb75",
        data_dir: str = "news_data",
    ):
        """
        Args:
            api_key: NewsAPI í‚¤
            data_dir: ë°ì´í„° ì €ì¥ ë””ë ‰í† ë¦¬
        """
        self.api_key = api_key
        self.data_dir = data_dir

        # ë°ì´í„° ë””ë ‰í† ë¦¬ ìƒì„±
        os.makedirs(data_dir, exist_ok=True)

        # ë¶„ì„ ëª¨ë“ˆë“¤ ì´ˆê¸°í™”
        self.data_collector = NewsDataCollector(api_key)
        self.preprocessor = NewsPreprocessor()
        self.text_analyzer = TextAnalyzer()
        self.clustering = NewsClustering()
        self.topic_modeler = TopicModeler()
        self.sentiment_analyzer = SentimentAnalyzer()
        self.similarity_analyzer = SimilarityAnalyzer()
        self.llm_interface = LLMInterface()

        # í˜„ì¬ ë°ì´í„°
        self.current_data = None
        self.processed_data = None
        self.tfidf_matrix = None

        # LLM ì¸í„°í˜ì´ìŠ¤ì— í•¨ìˆ˜ë“¤ ë“±ë¡
        self._register_functions()

    def _register_functions(self):
        """LLM ì¸í„°í˜ì´ìŠ¤ì— ë¶„ì„ í•¨ìˆ˜ë“¤ ë“±ë¡"""
        self.llm_interface.register_function("collect_news", self._collect_news_wrapper)
        self.llm_interface.register_function(
            "analyze_keywords", self._analyze_keywords_wrapper
        )
        self.llm_interface.register_function(
            "analyze_sentiment", self._analyze_sentiment_wrapper
        )
        self.llm_interface.register_function(
            "perform_clustering", self._perform_clustering_wrapper
        )
        self.llm_interface.register_function("model_topics", self._model_topics_wrapper)
        self.llm_interface.register_function(
            "analyze_similarity", self._analyze_similarity_wrapper
        )
        self.llm_interface.register_function(
            "run_comprehensive_analysis", self._run_comprehensive_analysis_wrapper
        )

    def _collect_news_wrapper(
        self, n_articles: int = 20, category: str = "technology", **kwargs
    ):
        """ë‰´ìŠ¤ ìˆ˜ì§‘ ë˜í¼ í•¨ìˆ˜"""
        return self.collect_news(n_articles=n_articles, category=category)

    def _analyze_keywords_wrapper(self, n_keywords: int = 20, **kwargs):
        """í‚¤ì›Œë“œ ë¶„ì„ ë˜í¼ í•¨ìˆ˜"""
        return self.analyze_keywords(n_keywords=n_keywords)

    def _analyze_sentiment_wrapper(self, **kwargs):
        """ê°ì • ë¶„ì„ ë˜í¼ í•¨ìˆ˜"""
        return self.analyze_sentiment()

    def _perform_clustering_wrapper(self, n_clusters: int = 5, **kwargs):
        """í´ëŸ¬ìŠ¤í„°ë§ ë˜í¼ í•¨ìˆ˜"""
        return self.perform_clustering(n_clusters=n_clusters)

    def _model_topics_wrapper(self, n_topics: int = 5, **kwargs):
        """í† í”½ ëª¨ë¸ë§ ë˜í¼ í•¨ìˆ˜"""
        return self.model_topics(n_topics=n_topics)

    def _analyze_similarity_wrapper(self, **kwargs):
        """ìœ ì‚¬ë„ ë¶„ì„ ë˜í¼ í•¨ìˆ˜"""
        return self.analyze_similarity()

    def _run_comprehensive_analysis_wrapper(self, **kwargs):
        """ì¢…í•© ë¶„ì„ ë˜í¼ í•¨ìˆ˜"""
        return self.run_comprehensive_analysis()

    def collect_news(
        self,
        n_articles: int = 20,
        category: str = "technology",
        include_content: bool = True,
    ) -> Dict[str, Any]:
        """
        ë‰´ìŠ¤ ë°ì´í„° ìˆ˜ì§‘

        Args:
            n_articles: ìˆ˜ì§‘í•  ê¸°ì‚¬ ìˆ˜
            category: ë‰´ìŠ¤ ì¹´í…Œê³ ë¦¬
            include_content: ìƒì„¸ ë‚´ìš© í¬í•¨ ì—¬ë¶€

        Returns:
            ìˆ˜ì§‘ ê²°ê³¼
        """
        logger.info(f"\nğŸ“° ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹œì‘ ({category} ì¹´í…Œê³ ë¦¬, {n_articles}ê°œ ê¸°ì‚¬)")

        try:
            # ë‰´ìŠ¤ ìˆ˜ì§‘
            df = self.data_collector.collect_and_process(
                category=category,
                num_articles=n_articles,
                include_content=include_content,
            )

            if df.empty:
                return {"success": False, "message": "ìˆ˜ì§‘ëœ ë‰´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤."}

            # ë°ì´í„° ì €ì¥
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"{self.data_dir}/news_{category}_{timestamp}.csv"
            df.to_csv(filename, index=False, encoding="utf-8-sig")

            self.current_data = df
            logger.success(f" ë‰´ìŠ¤ ìˆ˜ì§‘ ì™„ë£Œ: {len(df)}ê°œ ê¸°ì‚¬")
            logger.info(f" ì €ì¥ë¨: {filename}")

            return {
                "success": True,
                "count": len(df),
                "filename": filename,
                "message": f"{len(df)}ê°œ ë‰´ìŠ¤ ê¸°ì‚¬ê°€ ìˆ˜ì§‘ë˜ì—ˆìŠµë‹ˆë‹¤.",
            }

        except Exception as e:
            return {
                "success": False,
                "error": str(e),
                "message": f"ë‰´ìŠ¤ ìˆ˜ì§‘ ì¤‘ ì˜¤ë¥˜: {e}",
            }

    def preprocess_data(self) -> Dict[str, Any]:
        """
        ë°ì´í„° ì „ì²˜ë¦¬

        Returns:
            ì „ì²˜ë¦¬ ê²°ê³¼
        """
        if self.current_data is None:
            return {"success": False, "message": "ë¨¼ì € ë‰´ìŠ¤ ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•´ì£¼ì„¸ìš”."}

        logger.info("\nğŸ”„ ë°ì´í„° ì „ì²˜ë¦¬ ì‹œì‘")

        try:
            # ì „ì²˜ë¦¬ ìˆ˜í–‰
            processed_df = self.preprocessor.preprocess_dataframe(self.current_data)

            if processed_df.empty:
                return {
                    "success": False,
                    "message": "ì „ì²˜ë¦¬ í›„ ìœ íš¨í•œ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.",
                }

            self.processed_data = processed_df

            # ì „ì²˜ë¦¬ëœ ë°ì´í„° ì €ì¥
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"{self.data_dir}/news_processed_{timestamp}.csv"
            processed_df.to_csv(filename, index=False, encoding="utf-8-sig")

            logger.success(f" ì „ì²˜ë¦¬ ì™„ë£Œ: {len(processed_df)}ê°œ ê¸°ì‚¬")

            return {
                "success": True,
                "count": len(processed_df),
                "filename": filename,
                "message": "ë°ì´í„° ì „ì²˜ë¦¬ê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.",
            }

        except Exception as e:
            return {
                "success": False,
                "error": str(e),
                "message": f"ì „ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}",
            }

    def analyze_keywords(self, n_keywords: int = 20) -> Dict[str, Any]:
        """
        TF-IDF í‚¤ì›Œë“œ ë¶„ì„

        Args:
            n_keywords: ì¶”ì¶œí•  í‚¤ì›Œë“œ ìˆ˜

        Returns:
            í‚¤ì›Œë“œ ë¶„ì„ ê²°ê³¼
        """
        if not self._ensure_processed_data():
            return {"success": False, "message": "ì „ì²˜ë¦¬ëœ ë°ì´í„°ê°€ í•„ìš”í•©ë‹ˆë‹¤."}

        logger.info(f"\nğŸ” TF-IDF í‚¤ì›Œë“œ ë¶„ì„ ì‹œì‘ (ìƒìœ„ {n_keywords}ê°œ)")

        try:
            # TF-IDF ë¶„ì„
            texts = self.processed_data["processed_text"].tolist()
            self.tfidf_matrix = self.text_analyzer.fit_transform(texts)

            # í‚¤ì›Œë“œ ì¶”ì¶œ
            keywords = self.text_analyzer.get_top_keywords_global(n_keywords)

            # ê²°ê³¼ ì €ì¥
            results = self.text_analyzer.export_analysis_results(
                self.processed_data, output_prefix=f"{self.data_dir}/keywords_analysis"
            )

            logger.success(" í‚¤ì›Œë“œ ë¶„ì„ ì™„ë£Œ")

            return {
                "success": True,
                "keywords": keywords[:10],  # ìƒìœ„ 10ê°œë§Œ ë°˜í™˜
                "total_keywords": len(keywords),
                "files": results,
                "message": f"ìƒìœ„ {n_keywords}ê°œ í‚¤ì›Œë“œê°€ ë¶„ì„ë˜ì—ˆìŠµë‹ˆë‹¤.",
            }

        except Exception as e:
            return {
                "success": False,
                "error": str(e),
                "message": f"í‚¤ì›Œë“œ ë¶„ì„ ì¤‘ ì˜¤ë¥˜: {e}",
            }

    def analyze_sentiment(self) -> Dict[str, Any]:
        """
        ê°ì • ë¶„ì„

        Returns:
            ê°ì • ë¶„ì„ ê²°ê³¼
        """
        if not self._ensure_processed_data():
            return {"success": False, "message": "ì „ì²˜ë¦¬ëœ ë°ì´í„°ê°€ í•„ìš”í•©ë‹ˆë‹¤."}

        logger.info("\nğŸ˜Š ê°ì • ë¶„ì„ ì‹œì‘")

        try:
            # ê°ì • ë¶„ì„ ìˆ˜í–‰
            sentiment_df = self.sentiment_analyzer.analyze_dataframe(
                self.processed_data
            )

            # í†µê³„ ê³„ì‚°
            stats = self.sentiment_analyzer.get_sentiment_statistics(sentiment_df)

            # ê²°ê³¼ ì €ì¥
            results = self.sentiment_analyzer.export_results(
                sentiment_df, output_prefix=f"{self.data_dir}/sentiment_analysis"
            )

            logger.success(" ê°ì • ë¶„ì„ ì™„ë£Œ")

            return {
                "success": True,
                "statistics": stats,
                "files": results,
                "message": "ê°ì • ë¶„ì„ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.",
            }

        except Exception as e:
            return {
                "success": False,
                "error": str(e),
                "message": f"ê°ì • ë¶„ì„ ì¤‘ ì˜¤ë¥˜: {e}",
            }

    def perform_clustering(self, n_clusters: int = 5) -> Dict[str, Any]:
        """
        K-Means í´ëŸ¬ìŠ¤í„°ë§

        Args:
            n_clusters: í´ëŸ¬ìŠ¤í„° ìˆ˜

        Returns:
            í´ëŸ¬ìŠ¤í„°ë§ ê²°ê³¼
        """
        if not self._ensure_tfidf_matrix():
            return {
                "success": False,
                "message": "TF-IDF í–‰ë ¬ì´ í•„ìš”í•©ë‹ˆë‹¤. ë¨¼ì € í‚¤ì›Œë“œ ë¶„ì„ì„ ì‹¤í–‰í•´ì£¼ì„¸ìš”.",
            }

        logger.info(f"\nğŸ¯ K-Means í´ëŸ¬ìŠ¤í„°ë§ ì‹œì‘ ({n_clusters}ê°œ í´ëŸ¬ìŠ¤í„°)")

        try:
            # í´ëŸ¬ìŠ¤í„°ë§ ê°ì²´ ìƒˆë¡œ ìƒì„± (ìˆ˜ì •ëœ í´ë˜ìŠ¤ ì‚¬ìš©)
            feature_names = self.text_analyzer.feature_names
            self.clustering = NewsClustering(
                n_clusters=n_clusters,
                tfidf_matrix=self.tfidf_matrix,
                feature_names=feature_names,
            )

            # í´ëŸ¬ìŠ¤í„°ë§ ìˆ˜í–‰
            labels = self.clustering.fit_predict()
            logger.info(f"í´ëŸ¬ìŠ¤í„° ë¼ë²¨ ìƒ˜í”Œ: {labels[:10]}")

            # í´ëŸ¬ìŠ¤í„° ë¶„ì„
            clustered_df = self.clustering.analyze_clusters(self.processed_data)

            # ê²°ê³¼ ì €ì¥
            results = self.clustering.export_results(
                clustered_df, output_prefix=f"{self.data_dir}/clustering"
            )

            # ìš”ì•½ ì •ë³´
            summary = self.clustering.get_cluster_summary()

            logger.success(" í´ëŸ¬ìŠ¤í„°ë§ ì™„ë£Œ")

            return {
                "success": True,
                "summary": summary,
                "files": results,
                "message": f"{n_clusters}ê°œ í´ëŸ¬ìŠ¤í„°ë¡œ ë¶„ë¥˜ê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.",
            }

        except Exception as e:
            return {
                "success": False,
                "error": str(e),
                "message": f"í´ëŸ¬ìŠ¤í„°ë§ ì¤‘ ì˜¤ë¥˜: {e}",
            }

    def model_topics(self, n_topics: int = 5) -> Dict[str, Any]:
        """
        LDA í† í”½ ëª¨ë¸ë§

        Args:
            n_topics: í† í”½ ìˆ˜

        Returns:
            í† í”½ ëª¨ë¸ë§ ê²°ê³¼
        """
        if not self._ensure_processed_data():
            return {"success": False, "message": "ì „ì²˜ë¦¬ëœ ë°ì´í„°ê°€ í•„ìš”í•©ë‹ˆë‹¤."}

        logger.info(f"\nğŸ­ LDA í† í”½ ëª¨ë¸ë§ ì‹œì‘ ({n_topics}ê°œ í† í”½)")

        try:
            # í† í”½ ëª¨ë¸ë§ ìˆ˜í–‰
            self.topic_modeler.n_topics = n_topics
            texts = self.processed_data["processed_text"].tolist()
            doc_topic_probs = self.topic_modeler.fit_transform(texts)
            logger.info(doc_topic_probs[:10])

            # í† í”½ ë¶„ì„
            topic_df = self.topic_modeler.analyze_topics(self.processed_data)

            # ê²°ê³¼ ì €ì¥
            results = self.topic_modeler.export_results(
                topic_df, output_prefix=f"{self.data_dir}/topic_modeling"
            )

            # ëª¨ë¸ ì •ë³´
            model_info = self.topic_modeler.get_model_info()

            logger.success(" í† í”½ ëª¨ë¸ë§ ì™„ë£Œ")

            return {
                "success": True,
                "model_info": model_info,
                "files": results,
                "message": f"{n_topics}ê°œ í† í”½ì´ ì¶”ì¶œë˜ì—ˆìŠµë‹ˆë‹¤.",
            }

        except Exception as e:
            return {
                "success": False,
                "error": str(e),
                "message": f"í† í”½ ëª¨ë¸ë§ ì¤‘ ì˜¤ë¥˜: {e}",
            }

    def analyze_similarity(self) -> Dict[str, Any]:
        """
        ë¬¸ì„œ ìœ ì‚¬ë„ ë¶„ì„

        Returns:
            ìœ ì‚¬ë„ ë¶„ì„ ê²°ê³¼
        """
        if not self._ensure_tfidf_matrix():
            return {
                "success": False,
                "message": "TF-IDF í–‰ë ¬ì´ í•„ìš”í•©ë‹ˆë‹¤. ë¨¼ì € í‚¤ì›Œë“œ ë¶„ì„ì„ ì‹¤í–‰í•´ì£¼ì„¸ìš”.",
            }

        logger.info("\nğŸ”— ë¬¸ì„œ ìœ ì‚¬ë„ ë¶„ì„ ì‹œì‘")

        try:
            # ìœ ì‚¬ë„ ë¶„ì„ ìˆ˜í–‰
            self.similarity_analyzer.set_tfidf_matrix(self.tfidf_matrix)
            similarity_matrix = self.similarity_analyzer.compute_similarity_matrix()
            logger.info(similarity_matrix[:10])

            # í†µê³„ ê³„ì‚°
            stats = self.similarity_analyzer.get_similarity_statistics()

            # ìœ ì‚¬í•œ ë¬¸ì„œ ìŒ ì°¾ê¸°
            similar_pairs = self.similarity_analyzer.find_most_similar_pairs(
                self.processed_data, 5
            )

            # ê²°ê³¼ ì €ì¥
            results = self.similarity_analyzer.export_results(
                self.processed_data,
                output_prefix=f"{self.data_dir}/similarity_analysis",
            )

            logger.success(" ìœ ì‚¬ë„ ë¶„ì„ ì™„ë£Œ")

            return {
                "success": True,
                "statistics": stats,
                "similar_pairs": similar_pairs[:3],  # ìƒìœ„ 3ê°œë§Œ ë°˜í™˜
                "files": results,
                "message": "ë¬¸ì„œ ìœ ì‚¬ë„ ë¶„ì„ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.",
            }

        except Exception as e:
            return {
                "success": False,
                "error": str(e),
                "message": f"ìœ ì‚¬ë„ ë¶„ì„ ì¤‘ ì˜¤ë¥˜: {e}",
            }

    def run_comprehensive_analysis(self) -> Dict[str, Any]:
        """
        ì¢…í•©ì ì¸ ë‰´ìŠ¤ ë¶„ì„ ì‹¤í–‰

        Returns:
            ì „ì²´ ë¶„ì„ ê²°ê³¼
        """
        logger.info("\nğŸš€ ì¢…í•© ë‰´ìŠ¤ ë¶„ì„ ì‹œì‘")

        results = {}

        # 1. ë°ì´í„° ì „ì²˜ë¦¬ (í•„ìš”í•œ ê²½ìš°)
        if self.processed_data is None:
            logger.info("ğŸ“‹ 1/6: ë°ì´í„° ì „ì²˜ë¦¬")
            results["preprocessing"] = self.preprocess_data()

        # 2. í‚¤ì›Œë“œ ë¶„ì„
        logger.info("ğŸ“‹ 2/6: í‚¤ì›Œë“œ ë¶„ì„")
        results["keywords"] = self.analyze_keywords(20)

        # 3. ê°ì • ë¶„ì„
        logger.info("ğŸ“‹ 3/6: ê°ì • ë¶„ì„")
        results["sentiment"] = self.analyze_sentiment()

        # 4. í´ëŸ¬ìŠ¤í„°ë§
        logger.info("ğŸ“‹ 4/6: í´ëŸ¬ìŠ¤í„°ë§")
        results["clustering"] = self.perform_clustering(5)

        # 5. í† í”½ ëª¨ë¸ë§
        logger.info("ğŸ“‹ 5/6: í† í”½ ëª¨ë¸ë§")
        results["topics"] = self.model_topics(5)

        # 6. ìœ ì‚¬ë„ ë¶„ì„
        logger.info("ğŸ“‹ 6/6: ìœ ì‚¬ë„ ë¶„ì„")
        results["similarity"] = self.analyze_similarity()

        # ì¢…í•© ë¦¬í¬íŠ¸ ìƒì„±
        success_count = sum(1 for r in results.values() if r.get("success", False))
        total_count = len(results)

        logger.info(f"\nâœ… ì¢…í•© ë¶„ì„ ì™„ë£Œ: {success_count}/{total_count} ì„±ê³µ")

        return {
            "success": success_count > 0,
            "completed_analyses": success_count,
            "total_analyses": total_count,
            "results": results,
            "message": f"ì¢…í•© ë¶„ì„ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤. ({success_count}/{total_count} ì„±ê³µ)",
        }

    def process_natural_query(self, query: str) -> Dict[str, Any]:
        """
        ìì—°ì–´ ì¿¼ë¦¬ ì²˜ë¦¬

        Args:
            query: ì‚¬ìš©ìì˜ ìì—°ì–´ ì¿¼ë¦¬

        Returns:
            ì¿¼ë¦¬ ì²˜ë¦¬ ê²°ê³¼
        """
        logger.info(f"\nğŸ¤– ìì—°ì–´ ì¿¼ë¦¬ ì²˜ë¦¬: '{query}'")

        try:
            result = self.llm_interface.process_query(query)
            return result
        except Exception as e:
            return {
                "success": False,
                "error": str(e),
                "message": f"ì¿¼ë¦¬ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}",
            }

    def get_status(self) -> Dict[str, Any]:
        """
        í˜„ì¬ ìƒíƒœ ì •ë³´ ë°˜í™˜

        Returns:
            ì‹œìŠ¤í…œ ìƒíƒœ ì •ë³´
        """
        return {
            "has_raw_data": self.current_data is not None,
            "has_processed_data": self.processed_data is not None,
            "has_tfidf_matrix": self.tfidf_matrix is not None,
            "raw_data_count": (
                len(self.current_data) if self.current_data is not None else 0
            ),
            "processed_data_count": (
                len(self.processed_data) if self.processed_data is not None else 0
            ),
            "data_directory": self.data_dir,
        }

    def _ensure_processed_data(self) -> bool:
        """ì „ì²˜ë¦¬ëœ ë°ì´í„° í™•ì¸ ë° ìë™ ì²˜ë¦¬"""
        if self.processed_data is None:
            if self.current_data is None:
                return False
            self.preprocess_data()
        return self.processed_data is not None

    def _ensure_tfidf_matrix(self) -> bool:
        """TF-IDF í–‰ë ¬ í™•ì¸"""
        if self.tfidf_matrix is None:
            if not self._ensure_processed_data():
                return False
            # ìë™ìœ¼ë¡œ í‚¤ì›Œë“œ ë¶„ì„ ì‹¤í–‰
            result = self.analyze_keywords()
            return result.get("success", False)
        return True

    def get_help(self) -> str:
        """ë„ì›€ë§ ë°˜í™˜"""
        return self.llm_interface.get_help_message()

    def load_data_from_file(self, filename: str) -> Dict[str, Any]:
        """
        íŒŒì¼ì—ì„œ ë°ì´í„° ë¡œë“œ

        Args:
            filename: CSV íŒŒì¼ ê²½ë¡œ

        Returns:
            ë¡œë“œ ê²°ê³¼
        """
        try:
            df = pd.read_csv(filename)
            self.current_data = df

            # ì „ì²˜ë¦¬ëœ ë°ì´í„°ì¸ì§€ í™•ì¸
            if "processed_text" in df.columns:
                self.processed_data = df

            return {
                "success": True,
                "count": len(df),
                "message": f"{len(df)}ê°œ ê¸°ì‚¬ë¥¼ íŒŒì¼ì—ì„œ ë¡œë“œí–ˆìŠµë‹ˆë‹¤.",
            }
        except Exception as e:
            return {
                "success": False,
                "error": str(e),
                "message": f"íŒŒì¼ ë¡œë“œ ì¤‘ ì˜¤ë¥˜: {e}",
            }

    def run_full_system_test(
        self,
        test_articles: int = 5,
        test_category: str = "technology",
        save_results: bool = True,
    ) -> Dict[str, Any]:
        """
        ëª¨ë“  ë¶„ì„ ëª¨ë“ˆì´ ì •ìƒ ë™ì‘í•˜ëŠ”ì§€ í™•ì¸í•˜ëŠ” ì¢…í•© í…ŒìŠ¤íŠ¸ í•¨ìˆ˜

        Args:
            test_articles: í…ŒìŠ¤íŠ¸ìš© ê¸°ì‚¬ ìˆ˜ (ê¸°ë³¸ê°’: 5ê°œ)
            test_category: í…ŒìŠ¤íŠ¸ ì¹´í…Œê³ ë¦¬ (ê¸°ë³¸ê°’: technology)
            save_results: ê²°ê³¼ íŒŒì¼ ì €ì¥ ì—¬ë¶€

        Returns:
            í…ŒìŠ¤íŠ¸ ê²°ê³¼ ìƒì„¸ ë³´ê³ ì„œ
        """
        logger.info("\nğŸ§ª ë‰´ìŠ¤ ì—ì´ì „íŠ¸ ì‹œìŠ¤í…œ ì¢…í•© í…ŒìŠ¤íŠ¸ ì‹œì‘")
        logger.info(
            f"   í…ŒìŠ¤íŠ¸ ì„¤ì •: {test_articles}ê°œ ê¸°ì‚¬, ì¹´í…Œê³ ë¦¬: {test_category}"
        )
        logger.info("=" * 60)

        test_results = {
            "overall_success": False,
            "test_timestamp": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
            "test_config": {
                "articles": test_articles,
                "category": test_category,
                "save_results": save_results,
            },
            "module_tests": {},
            "performance_metrics": {},
            "error_log": [],
        }

        start_time = datetime.now()

        try:
            # 1. ë°ì´í„° ìˆ˜ì§‘ í…ŒìŠ¤íŠ¸
            logger.info("ğŸ” 1/7: ë°ì´í„° ìˆ˜ì§‘ ëª¨ë“ˆ í…ŒìŠ¤íŠ¸")
            collect_start = datetime.now()
            collect_result = self.collect_news(
                n_articles=test_articles,
                category=test_category,
                include_content=False,  # ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ë¥¼ ìœ„í•´ ìƒì„¸ ë‚´ìš© ì œì™¸
            )
            collect_time = (datetime.now() - collect_start).total_seconds()

            test_results["module_tests"]["data_collection"] = {
                "success": collect_result.get("success", False),
                "execution_time_seconds": collect_time,
                "articles_collected": collect_result.get("count", 0),
                "result": collect_result,
            }

            if not collect_result.get("success"):
                test_results["error_log"].append("ë°ì´í„° ìˆ˜ì§‘ ì‹¤íŒ¨")
                logger.error(f"   âŒ ë°ì´í„° ìˆ˜ì§‘ ì‹¤íŒ¨: {collect_result.get('message')}")
                return test_results

            logger.success(
                f"   âœ… ë°ì´í„° ìˆ˜ì§‘ ì„±ê³µ ({collect_result.get('count')}ê°œ ê¸°ì‚¬, {collect_time:.1f}ì´ˆ)"  # noqa: E501
            )

            # 2. ë°ì´í„° ì „ì²˜ë¦¬ í…ŒìŠ¤íŠ¸
            logger.info("ğŸ”„ 2/7: ë°ì´í„° ì „ì²˜ë¦¬ ëª¨ë“ˆ í…ŒìŠ¤íŠ¸")
            preprocess_start = datetime.now()
            preprocess_result = self.preprocess_data()
            preprocess_time = (datetime.now() - preprocess_start).total_seconds()

            test_results["module_tests"]["preprocessing"] = {
                "success": preprocess_result.get("success", False),
                "execution_time_seconds": preprocess_time,
                "processed_articles": preprocess_result.get("count", 0),
                "result": preprocess_result,
            }

            if not preprocess_result.get("success"):
                test_results["error_log"].append("ë°ì´í„° ì „ì²˜ë¦¬ ì‹¤íŒ¨")
                logger.error(f"   âŒ ì „ì²˜ë¦¬ ì‹¤íŒ¨: {preprocess_result.get('message')}")
                return test_results

            logger.success(
                f"   âœ… ë°ì´í„° ì „ì²˜ë¦¬ ì„±ê³µ ({preprocess_result.get('count')}ê°œ ê¸°ì‚¬, {preprocess_time:.1f}ì´ˆ)"  # noqa: E501
            )

            # 3. TF-IDF í‚¤ì›Œë“œ ë¶„ì„ í…ŒìŠ¤íŠ¸
            logger.info("ğŸ” 3/7: TF-IDF í‚¤ì›Œë“œ ë¶„ì„ í…ŒìŠ¤íŠ¸")
            keywords_start = datetime.now()
            keywords_result = self.analyze_keywords(n_keywords=10)
            keywords_time = (datetime.now() - keywords_start).total_seconds()

            test_results["module_tests"]["keywords_analysis"] = {
                "success": keywords_result.get("success", False),
                "execution_time_seconds": keywords_time,
                "keywords_found": keywords_result.get("total_keywords", 0),
                "top_keywords": keywords_result.get("keywords", [])[:3],  # ìƒìœ„ 3ê°œë§Œ
                "result": keywords_result,
            }

            if not keywords_result.get("success"):
                test_results["error_log"].append("í‚¤ì›Œë“œ ë¶„ì„ ì‹¤íŒ¨")
                logger.error(
                    f"   âŒ í‚¤ì›Œë“œ ë¶„ì„ ì‹¤íŒ¨: {keywords_result.get('message')}"
                )
            else:
                logger.success(
                    f"   âœ… í‚¤ì›Œë“œ ë¶„ì„ ì„±ê³µ ({keywords_result.get('total_keywords')}ê°œ í‚¤ì›Œë“œ, {keywords_time:.1f}ì´ˆ)"  # noqa: E501
                )

            # 4. ê°ì • ë¶„ì„ í…ŒìŠ¤íŠ¸
            logger.info("ğŸ˜Š 4/7: ê°ì • ë¶„ì„ í…ŒìŠ¤íŠ¸")
            sentiment_start = datetime.now()
            sentiment_result = self.analyze_sentiment()
            sentiment_time = (datetime.now() - sentiment_start).total_seconds()

            test_results["module_tests"]["sentiment_analysis"] = {
                "success": sentiment_result.get("success", False),
                "execution_time_seconds": sentiment_time,
                "sentiment_stats": sentiment_result.get("statistics", {}),
                "result": sentiment_result,
            }

            if not sentiment_result.get("success"):
                test_results["error_log"].append("ê°ì • ë¶„ì„ ì‹¤íŒ¨")
                logger.error(f"   âŒ ê°ì • ë¶„ì„ ì‹¤íŒ¨: {sentiment_result.get('message')}")
            else:
                stats = sentiment_result.get("statistics", {})
                logger.success(
                    f"   âœ… ê°ì • ë¶„ì„ ì„±ê³µ (ê¸ì •: {stats.get('positive_ratio', 0):.1%}, {sentiment_time:.1f}ì´ˆ)"  # noqa: E501
                )

            # 5. í´ëŸ¬ìŠ¤í„°ë§ í…ŒìŠ¤íŠ¸
            logger.info("ğŸ¯ 5/7: K-Means í´ëŸ¬ìŠ¤í„°ë§ í…ŒìŠ¤íŠ¸")
            clustering_start = datetime.now()

            # í´ëŸ¬ìŠ¤í„°ë§ ê°ì²´ë¥¼ ìƒˆë¡œ ì´ˆê¸°í™” (ìˆ˜ì •ëœ í´ë˜ìŠ¤ ì‚¬ìš©)
            if self.tfidf_matrix is not None:
                feature_names = self.text_analyzer.feature_names
                self.clustering.set_features(self.tfidf_matrix, feature_names)
                self.clustering.n_clusters = min(
                    3, len(self.processed_data) // 2
                )  # ë°ì´í„° ìˆ˜ì— ë§ê²Œ ì¡°ì •

                clustering_result = {}
                try:
                    labels = self.clustering.fit_predict()
                    logger.info(f"í´ëŸ¬ìŠ¤í„° ë¼ë²¨ ìƒ˜í”Œ: {labels[:10]}")
                    clustered_df = self.clustering.analyze_clusters(
                        self.processed_data, top_keywords=5
                    )
                    summary = self.clustering.get_cluster_summary()

                    clustering_result = {
                        "success": True,
                        "summary": summary,
                        "message": f"{self.clustering.n_clusters}ê°œ í´ëŸ¬ìŠ¤í„°ë¡œ ë¶„ë¥˜ ì™„ë£Œ",
                    }

                    if save_results:
                        export_results = self.clustering.export_results(
                            clustered_df,
                            output_prefix=f"{self.data_dir}/test_clustering",
                        )
                        clustering_result["files"] = export_results

                except Exception as e:
                    clustering_result = {
                        "success": False,
                        "error": str(e),
                        "message": f"í´ëŸ¬ìŠ¤í„°ë§ ì¤‘ ì˜¤ë¥˜: {e}",
                    }
            else:
                clustering_result = {
                    "success": False,
                    "message": "TF-IDF í–‰ë ¬ì´ ì—†ì–´ í´ëŸ¬ìŠ¤í„°ë§ì„ ê±´ë„ˆëœë‹ˆë‹¤.",
                }

            clustering_time = (datetime.now() - clustering_start).total_seconds()

            test_results["module_tests"]["clustering"] = {
                "success": clustering_result.get("success", False),
                "execution_time_seconds": clustering_time,
                "clusters_created": clustering_result.get("summary", {}).get(
                    "n_clusters", 0
                ),
                "result": clustering_result,
            }

            if not clustering_result.get("success"):
                test_results["error_log"].append("í´ëŸ¬ìŠ¤í„°ë§ ì‹¤íŒ¨")
                logger.error(
                    f"   âŒ í´ëŸ¬ìŠ¤í„°ë§ ì‹¤íŒ¨: {clustering_result.get('message')}"
                )
            else:
                n_clusters = clustering_result.get("summary", {}).get("n_clusters", 0)
                logger.success(
                    f"   âœ… í´ëŸ¬ìŠ¤í„°ë§ ì„±ê³µ ({n_clusters}ê°œ í´ëŸ¬ìŠ¤í„°, {clustering_time:.1f}ì´ˆ)"
                )

            # 6. í† í”½ ëª¨ë¸ë§ í…ŒìŠ¤íŠ¸
            logger.info("ğŸ­ 6/7: LDA í† í”½ ëª¨ë¸ë§ í…ŒìŠ¤íŠ¸")
            topics_start = datetime.now()
            topics_result = self.model_topics(
                n_topics=min(3, len(self.processed_data) // 3)
            )  # ë°ì´í„° ìˆ˜ì— ë§ê²Œ ì¡°ì •
            topics_time = (datetime.now() - topics_start).total_seconds()

            test_results["module_tests"]["topic_modeling"] = {
                "success": topics_result.get("success", False),
                "execution_time_seconds": topics_time,
                "topics_found": topics_result.get("model_info", {}).get("n_topics", 0),
                "perplexity": topics_result.get("model_info", {}).get("perplexity", 0),
                "result": topics_result,
            }

            if not topics_result.get("success"):
                test_results["error_log"].append("í† í”½ ëª¨ë¸ë§ ì‹¤íŒ¨")
                logger.error(f"   âŒ í† í”½ ëª¨ë¸ë§ ì‹¤íŒ¨: {topics_result.get('message')}")
            else:
                n_topics = topics_result.get("model_info", {}).get("n_topics", 0)
                perplexity = topics_result.get("model_info", {}).get("perplexity", 0)
                logger.success(
                    f"   âœ… í† í”½ ëª¨ë¸ë§ ì„±ê³µ ({n_topics}ê°œ í† í”½, Perplexity: {perplexity:.1f}, {topics_time:.1f}ì´ˆ)"  # noqa: E501
                )

            # 7. ìœ ì‚¬ë„ ë¶„ì„ í…ŒìŠ¤íŠ¸
            logger.info("ğŸ”— 7/7: ë¬¸ì„œ ìœ ì‚¬ë„ ë¶„ì„ í…ŒìŠ¤íŠ¸")
            similarity_start = datetime.now()
            similarity_result = self.analyze_similarity()
            similarity_time = (datetime.now() - similarity_start).total_seconds()

            test_results["module_tests"]["similarity_analysis"] = {
                "success": similarity_result.get("success", False),
                "execution_time_seconds": similarity_time,
                "similarity_stats": similarity_result.get("statistics", {}),
                "similar_pairs_found": len(similarity_result.get("similar_pairs", [])),
                "result": similarity_result,
            }

            if not similarity_result.get("success"):
                test_results["error_log"].append("ìœ ì‚¬ë„ ë¶„ì„ ì‹¤íŒ¨")
                logger.error(
                    f"   âŒ ìœ ì‚¬ë„ ë¶„ì„ ì‹¤íŒ¨: {similarity_result.get('message')}"
                )
            else:
                avg_similarity = similarity_result.get("statistics", {}).get(
                    "mean_similarity", 0
                )
                logger.success(
                    f"   âœ… ìœ ì‚¬ë„ ë¶„ì„ ì„±ê³µ (í‰ê·  ìœ ì‚¬ë„: {avg_similarity:.3f}, {similarity_time:.1f}ì´ˆ)"  # noqa: E501
                )

            # ì„±ê³¼ ì¸¡ì • ë° ìš”ì•½
            total_time = (datetime.now() - start_time).total_seconds()
            successful_tests = sum(
                1 for test in test_results["module_tests"].values() if test["success"]
            )
            total_tests = len(test_results["module_tests"])

            test_results["performance_metrics"] = {
                "total_execution_time_seconds": total_time,
                "successful_tests": successful_tests,
                "total_tests": total_tests,
                "success_rate": (
                    successful_tests / total_tests if total_tests > 0 else 0
                ),
                "articles_processed": test_articles,
                "throughput_articles_per_second": (
                    test_articles / total_time if total_time > 0 else 0
                ),
            }

            test_results["overall_success"] = (
                successful_tests >= 6
            )  # 7ê°œ ì¤‘ 6ê°œ ì´ìƒ ì„±ê³µ

            # ê²°ê³¼ ìš”ì•½ ì¶œë ¥
            logger.info("\n" + "=" * 60)
            logger.info("ğŸ ë‰´ìŠ¤ ì—ì´ì „íŠ¸ ì‹œìŠ¤í…œ ì¢…í•© í…ŒìŠ¤íŠ¸ ì™„ë£Œ")
            logger.info("=" * 60)

            if test_results["overall_success"]:
                logger.success(
                    f"âœ… ì „ì²´ í…ŒìŠ¤íŠ¸ ì„±ê³µ! ({successful_tests}/{total_tests} ëª¨ë“ˆ í†µê³¼)"
                )
            else:
                logger.warning(
                    f"âš ï¸ ì¼ë¶€ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨ ({successful_tests}/{total_tests} ëª¨ë“ˆ í†µê³¼)"
                )

            logger.info("ğŸ“Š ì„±ëŠ¥ ì§€í‘œ:")
            logger.info(f"   â€¢ ì´ ì‹¤í–‰ ì‹œê°„: {total_time:.1f}ì´ˆ")
            logger.info(f"   â€¢ ì²˜ë¦¬ ì†ë„: {test_articles / total_time:.2f} ê¸°ì‚¬/ì´ˆ")
            logger.info(f"   â€¢ ì„±ê³µë¥ : {successful_tests / total_tests:.1%}")

            if test_results["error_log"]:
                logger.info(f"âŒ ì‹¤íŒ¨í•œ ëª¨ë“ˆ: {', '.join(test_results['error_log'])}")

            # í…ŒìŠ¤íŠ¸ ê²°ê³¼ íŒŒì¼ ì €ì¥
            if save_results:
                timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                result_filename = f"{self.data_dir}/system_test_result_{timestamp}.json"

                import json

                with open(result_filename, "w", encoding="utf-8") as f:
                    json.dump(
                        test_results, f, ensure_ascii=False, indent=2, default=str
                    )

                logger.info(f"ğŸ“ í…ŒìŠ¤íŠ¸ ê²°ê³¼ê°€ ì €ì¥ë¨: {result_filename}")

            return test_results

        except Exception as e:
            logger.error(f"âŒ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸ ì¤‘ ì˜ˆì™¸ ë°œìƒ: {e}")
            test_results["error_log"].append(f"ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸ ì˜ˆì™¸: {str(e)}")
            test_results["overall_success"] = False
            return test_results

    def quick_health_check(self) -> Dict[str, Any]:
        """
        ê°„ë‹¨í•œ ì‹œìŠ¤í…œ ìƒíƒœ í™•ì¸ (API ì—°ê²°, ëª¨ë“ˆ ë¡œë“œ ë“±)

        Returns:
            ìƒíƒœ í™•ì¸ ê²°ê³¼
        """
        logger.info("\nğŸ¥ ì‹œìŠ¤í…œ í—¬ìŠ¤ ì²´í¬ ì‹œì‘")

        health_status = {
            "overall_healthy": True,
            "checks": {},
            "timestamp": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
        }

        # 1. ëª¨ë“ˆ ë¡œë“œ í™•ì¸
        try:
            modules_check = {
                "data_collector": self.data_collector is not None,
                "preprocessor": self.preprocessor is not None,
                "text_analyzer": self.text_analyzer is not None,
                "clustering": self.clustering is not None,
                "topic_modeler": self.topic_modeler is not None,
                "sentiment_analyzer": self.sentiment_analyzer is not None,
                "similarity_analyzer": self.similarity_analyzer is not None,
                "llm_interface": self.llm_interface is not None,
            }

            health_status["checks"]["modules_loaded"] = {
                "status": all(modules_check.values()),
                "details": modules_check,
            }

            if not all(modules_check.values()):
                health_status["overall_healthy"] = False
                logger.warning("   âš ï¸ ì¼ë¶€ ëª¨ë“ˆì´ ë¡œë“œë˜ì§€ ì•ŠìŒ")
            else:
                logger.success("   âœ… ëª¨ë“  ëª¨ë“ˆ ì •ìƒ ë¡œë“œ")

        except Exception as e:
            health_status["checks"]["modules_loaded"] = {
                "status": False,
                "error": str(e),
            }
            health_status["overall_healthy"] = False
            logger.error(f"   âŒ ëª¨ë“ˆ í™•ì¸ ì‹¤íŒ¨: {e}")

        # 2. API í‚¤ í™•ì¸
        try:
            api_check = self.api_key is not None and len(self.api_key) > 0
            health_status["checks"]["api_key"] = {
                "status": api_check,
                "has_key": api_check,
            }

            if api_check:
                logger.success("   âœ… API í‚¤ ì„¤ì •ë¨")
            else:
                logger.warning("   âš ï¸ API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•ŠìŒ")

        except Exception as e:
            health_status["checks"]["api_key"] = {"status": False, "error": str(e)}
            logger.error(f"   âŒ API í‚¤ í™•ì¸ ì‹¤íŒ¨: {e}")

        # 3. ë°ì´í„° ë””ë ‰í† ë¦¬ í™•ì¸
        try:
            import os

            dir_exists = os.path.exists(self.data_dir)
            dir_writable = os.access(self.data_dir, os.W_OK) if dir_exists else False

            health_status["checks"]["data_directory"] = {
                "status": dir_exists and dir_writable,
                "exists": dir_exists,
                "writable": dir_writable,
                "path": self.data_dir,
            }

            if dir_exists and dir_writable:
                logger.success(f"   âœ… ë°ì´í„° ë””ë ‰í† ë¦¬ ì •ìƒ ({self.data_dir})")
            else:
                logger.warning(
                    f"   âš ï¸ ë°ì´í„° ë””ë ‰í† ë¦¬ ë¬¸ì œ (ì¡´ì¬: {dir_exists}, ì“°ê¸°ê°€ëŠ¥: {dir_writable})"
                )

        except Exception as e:
            health_status["checks"]["data_directory"] = {
                "status": False,
                "error": str(e),
            }
            health_status["overall_healthy"] = False
            logger.error(f"   âŒ ë””ë ‰í† ë¦¬ í™•ì¸ ì‹¤íŒ¨: {e}")

        # 4. ë°ì´í„° ìƒíƒœ í™•ì¸
        status = self.get_status()
        health_status["checks"]["data_status"] = {"status": True, "details": status}

        logger.info(
            f"   ğŸ“Š ë°ì´í„° ìƒíƒœ: ì›ë³¸ {status['raw_data_count']}ê°œ, ì „ì²˜ë¦¬ {status['processed_data_count']}ê°œ"  # noqa: E501
        )

        # ì „ì²´ ê²°ê³¼
        if health_status["overall_healthy"]:
            logger.success("ğŸ‰ ì‹œìŠ¤í…œ í—¬ìŠ¤ ì²´í¬ ì™„ë£Œ - ëª¨ë“  ìƒíƒœ ì •ìƒ")
        else:
            logger.warning("âš ï¸ ì‹œìŠ¤í…œ í—¬ìŠ¤ ì²´í¬ ì™„ë£Œ - ì¼ë¶€ ë¬¸ì œ ë°œê²¬")

        return health_status


if __name__ == "__main__":
    logger.info("ğŸ¤– ë‰´ìŠ¤ ì—ì´ì „íŠ¸ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸")

    # ì—ì´ì „íŠ¸ ì´ˆê¸°í™”
    agent = NewsAgent()

    # 1. í—¬ìŠ¤ ì²´í¬ ë¨¼ì € ì‹¤í–‰
    logger.info("\n" + "=" * 50)
    logger.info("ğŸ¥ ì‹œìŠ¤í…œ í—¬ìŠ¤ ì²´í¬ ì‹¤í–‰")
    logger.info("=" * 50)

    health_result = agent.quick_health_check()

    if health_result["overall_healthy"]:
        logger.success("âœ… ì‹œìŠ¤í…œ ìƒíƒœ ì •ìƒ - ì¢…í•© í…ŒìŠ¤íŠ¸ ì§„í–‰")

        # 2. ì¢…í•© í…ŒìŠ¤íŠ¸ ì‹¤í–‰
        logger.info("\n" + "=" * 50)
        logger.info("ğŸ§ª ì¢…í•© ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸ ì‹¤í–‰")
        logger.info("=" * 50)

        test_result = agent.run_full_system_test(
            test_articles=3,  # ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ë¥¼ ìœ„í•´ 3ê°œ ê¸°ì‚¬ë§Œ
            test_category="technology",
            save_results=True,
        )

        # 3. í…ŒìŠ¤íŠ¸ ê²°ê³¼ ìš”ì•½
        if test_result["overall_success"]:
            logger.success("\nğŸ‰ ë‰´ìŠ¤ ì—ì´ì „íŠ¸ ì‹œìŠ¤í…œì´ ì •ìƒì ìœ¼ë¡œ ì‘ë™í•©ë‹ˆë‹¤!")
            logger.info("ğŸ“Š ëª¨ë“  í•µì‹¬ ê¸°ëŠ¥ì´ í…ŒìŠ¤íŠ¸ë¥¼ í†µê³¼í–ˆìŠµë‹ˆë‹¤.")
        else:
            logger.warning("\nâš ï¸ ì¼ë¶€ ê¸°ëŠ¥ì—ì„œ ë¬¸ì œê°€ ë°œê²¬ë˜ì—ˆìŠµë‹ˆë‹¤.")
            logger.info("ğŸ”§ ë¡œê·¸ë¥¼ í™•ì¸í•˜ê³  ë¬¸ì œë¥¼ í•´ê²°í•´ì£¼ì„¸ìš”.")

        # 4. ì‚¬ìš©ë²• ì•ˆë‚´
        logger.info("\n" + "=" * 50)
        logger.info("ğŸ“– ë‰´ìŠ¤ ì—ì´ì „íŠ¸ ì‚¬ìš©ë²•")
        logger.info("=" * 50)

        sample_queries = [
            "ìµœì‹  IT ë‰´ìŠ¤ 10ê°œ ìˆ˜ì§‘í•´ì¤˜",
            "ì¤‘ìš”í•œ í‚¤ì›Œë“œ 15ê°œ ë¶„ì„í•´ì¤˜",
            "ê°ì • ë¶„ì„ ì‹¤í–‰í•´ì¤˜",
            "ë¹„ìŠ·í•œ ë‰´ìŠ¤ë¼ë¦¬ 5ê°œ ê·¸ë£¹ìœ¼ë¡œ ë¶„ë¥˜í•´ì¤˜",
            "ëª¨ë“  ë¶„ì„ì„ ì‹¤í–‰í•´ì¤˜",
        ]

        logger.info("ğŸ’¬ ì§€ì›í•˜ëŠ” ìì—°ì–´ ì¿¼ë¦¬ ì˜ˆì‹œ:")
        for i, query in enumerate(sample_queries, 1):
            logger.info(f"   {i}. '{query}'")

        logger.info("\nğŸš€ ì‹¤ì œ ì‚¬ìš© ì˜ˆì‹œ:")
        logger.info("```python")
        logger.info("from src.news_agent.news_agent import NewsAgent")
        logger.info("")
        logger.info("# ì—ì´ì „íŠ¸ ì´ˆê¸°í™”")
        logger.info("agent = NewsAgent()")
        logger.info("")
        logger.info("# ìì—°ì–´ ì¿¼ë¦¬ë¡œ ë¶„ì„ ì‹¤í–‰")
        logger.info(
            "result = agent.process_natural_query('ìµœì‹  IT ë‰´ìŠ¤ 20ê°œ ìˆ˜ì§‘í•´ì¤˜')"
        )
        logger.info("result = agent.process_natural_query('ëª¨ë“  ë¶„ì„ ì‹¤í–‰í•´ì¤˜')")
        logger.info("")
        logger.info("# ê°œë³„ í•¨ìˆ˜ ì§ì ‘ í˜¸ì¶œ")
        logger.info("agent.collect_news(n_articles=20, category='technology')")
        logger.info("agent.run_comprehensive_analysis()")
        logger.info("```")

    else:
        logger.error("âŒ ì‹œìŠ¤í…œ í—¬ìŠ¤ ì²´í¬ ì‹¤íŒ¨ - ì„¤ì •ì„ í™•ì¸í•´ì£¼ì„¸ìš”")

        failed_checks = [
            name
            for name, check in health_result.get("checks", {}).items()
            if not check.get("status", False)
        ]

        if failed_checks:
            logger.error(f"ì‹¤íŒ¨í•œ ì²´í¬ í•­ëª©: {', '.join(failed_checks)}")

        logger.info("\nğŸ”§ ë¬¸ì œ í•´ê²° ë°©ë²•:")
        logger.info("1. API í‚¤ê°€ ì˜¬ë°”ë¥´ê²Œ ì„¤ì •ë˜ì—ˆëŠ”ì§€ í™•ì¸")
        logger.info("2. í•„ìš”í•œ Python íŒ¨í‚¤ì§€ê°€ ëª¨ë‘ ì„¤ì¹˜ë˜ì—ˆëŠ”ì§€ í™•ì¸")
        logger.info("3. ë°ì´í„° ë””ë ‰í† ë¦¬ ê¶Œí•œ í™•ì¸")
        logger.info("4. ì¸í„°ë„· ì—°ê²° ìƒíƒœ í™•ì¸")
