"""
ë‰´ìŠ¤ í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ ëª¨ë“ˆ
"""

import re
from typing import List, Optional

import nltk
import pandas as pd
from loguru import logger
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize


class NewsPreprocessor:
    """ë‰´ìŠ¤ í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ í´ë˜ìŠ¤"""

    def __init__(self, download_nltk_data: bool = True):
        """
        Args:
            download_nltk_data: NLTK ë°ì´í„° ìë™ ë‹¤ìš´ë¡œë“œ ì—¬ë¶€
        """
        if download_nltk_data:
            self._download_nltk_requirements()

        self.stop_words = set(stopwords.words("english"))
        self._add_custom_stopwords()

    def _download_nltk_requirements(self):
        """í•„ìš”í•œ NLTK ë°ì´í„° ë‹¤ìš´ë¡œë“œ"""
        try:
            nltk.download("punkt_tab", quiet=True)
            nltk.download("punkt", quiet=True)
            nltk.download("stopwords", quiet=True)
            nltk.download("wordnet", quiet=True)
            nltk.download("averaged_perceptron_tagger_eng", quiet=True)
        except Exception as e:
            logger.info(f"NLTK ë°ì´í„° ë‹¤ìš´ë¡œë“œ ì¤‘ ì—ëŸ¬: {e}")

    def _add_custom_stopwords(self):
        """ì»¤ìŠ¤í…€ ë¶ˆìš©ì–´ ì¶”ê°€"""
        custom_stopwords = {
            "said",
            "says",
            "also",
            "would",
            "could",
            "one",
            "two",
            "new",
            "first",
            "last",
            "year",
            "years",
            "time",
            "way",
            "company",
            "companies",
            "people",
            "get",
            "make",
            "go",
            "see",
            "know",
            "take",
            "use",
            "work",
            "day",
            "part",
            "number",
            "system",
            "reuters",
            "ap",
            "associated",
            "press",
            "news",
            "report",
            "reports",
            "according",
            "sources",
            "source",
        }
        self.stop_words.update(custom_stopwords)

    def clean_text(self, text: str) -> str:
        """
        í…ìŠ¤íŠ¸ ì •ë¦¬

        Args:
            text: ì›ë³¸ í…ìŠ¤íŠ¸

        Returns:
            ì •ë¦¬ëœ í…ìŠ¤íŠ¸
        """
        if not isinstance(text, str) or not text.strip():
            return ""

        # ì†Œë¬¸ì ë³€í™˜
        text = text.lower()

        # HTML íƒœê·¸ ì œê±°
        text = re.sub(r"<[^>]+>", "", text)

        # URL ì œê±°
        text = re.sub(r"http\S+|www\S+|https\S+", "", text)

        # ì´ë©”ì¼ ì œê±°
        text = re.sub(r"\S+@\S+", "", text)

        # íŠ¹ìˆ˜ë¬¸ì ì œê±° (ì•ŒíŒŒë²³, ìˆ«ì, ê³µë°±ë§Œ ìœ ì§€)
        text = re.sub(r"[^a-zA-Z0-9\s]", " ", text)

        # ì—°ì†ëœ ê³µë°±ì„ í•˜ë‚˜ë¡œ í†µí•©
        text = re.sub(r"\s+", " ", text).strip()

        return text

    def tokenize_and_filter(self, text: str, min_length: int = 2) -> List[str]:
        """
        í† í°í™” ë° ë¶ˆìš©ì–´ ì œê±°

        Args:
            text: ì…ë ¥ í…ìŠ¤íŠ¸
            min_length: ìµœì†Œ í† í° ê¸¸ì´

        Returns:
            í•„í„°ë§ëœ í† í° ë¦¬ìŠ¤íŠ¸
        """
        if not text:
            return []

        try:
            tokens = word_tokenize(text)
        except Exception:
            # í† í°í™” ì‹¤íŒ¨ ì‹œ ê³µë°±ìœ¼ë¡œ ë¶„í• 
            tokens = text.split()

        # ë¶ˆìš©ì–´ ì œê±° ë° ê¸¸ì´ í•„í„°ë§
        filtered_tokens = [
            token
            for token in tokens
            if (
                token not in self.stop_words
                and len(token) >= min_length
                and token.isalpha()
            )  # ì•ŒíŒŒë²³ë§Œ í¬í•¨
        ]

        return filtered_tokens

    def preprocess_single_text(self, text: str) -> List[str]:
        """
        ë‹¨ì¼ í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬

        Args:
            text: ì…ë ¥ í…ìŠ¤íŠ¸

        Returns:
            ì „ì²˜ë¦¬ëœ í† í° ë¦¬ìŠ¤íŠ¸
        """
        cleaned_text = self.clean_text(text)
        return self.tokenize_and_filter(cleaned_text)

    def preprocess_dataframe(
        self, df: pd.DataFrame, text_columns: Optional[List[str]] = None
    ) -> pd.DataFrame:
        """
        DataFrame ì „ì²´ ì „ì²˜ë¦¬

        Args:
            df: ì…ë ¥ DataFrame
            text_columns: ì²˜ë¦¬í•  í…ìŠ¤íŠ¸ ì»¬ëŸ¼ ë¦¬ìŠ¤íŠ¸
                         (Noneì¸ ê²½ìš° ê¸°ë³¸ ì»¬ëŸ¼ë“¤ ì‚¬ìš©)

        Returns:
            ì „ì²˜ë¦¬ê°€ ì™„ë£Œëœ DataFrame
        """
        if df.empty:
            return df

        df_copy = df.copy()

        # ê¸°ë³¸ í…ìŠ¤íŠ¸ ì»¬ëŸ¼ ì„¤ì •
        if text_columns is None:
            text_columns = ["Title", "Description", "Parsed Content", "Content"]

        def combine_and_process_row(row):
            """í–‰ì˜ ì—¬ëŸ¬ í…ìŠ¤íŠ¸ ì»¬ëŸ¼ì„ ê²°í•©í•˜ì—¬ ì „ì²˜ë¦¬"""
            texts = []

            for col in text_columns:
                if col in row and pd.notna(row[col]):
                    texts.append(str(row[col]))

            combined_text = " ".join(texts)
            return self.preprocess_single_text(combined_text)

        # ì „ì²˜ë¦¬ ì ìš©
        logger.info("ğŸ”„ í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ë¥¼ ì§„í–‰í•©ë‹ˆë‹¤...")
        df_copy["processed_tokens"] = df_copy.apply(combine_and_process_row, axis=1)

        # ì „ì²˜ë¦¬ëœ í…ìŠ¤íŠ¸ë¥¼ ë¬¸ìì—´ë¡œ ë³€í™˜
        df_copy["processed_text"] = df_copy["processed_tokens"].apply(
            lambda tokens: " ".join(tokens) if tokens else ""
        )

        # ë¹ˆ ë‚´ìš© ì œê±°
        initial_count = len(df_copy)
        df_copy = df_copy[df_copy["processed_tokens"].str.len() > 0]
        final_count = len(df_copy)

        removed_count = initial_count - final_count
        if removed_count > 0:
            logger.warning(
                f"  ë¹ˆ ë‚´ìš©ìœ¼ë¡œ ì¸í•´ {removed_count}ê°œ ê¸°ì‚¬ê°€ ì œê±°ë˜ì—ˆìŠµë‹ˆë‹¤."
            )

        logger.success(f" ì „ì²˜ë¦¬ ì™„ë£Œ: {final_count}ê°œ ê¸°ì‚¬")
        return df_copy

    def get_vocabulary(self, df: pd.DataFrame) -> dict:
        """
        ì „ì²˜ë¦¬ëœ DataFrameì—ì„œ ì–´íœ˜ í†µê³„ ì¶”ì¶œ

        Args:
            df: ì „ì²˜ë¦¬ëœ DataFrame

        Returns:
            ì–´íœ˜ í†µê³„ ë”•ì…”ë„ˆë¦¬
        """
        if "processed_tokens" not in df.columns:
            return {}

        all_tokens = []
        for tokens in df["processed_tokens"]:
            all_tokens.extend(tokens)

        vocabulary = {
            "total_tokens": len(all_tokens),
            "unique_tokens": len(set(all_tokens)),
            "avg_tokens_per_doc": len(all_tokens) / len(df) if len(df) > 0 else 0,
        }

        return vocabulary

    def filter_by_frequency(
        self, df: pd.DataFrame, min_freq: int = 2, max_freq_ratio: float = 0.8
    ) -> pd.DataFrame:
        """
        ë¹ˆë„ìˆ˜ ê¸°ë°˜ í† í° í•„í„°ë§

        Args:
            df: ì „ì²˜ë¦¬ëœ DataFrame
            min_freq: ìµœì†Œ ì¶œí˜„ ë¹ˆë„
            max_freq_ratio: ìµœëŒ€ ì¶œí˜„ ë¹„ìœ¨ (ë¬¸ì„œ ëŒ€ë¹„)

        Returns:
            í•„í„°ë§ëœ DataFrame
        """
        if "processed_tokens" not in df.columns:
            return df

        # í† í° ë¹ˆë„ ê³„ì‚°
        token_freq = {}
        for tokens in df["processed_tokens"]:
            for token in tokens:
                token_freq[token] = token_freq.get(token, 0) + 1

        # ë¬¸ì„œ ë¹ˆë„ ê³„ì‚°
        doc_freq = {}
        for tokens in df["processed_tokens"]:
            unique_tokens = set(tokens)
            for token in unique_tokens:
                doc_freq[token] = doc_freq.get(token, 0) + 1

        total_docs = len(df)
        max_doc_freq = int(total_docs * max_freq_ratio)

        # í•„í„°ë§ ì¡°ê±´ ì ìš©
        def filter_tokens(tokens):
            return [
                token
                for token in tokens
                if (
                    token_freq.get(token, 0) >= min_freq
                    and doc_freq.get(token, 0) <= max_doc_freq
                )
            ]

        df_copy = df.copy()
        df_copy["processed_tokens"] = df_copy["processed_tokens"].apply(filter_tokens)
        df_copy["processed_text"] = df_copy["processed_tokens"].apply(
            lambda tokens: " ".join(tokens)
        )

        # ë¹ˆ ë¬¸ì„œ ì œê±°
        df_copy = df_copy[df_copy["processed_tokens"].str.len() > 0]

        logger.success(f" ë¹ˆë„ í•„í„°ë§ ì™„ë£Œ: {len(df_copy)}ê°œ ê¸°ì‚¬")
        return df_copy


if __name__ == "__main__":
    # ì‚¬ìš© ì˜ˆì œ
    preprocessor = NewsPreprocessor()

    # ë‹¨ì¼ í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ ì˜ˆì œ
    sample_text = "This is a sample news article about technology and AI."
    processed_tokens = preprocessor.preprocess_single_text(sample_text)
    logger.info("ì „ì²˜ë¦¬ ê²°ê³¼:", processed_tokens)

    # DataFrame ì „ì²˜ë¦¬ ì˜ˆì œ (CSV íŒŒì¼ì´ ìˆëŠ” ê²½ìš°)
    try:
        df = pd.read_csv("../collected_news.csv")
        processed_df = preprocessor.preprocess_dataframe(df)

        if processed_df.empty:
            logger.warning(
                "ì „ì²˜ë¦¬ ê²°ê³¼ DataFrameì´ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤. CSV/ì „ì²˜ë¦¬ ë‹¨ê³„ë¥¼ í™•ì¸í•˜ì„¸ìš”."
            )
        else:
            vocab_stats = preprocessor.get_vocabulary(processed_df)
            logger.info(
                "ì–´íœ˜ í†µê³„ | total={total_tokens}, unique={unique_tokens}, avg_per_doc={avg_tokens_per_doc:.2f}",  # noqa: E501
                **vocab_stats,
            )

        # ê²°ê³¼ ì €ì¥
        processed_df.to_csv("processed_news.csv", index=False)
        logger.info("ğŸ’¾ processed_news.csvë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")

    except FileNotFoundError:
        logger.info("ğŸ“„ ë‰´ìŠ¤ ë°ì´í„° íŒŒì¼ì„ ë¨¼ì € ìˆ˜ì§‘í•´ì£¼ì„¸ìš”.")
