"""
ê°„ë‹¨í•œ LDA í† í”½ ëª¨ë¸ë§ ëª¨ë“ˆ
í•µì‹¬ ê¸°ëŠ¥ë§Œ í¬í•¨
"""

from typing import Any, Dict, List

import numpy as np
import pandas as pd
from loguru import logger
from sklearn.decomposition import LatentDirichletAllocation
from sklearn.feature_extraction.text import CountVectorizer


class TopicModeler:
    """LDA í† í”½ ëª¨ë¸ë§ í´ë˜ìŠ¤"""

    def __init__(
        self, n_topics: int = 3, max_features: int = 500, random_state: int = 42
    ):
        """
        Args:
            n_topics: ì¶”ì¶œí•  í† í”½ ìˆ˜
            max_features: ìµœëŒ€ íŠ¹ì„± ìˆ˜
            random_state: ëœë¤ ì‹œë“œ
        """
        self.n_topics = n_topics
        self.random_state = random_state

        self.vectorizer = CountVectorizer(
            max_features=max_features,
            min_df=1,
            max_df=0.95,
            lowercase=False,
            stop_words=None,
        )

        self.lda = LatentDirichletAllocation(
            n_components=n_topics, random_state=random_state, max_iter=50
        )

        self.is_fitted = False
        self.doc_topic_probs = None

    def fit_transform(self, texts: List[str]) -> np.ndarray:
        """í† í”½ ëª¨ë¸ë§ ìˆ˜í–‰"""
        if not texts:
            raise ValueError("í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤")

        valid_texts = [text for text in texts if text and text.strip()]
        if not valid_texts:
            raise ValueError("ìœ íš¨í•œ í…ìŠ¤íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤")

        # ë²¡í„°í™” ë° LDA í•™ìŠµ
        count_matrix = self.vectorizer.fit_transform(valid_texts)
        self.doc_topic_probs = self.lda.fit_transform(count_matrix)
        self.is_fitted = True

        logger.success(
            f"í† í”½ ëª¨ë¸ë§ ì™„ë£Œ: {self.n_topics}ê°œ í† í”½, {len(valid_texts)}ê°œ ë¬¸ì„œ"
        )
        return self.doc_topic_probs

    def get_top_words(self, n_words: int = 5) -> List[List[str]]:
        """ê° í† í”½ì˜ ìƒìœ„ ë‹¨ì–´ ì¶”ì¶œ"""
        if not self.is_fitted:
            return []

        feature_names = self.vectorizer.get_feature_names_out()
        topics = []

        for topic in self.lda.components_:
            top_indices = topic.argsort()[-n_words:][::-1]
            top_words = [feature_names[i] for i in top_indices]
            topics.append(top_words)

        return topics

    def analyze(self, df: pd.DataFrame) -> pd.DataFrame:
        """í† í”½ ë¶„ì„ ë° ê²°ê³¼ ë°˜í™˜"""
        if not self.is_fitted:
            raise ValueError("ëª¨ë¸ì´ í•™ìŠµë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")

        df_result = df.copy()
        df_result["main_topic"] = self.doc_topic_probs.argmax(axis=1)
        df_result["topic_confidence"] = self.doc_topic_probs.max(axis=1)

        # ê²°ê³¼ ì¶œë ¥
        logger.info("\nğŸ­ í† í”½ ë¶„ì„ ê²°ê³¼:")
        topics = self.get_top_words(5)

        for i, topic_words in enumerate(topics):
            topic_docs = df_result[df_result["main_topic"] == i]
            logger.info(
                f"í† í”½ {i + 1} ({len(topic_docs)}ê°œ ë¬¸ì„œ): {', '.join(topic_words)}"
            )

        return df_result

    def save_results(self, df_result: pd.DataFrame, prefix: str = "topics") -> str:
        """ê²°ê³¼ ì €ì¥"""
        filename = f"{prefix}_results.csv"
        df_result.to_csv(filename, index=False, encoding="utf-8-sig")
        logger.success(f"í† í”½ ë¶„ì„ ê²°ê³¼ ì €ì¥: {filename}")
        return filename

    def get_info(self) -> Dict[str, Any]:
        """ëª¨ë¸ ì •ë³´ ë°˜í™˜"""
        if not self.is_fitted:
            return {}

        return {
            "n_topics": self.n_topics,
            "n_documents": self.doc_topic_probs.shape[0],
            "is_fitted": self.is_fitted,
        }


if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸ ì½”ë“œ
    sample_texts = [
        "machine learning artificial intelligence deep neural networks",
        "cloud computing aws azure google servers infrastructure",
        "cybersecurity privacy data protection encryption firewall",
        "mobile apps ios android development programming",
        "blockchain cryptocurrency bitcoin ethereum smart contracts",
    ]

    modeler = TopicModeler(n_topics=3)
    doc_topics = modeler.fit_transform(sample_texts)

    topics = modeler.get_top_words(3)
    for i, topic in enumerate(topics):
        print(f"Topic {i + 1}: {', '.join(topic)}")
