"""
TF-IDF ê¸°ë°˜ í…ìŠ¤íŠ¸ ë¶„ì„ ëª¨ë“ˆ
"""

from typing import List, Tuple

import numpy as np
import pandas as pd
from loguru import logger
from sklearn.feature_extraction.text import TfidfVectorizer


class TextAnalyzer:
    """TF-IDF ê¸°ë°˜ í…ìŠ¤íŠ¸ ë¶„ì„ í´ë˜ìŠ¤"""

    def __init__(
        self,
        max_features: int = 1000,
        ngram_range: Tuple[int, int] = (1, 2),
        min_df: int = 2,
        max_df: float = 0.95,
    ):
        """
        Args:
            max_features: ìµœëŒ€ íŠ¹ì„± ìˆ˜
            ngram_range: n-gram ë²”ìœ„ (1-gram, 2-gram ë“±)
            min_df: ìµœì†Œ ë¬¸ì„œ ë¹ˆë„
            max_df: ìµœëŒ€ ë¬¸ì„œ ë¹ˆë„ ë¹„ìœ¨
        """
        self.vectorizer = TfidfVectorizer(
            max_features=max_features,
            ngram_range=ngram_range,
            min_df=min_df,
            max_df=max_df,
            lowercase=False,  # ì „ì²˜ë¦¬ì—ì„œ ì´ë¯¸ ì†Œë¬¸ì ë³€í™˜
            stop_words=None,  # ì „ì²˜ë¦¬ì—ì„œ ì´ë¯¸ ë¶ˆìš©ì–´ ì œê±°
        )
        self.tfidf_matrix = None
        self.feature_names = None
        self.is_fitted = False

    def fit_transform(self, texts: List[str]):
        """
        TF-IDF ë²¡í„°í™” ìˆ˜í–‰

        Args:
            texts: í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸

        Returns:
            TF-IDF í–‰ë ¬
        """
        if not texts:
            raise ValueError("í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")

        # ë¹ˆ í…ìŠ¤íŠ¸ ì œê±°
        valid_texts = [text for text in texts if text and text.strip()]

        if not valid_texts:
            raise ValueError("ìœ íš¨í•œ í…ìŠ¤íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤.")

        self.tfidf_matrix = self.vectorizer.fit_transform(valid_texts)
        self.feature_names = self.vectorizer.get_feature_names_out()
        self.is_fitted = True

        logger.success(" TF-IDF ë²¡í„°í™” ì™„ë£Œ")
        logger.info(f"   ë¬¸ì„œ ìˆ˜: {self.tfidf_matrix.shape[0]}")
        logger.info(f"   íŠ¹ì„± ìˆ˜: {self.tfidf_matrix.shape[1]}")

        return self.tfidf_matrix

    def transform(self, texts: List[str]):
        """
        ìƒˆë¡œìš´ í…ìŠ¤íŠ¸ì— ëŒ€í•´ TF-IDF ë³€í™˜ ìˆ˜í–‰

        Args:
            texts: ë³€í™˜í•  í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸

        Returns:
            TF-IDF í–‰ë ¬
        """
        if not self.is_fitted:
            raise ValueError("ë¨¼ì € fit_transformì„ í˜¸ì¶œí•´ì•¼ í•©ë‹ˆë‹¤.")

        return self.vectorizer.transform(texts)

    def get_matrix_info(self) -> dict:
        """TF-IDF í–‰ë ¬ ì •ë³´ ë°˜í™˜"""
        if not self.is_fitted:
            return {}

        tfidf_matrix = self.tfidf_matrix
        total_elements = tfidf_matrix.shape[0] * tfidf_matrix.shape[1]
        non_zero_elements = tfidf_matrix.nnz
        sparsity = (1 - non_zero_elements / total_elements) * 100

        return {
            "shape": tfidf_matrix.shape,
            "total_elements": total_elements,
            "non_zero_elements": non_zero_elements,
            "sparsity_percentage": sparsity,
            "dtype": str(tfidf_matrix.dtype),
        }

    def get_top_keywords_global(self, n_keywords: int = 20) -> List[Tuple[str, float]]:
        """
        ì „ì²´ ì½”í¼ìŠ¤ì—ì„œ ìƒìœ„ í‚¤ì›Œë“œ ì¶”ì¶œ

        Args:
            n_keywords: ì¶”ì¶œí•  í‚¤ì›Œë“œ ìˆ˜

        Returns:
            (í‚¤ì›Œë“œ, ì ìˆ˜) íŠœí”Œ ë¦¬ìŠ¤íŠ¸
        """
        if not self.is_fitted:
            return []

        # ê° íŠ¹ì„±ì˜ í‰ê·  TF-IDF ì ìˆ˜ ê³„ì‚°
        mean_scores = np.array(self.tfidf_matrix.mean(axis=0)).flatten()

        # ìƒìœ„ í‚¤ì›Œë“œ ì¶”ì¶œ
        top_indices = mean_scores.argsort()[-n_keywords:][::-1]
        top_keywords = [(self.feature_names[i], mean_scores[i]) for i in top_indices]

        return top_keywords

    def get_document_keywords(
        self, doc_index: int, n_keywords: int = 10
    ) -> List[Tuple[str, float]]:
        """
        íŠ¹ì • ë¬¸ì„œì˜ ìƒìœ„ í‚¤ì›Œë“œ ì¶”ì¶œ

        Args:
            doc_index: ë¬¸ì„œ ì¸ë±ìŠ¤
            n_keywords: ì¶”ì¶œí•  í‚¤ì›Œë“œ ìˆ˜

        Returns:
            (í‚¤ì›Œë“œ, ì ìˆ˜) íŠœí”Œ ë¦¬ìŠ¤íŠ¸
        """
        if not self.is_fitted:
            return []

        if doc_index >= self.tfidf_matrix.shape[0]:
            return []

        doc_scores = self.tfidf_matrix[doc_index].toarray().flatten()
        top_indices = doc_scores.argsort()[-n_keywords:][::-1]

        doc_keywords = [
            (self.feature_names[i], doc_scores[i])
            for i in top_indices
            if doc_scores[i] > 0
        ]

        return doc_keywords

    def analyze_keywords_comprehensive(
        self,
        df: pd.DataFrame,
        show_top_global: int = 20,
        show_top_per_doc: int = 5,
        show_sample_docs: int = 3,
    ) -> dict:
        """
        ì¢…í•©ì ì¸ í‚¤ì›Œë“œ ë¶„ì„ ìˆ˜í–‰

        Args:
            df: ì›ë³¸ DataFrame
            show_top_global: ì „ì—­ ìƒìœ„ í‚¤ì›Œë“œ ìˆ˜
            show_top_per_doc: ë¬¸ì„œë³„ ìƒìœ„ í‚¤ì›Œë“œ ìˆ˜
            show_sample_docs: ìƒ˜í”Œë¡œ ë³´ì—¬ì¤„ ë¬¸ì„œ ìˆ˜

        Returns:
            ë¶„ì„ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        """
        if not self.is_fitted:
            return {}

        results = {}

        # ì „ì²´ ìƒìœ„ í‚¤ì›Œë“œ
        top_keywords = self.get_top_keywords_global(show_top_global)
        results["global_keywords"] = top_keywords

        logger.info("\nğŸ” TF-IDF í‚¤ì›Œë“œ ë¶„ì„")
        logger.info("-" * 50)
        logger.info(f" ì „ì²´ ìƒìœ„ {show_top_global}ê°œ í‚¤ì›Œë“œ:")
        for i, (keyword, score) in enumerate(top_keywords, 1):
            logger.info(f"   {i:2d}. {keyword:<20} ({score:.4f})")

        # ë¬¸ì„œë³„ í‚¤ì›Œë“œ ë¶„ì„
        document_keywords = {}
        logger.info(
            f"\nğŸ“° ë¬¸ì„œë³„ ìƒìœ„ {show_top_per_doc}ê°œ í‚¤ì›Œë“œ (ìƒ˜í”Œ {show_sample_docs}ê°œ):"
        )

        for i in range(min(show_sample_docs, len(df), self.tfidf_matrix.shape[0])):
            title = df.iloc[i].get("Title", f"ë¬¸ì„œ {i}")[:50]
            doc_keywords = self.get_document_keywords(i, show_top_per_doc)
            document_keywords[i] = doc_keywords

            logger.info(f"\n   ğŸ“„ {title}...")
            for keyword, score in doc_keywords:
                logger.info(f"      â€¢ {keyword} ({score:.3f})")

        results["document_keywords"] = document_keywords
        return results

    def export_analysis_results(
        self, df: pd.DataFrame, output_prefix: str = "tfidf_analysis"
    ) -> dict:
        """
        ë¶„ì„ ê²°ê³¼ë¥¼ CSV íŒŒì¼ë¡œ ì €ì¥

        Args:
            df: ì›ë³¸ DataFrame
            output_prefix: ì¶œë ¥ íŒŒì¼ ì ‘ë‘ì‚¬

        Returns:
            ì €ì¥ëœ íŒŒì¼ ì •ë³´
        """
        if not self.is_fitted:
            return {}

        saved_files = {}

        # 1. ì „ì²´ ìƒìœ„ í‚¤ì›Œë“œ ì €ì¥
        top_keywords = self.get_top_keywords_global(50)
        keywords_df = pd.DataFrame(top_keywords, columns=["Keyword", "TF-IDF_Score"])
        keywords_file = f"{output_prefix}_top_keywords.csv"
        keywords_df.to_csv(keywords_file, index=False)
        saved_files["top_keywords"] = keywords_file

        # 2. ë¬¸ì„œë³„ í‚¤ì›Œë“œ ë¶„ì„ ê²°ê³¼ ì €ì¥
        document_results = []

        for i in range(min(len(df), self.tfidf_matrix.shape[0])):
            doc_keywords = self.get_document_keywords(i, 10)
            keywords = [kw[0] for kw in doc_keywords]
            scores = [kw[1] for kw in doc_keywords]

            result = {
                "Document_Index": i,
                "Title": df.iloc[i].get("Title", f"ë¬¸ì„œ {i}"),
                "Top_Keywords": ", ".join(keywords[:5]),
                "Top_Keyword_Scores": ", ".join(
                    [f"{score:.3f}" for score in scores[:5]]
                ),
                "All_Keywords": ", ".join(keywords),
                "All_Scores": ", ".join([f"{score:.3f}" for score in scores]),
            }
            document_results.append(result)

        doc_results_df = pd.DataFrame(document_results)
        doc_file = f"{output_prefix}_document_keywords.csv"
        doc_results_df.to_csv(doc_file, index=False)
        saved_files["document_keywords"] = doc_file

        # 3. í–‰ë ¬ ì •ë³´ ì €ì¥
        matrix_info = self.get_matrix_info()
        info_df = pd.DataFrame([matrix_info])
        info_file = f"{output_prefix}_matrix_info.csv"
        info_df.to_csv(info_file, index=False)
        saved_files["matrix_info"] = info_file

        logger.info("\nğŸ’¾ ë¶„ì„ ê²°ê³¼ ì €ì¥ ì™„ë£Œ:")
        for key, filename in saved_files.items():
            logger.info(f"   â€¢ {key}: {filename}")

        return saved_files

    def get_keyword_statistics(self) -> dict:
        """í‚¤ì›Œë“œ í†µê³„ ì •ë³´ ë°˜í™˜"""
        if not self.is_fitted:
            return {}

        # ë¬¸ì„œë³„ í†µê³„
        doc_scores = np.array(self.tfidf_matrix.sum(axis=1)).flatten()

        # ë‹¨ì–´ë³„ í†µê³„
        word_scores = np.array(self.tfidf_matrix.sum(axis=0)).flatten()

        return {
            "document_stats": {
                "mean_tfidf_sum": float(doc_scores.mean()),
                "max_tfidf_sum": float(doc_scores.max()),
                "min_tfidf_sum": float(doc_scores.min()),
                "std_tfidf_sum": float(doc_scores.std()),
            },
            "word_stats": {
                "mean_tfidf_sum": float(word_scores.mean()),
                "max_tfidf_sum": float(word_scores.max()),
                "min_tfidf_sum": float(word_scores.min()),
                "std_tfidf_sum": float(word_scores.std()),
                "top_word": self.feature_names[word_scores.argmax()],
                "bottom_word": self.feature_names[word_scores.argmin()],
            },
        }

    def print_statistics(self):
        """í†µê³„ ì •ë³´ ì¶œë ¥"""
        if not self.is_fitted:
            logger.error(" TF-IDF ëª¨ë¸ì´ í•™ìŠµë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return

        matrix_info = self.get_matrix_info()
        stats = self.get_keyword_statistics()

        logger.info("\nğŸ“Š TF-IDF ë¶„ì„ í†µê³„")
        logger.info("=" * 40)

        logger.info("ğŸ“„ ë§¤íŠ¸ë¦­ìŠ¤ ì •ë³´:")
        logger.info(f"   â€¢ í¬ê¸°: {matrix_info['shape']}")
        logger.info(f"   â€¢ í¬ì†Œì„±: {matrix_info['sparsity_percentage']:.2f}%")
        logger.info(f"   â€¢ 0ì´ ì•„ë‹Œ ì›ì†Œ: {matrix_info['non_zero_elements']:,}")

        logger.info("\nğŸ“Š ë¬¸ì„œë³„ í†µê³„:")
        doc_stats = stats["document_stats"]
        logger.info(f"   â€¢ í‰ê·  TF-IDF í•©: {doc_stats['mean_tfidf_sum']:.3f}")
        logger.info(f"   â€¢ ìµœëŒ€ê°’: {doc_stats['max_tfidf_sum']:.3f}")
        logger.info(f"   â€¢ ìµœì†Œê°’: {doc_stats['min_tfidf_sum']:.3f}")

        logger.info("\nğŸ”¤ ë‹¨ì–´ë³„ í†µê³„:")
        word_stats = stats["word_stats"]
        logger.info(f"   â€¢ í‰ê·  TF-IDF í•©: {word_stats['mean_tfidf_sum']:.3f}")
        logger.info(f"   â€¢ ê°€ì¥ ì¤‘ìš”í•œ ë‹¨ì–´: {word_stats['top_word']}")


if __name__ == "__main__":
    # ì‚¬ìš© ì˜ˆì œ
    analyzer = TextAnalyzer(max_features=1500, ngram_range=(1, 3))

    df = pd.read_csv("processed_news.csv")
    sample_texts = df["processed_text"].dropna().tolist()

    try:
        # TF-IDF ë¶„ì„ ìˆ˜í–‰
        analyzer.fit_transform(sample_texts)

        # í†µê³„ ì¶œë ¥
        analyzer.print_statistics()

        # í‚¤ì›Œë“œ ì¶”ì¶œ
        keywords = analyzer.get_top_keywords_global(10)
        logger.info("\nìƒìœ„ í‚¤ì›Œë“œ:")
        for keyword, score in keywords:
            logger.info(f"  â€¢ {keyword}: {score:.3f}")

    except Exception as e:
        logger.error(f" ì—ëŸ¬ ë°œìƒ: {e}")
