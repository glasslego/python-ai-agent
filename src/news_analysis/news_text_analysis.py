# flake8: noqa: E501
import re

import matplotlib.pyplot as plt
import pandas as pd
import seaborn as sns
from sklearn.cluster import KMeans
from sklearn.decomposition import PCA
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from wordcloud import WordCloud


class NewsTextAnalyzer:
    def __init__(self, df: pd.DataFrame):
        self.df = df
        self.tfidf_vectorizer = None
        self.tfidf_matrix = None
        self.processed_texts = []

    def preprocess_text(self, text):
        """í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬"""
        if pd.isna(text) or text == "":
            return ""

        # ì†Œë¬¸ì ë³€í™˜
        text = text.lower()
        # HTML íƒœê·¸ ì œê±°
        text = re.sub(r"<[^>]+>", "", text)
        # íŠ¹ìˆ˜ë¬¸ì ì œê±° (ì˜ë¬¸ì, ìˆ«ì, ê³µë°±ë§Œ ìœ ì§€)
        text = re.sub(r"[^a-zA-Z\s]", "", text)
        # ì—¬ëŸ¬ ê³µë°±ì„ í•˜ë‚˜ë¡œ
        text = re.sub(r"\s+", " ", text)
        return text.strip()

    def prepare_texts(self):
        """ë¶„ì„í•  í…ìŠ¤íŠ¸ ì¤€ë¹„"""
        # Parsed Contentì™€ Description ê²°í•©í•˜ì—¬ ë¶„ì„
        combined_texts = []
        for _, row in self.df.iterrows():
            content = str(row.get("Parsed Content", ""))
            description = str(row.get("Description", ""))
            title = str(row.get("Title", ""))

            combined_text = f"{title} {description} {content}"
            processed = self.preprocess_text(combined_text)
            combined_texts.append(processed)

        # ë¹ˆ í…ìŠ¤íŠ¸ í•„í„°ë§
        self.processed_texts = [text for text in combined_texts if text.strip()]
        return self.processed_texts

    def perform_tfidf_analysis(self, max_features=100):
        """TF-IDF ë¶„ì„ ìˆ˜í–‰"""
        if not self.processed_texts:
            self.prepare_texts()

        self.tfidf_vectorizer = TfidfVectorizer(
            max_features=max_features, stop_words="english", ngram_range=(1, 2)
        )

        self.tfidf_matrix = self.tfidf_vectorizer.fit_transform(self.processed_texts)

        # íŠ¹ì„±ëª…ê³¼ TF-IDF ì ìˆ˜
        feature_names = self.tfidf_vectorizer.get_feature_names_out()
        tfidf_scores = self.tfidf_matrix.sum(axis=0).A1

        # ìƒìœ„ í‚¤ì›Œë“œ ì¶”ì¶œ
        top_keywords = sorted(
            zip(feature_names, tfidf_scores), key=lambda x: x[1], reverse=True
        )[:20]

        return top_keywords

    def perform_clustering(self, n_clusters=3):
        """K-means í´ëŸ¬ìŠ¤í„°ë§"""
        if self.tfidf_matrix is None:
            self.perform_tfidf_analysis()

        kmeans = KMeans(n_clusters=n_clusters, random_state=42)
        clusters = kmeans.fit_predict(self.tfidf_matrix)

        # DataFrameì— í´ëŸ¬ìŠ¤í„° ì •ë³´ ì¶”ê°€
        valid_indices = [
            i for i, text in enumerate(self.prepare_texts()) if text.strip()
        ]
        cluster_df = self.df.iloc[valid_indices].copy()
        cluster_df["Cluster"] = clusters

        return cluster_df, kmeans

    def calculate_similarity_matrix(self):
        """ë¬¸ì„œê°„ ìœ ì‚¬ë„ ê³„ì‚°"""
        if self.tfidf_matrix is None:
            self.perform_tfidf_analysis()

        similarity_matrix = cosine_similarity(self.tfidf_matrix)
        return similarity_matrix

    def visualize_top_keywords(self, top_keywords=None):
        """ìƒìœ„ í‚¤ì›Œë“œ ì‹œê°í™”"""
        if top_keywords is None:
            top_keywords = self.perform_tfidf_analysis()

        keywords, scores = zip(*top_keywords)

        plt.figure(figsize=(12, 8))
        plt.barh(range(len(keywords)), scores)
        plt.yticks(range(len(keywords)), keywords)
        plt.xlabel("TF-IDF Score")
        plt.title("Top Keywords in IT News")
        plt.gca().invert_yaxis()
        plt.tight_layout()
        plt.show()

    def visualize_clusters(self, cluster_df=None):
        """í´ëŸ¬ìŠ¤í„° ì‹œê°í™”"""
        if cluster_df is None:
            cluster_df, _ = self.perform_clustering()

        if self.tfidf_matrix is None:
            self.perform_tfidf_analysis()

        # PCAë¥¼ ì‚¬ìš©í•œ ì°¨ì› ì¶•ì†Œ
        pca = PCA(n_components=2)
        reduced_data = pca.fit_transform(self.tfidf_matrix.toarray())

        plt.figure(figsize=(12, 8))
        scatter = plt.scatter(
            reduced_data[:, 0],
            reduced_data[:, 1],
            c=cluster_df["Cluster"],
            cmap="viridis",
            alpha=0.6,
        )
        plt.colorbar(scatter)
        plt.xlabel(f"PC1 ({pca.explained_variance_ratio_[0]:.2%} variance)")
        plt.ylabel(f"PC2 ({pca.explained_variance_ratio_[1]:.2%} variance)")
        plt.title("News Articles Clustering (PCA)")

        # ê° ì ì— ê¸°ì‚¬ ì œëª© ì¼ë¶€ í‘œì‹œ
        for i, title in enumerate(cluster_df["Title"]):
            plt.annotate(
                title[:30] + "...",
                (reduced_data[i, 0], reduced_data[i, 1]),
                fontsize=8,
                alpha=0.7,
            )

        plt.tight_layout()
        plt.show()

    def create_wordcloud(self):
        """ì›Œë“œí´ë¼ìš°ë“œ ìƒì„±"""
        if not self.processed_texts:
            self.prepare_texts()

        # ëª¨ë“  í…ìŠ¤íŠ¸ ê²°í•©
        all_text = " ".join(self.processed_texts)

        wordcloud = WordCloud(
            width=800,
            height=400,
            background_color="white",
            max_words=100,
            colormap="viridis",
        ).generate(all_text)

        plt.figure(figsize=(12, 6))
        plt.imshow(wordcloud, interpolation="bilinear")
        plt.axis("off")
        plt.title("IT News Word Cloud")
        plt.tight_layout()
        plt.show()

    def visualize_similarity_heatmap(self):
        """ë¬¸ì„œ ìœ ì‚¬ë„ íˆíŠ¸ë§µ"""
        similarity_matrix = self.calculate_similarity_matrix()

        plt.figure(figsize=(10, 8))

        # ì œëª©ì„ ë¼ë²¨ë¡œ ì‚¬ìš© (ì²˜ìŒ 30ìë§Œ)
        valid_indices = [
            i for i, text in enumerate(self.prepare_texts()) if text.strip()
        ]
        labels = [self.df.iloc[i]["Title"][:30] + "..." for i in valid_indices]

        sns.heatmap(
            similarity_matrix,
            annot=False,
            cmap="coolwarm",
            xticklabels=labels,
            yticklabels=labels,
        )
        plt.title("Document Similarity Matrix")
        plt.xticks(rotation=45, ha="right")
        plt.yticks(rotation=0)
        plt.tight_layout()
        plt.show()

    def analyze_article_stats(self):
        """ê¸°ì‚¬ í†µê³„ ë¶„ì„"""
        stats_data = []

        for _, row in self.df.iterrows():
            word_count = row.get("Word Count", 0)
            reading_time = row.get("Reading Time (min)", 0)
            source = row.get("Source", "Unknown")

            stats_data.append(
                {
                    "Source": source,
                    "Word Count": word_count,
                    "Reading Time": reading_time,
                    "Title": row.get("Title", "")[:40] + "...",
                }
            )

        stats_df = pd.DataFrame(stats_data)

        # í†µê³„ ì‹œê°í™”
        fig, axes = plt.subplots(2, 2, figsize=(15, 10))

        # 1. ì†ŒìŠ¤ë³„ ê¸°ì‚¬ ìˆ˜
        source_counts = stats_df["Source"].value_counts()
        axes[0, 0].bar(source_counts.index, source_counts.values)
        axes[0, 0].set_title("Articles by Source")
        axes[0, 0].set_xlabel("Source")
        axes[0, 0].set_ylabel("Count")
        axes[0, 0].tick_params(axis="x", rotation=45)

        # 2. ë‹¨ì–´ ìˆ˜ ë¶„í¬
        axes[0, 1].hist(stats_df["Word Count"], bins=10, alpha=0.7)
        axes[0, 1].set_title("Word Count Distribution")
        axes[0, 1].set_xlabel("Word Count")
        axes[0, 1].set_ylabel("Frequency")

        # 3. ì½ê¸° ì‹œê°„ ë¶„í¬
        axes[1, 0].hist(stats_df["Reading Time"], bins=10, alpha=0.7, color="orange")
        axes[1, 0].set_title("Reading Time Distribution")
        axes[1, 0].set_xlabel("Reading Time (minutes)")
        axes[1, 0].set_ylabel("Frequency")

        # 4. ë‹¨ì–´ ìˆ˜ vs ì½ê¸° ì‹œê°„
        axes[1, 1].scatter(stats_df["Word Count"], stats_df["Reading Time"], alpha=0.6)
        axes[1, 1].set_title("Word Count vs Reading Time")
        axes[1, 1].set_xlabel("Word Count")
        axes[1, 1].set_ylabel("Reading Time (minutes)")

        plt.tight_layout()
        plt.show()

        return stats_df

    def generate_full_analysis_report(self):
        """ì „ì²´ ë¶„ì„ ë³´ê³ ì„œ ìƒì„±"""
        print("ğŸ” IT ë‰´ìŠ¤ í…ìŠ¤íŠ¸ ë¶„ì„ ë³´ê³ ì„œ")
        print("=" * 50)

        # ê¸°ë³¸ ì •ë³´
        print(f"ğŸ“Š ì´ ê¸°ì‚¬ ìˆ˜: {len(self.df)}")
        print(f"ğŸ“Š ìœ íš¨í•œ í…ìŠ¤íŠ¸ê°€ ìˆëŠ” ê¸°ì‚¬ ìˆ˜: {len(self.processed_texts)}")

        # TF-IDF ë¶„ì„
        print("\nğŸ”‘ ìƒìœ„ í‚¤ì›Œë“œ (TF-IDF):")
        top_keywords = self.perform_tfidf_analysis()
        for i, (keyword, score) in enumerate(top_keywords[:10], 1):
            print(f"{i:2d}. {keyword:<20} (ì ìˆ˜: {score:.3f})")

        # í´ëŸ¬ìŠ¤í„°ë§
        print("\nğŸ¯ í´ëŸ¬ìŠ¤í„° ë¶„ì„:")
        cluster_df, _ = self.perform_clustering()
        for cluster_id in sorted(cluster_df["Cluster"].unique()):
            cluster_articles = cluster_df[cluster_df["Cluster"] == cluster_id]
            print(f"\ní´ëŸ¬ìŠ¤í„° {cluster_id} ({len(cluster_articles)}ê°œ ê¸°ì‚¬):")
            for _, article in cluster_articles.head(3).iterrows():
                print(f"  - {article['Title'][:60]}...")

        # í†µê³„ ì •ë³´
        print("\nğŸ“ˆ ê¸°ì‚¬ í†µê³„:")
        stats_df = self.analyze_article_stats()
        print(f"í‰ê·  ë‹¨ì–´ ìˆ˜: {stats_df['Word Count'].mean():.1f}")
        print(f"í‰ê·  ì½ê¸° ì‹œê°„: {stats_df['Reading Time'].mean():.1f}ë¶„")
        print(
            f"ì£¼ìš” ë‰´ìŠ¤ ì†ŒìŠ¤: {', '.join(stats_df['Source'].value_counts().head(3).index.tolist())}"
        )

        return {
            "top_keywords": top_keywords,
            "cluster_df": cluster_df,
            "stats_df": stats_df,
        }


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("ğŸš€ IT ë‰´ìŠ¤ í…ìŠ¤íŠ¸ ë¶„ì„ì„ ì‹œì‘í•©ë‹ˆë‹¤...")

    df = pd.read_csv("it_news_articles.csv")  # CSV íŒŒì¼ì—ì„œ ë°ì´í„° ë¡œë“œ

    # ë¶„ì„ê¸° ì´ˆê¸°í™”
    analyzer = NewsTextAnalyzer(df)

    # ì „ì²´ ë¶„ì„ ì‹¤í–‰
    results = analyzer.generate_full_analysis_report()

    print("\nğŸ“Š ì‹œê°í™”ë¥¼ ìƒì„±í•©ë‹ˆë‹¤...")

    # ê°ì¢… ì‹œê°í™” ì‹¤í–‰
    analyzer.visualize_top_keywords()
    analyzer.visualize_clusters()
    analyzer.create_wordcloud()
    analyzer.visualize_similarity_heatmap()
    analyzer.analyze_article_stats()

    print("âœ… ë¶„ì„ ì™„ë£Œ!")
    return analyzer, results


if __name__ == "__main__":
    analyzer, results = main()
