# flake8: noqa: E501
import numpy as np
import pandas as pd
from sklearn.feature_extraction.text import TfidfVectorizer


class TFIDFAnalyzer:
    """TF-IDF ê¸°ë°˜ í…ìŠ¤íŠ¸ ë¶„ì„"""

    # TODO
    def __init__(self, max_features=1000, ngram_range=(1, 2)):
        self.vectorizer = TfidfVectorizer(
            max_features=max_features, ngram_range=ngram_range, min_df=2, max_df=0.95
        )
        self.tfidf_matrix = None
        self.feature_names = None

    def fit_transform(self, texts):
        """TF-IDF ë²¡í„°í™” ìˆ˜í–‰"""
        self.tfidf_matrix = self.vectorizer.fit_transform(texts)
        self.feature_names = self.vectorizer.get_feature_names_out()

        print("âœ… TF-IDF ë²¡í„°í™” ì™„ë£Œ")
        print(f"   ë¬¸ì„œ ìˆ˜: {self.tfidf_matrix.shape[0]}")
        print(f"   íŠ¹ì„± ìˆ˜: {self.tfidf_matrix.shape[1]}")

        return self.tfidf_matrix

    def print_analyzer_result(self):
        tfidf_matrix = self.tfidf_matrix
        # 1. ê¸°ë³¸ ì •ë³´
        print(f"ğŸ“Š ë§¤íŠ¸ë¦­ìŠ¤ í¬ê¸°: {tfidf_matrix.shape}")
        print(f"   â€¢ ë¬¸ì„œ ìˆ˜: {tfidf_matrix.shape[0]}")
        print(f"   â€¢ íŠ¹ì„±(ë‹¨ì–´) ìˆ˜: {tfidf_matrix.shape[1]}")
        print(f"   â€¢ ë§¤íŠ¸ë¦­ìŠ¤ íƒ€ì…: {type(tfidf_matrix)}")
        print(f"   â€¢ ë°ì´í„° íƒ€ì…: {tfidf_matrix.dtype}")

        # 2. í¬ì†Œì„± ì •ë³´ (Sparsity)
        total_elements = tfidf_matrix.shape[0] * tfidf_matrix.shape[1]
        non_zero_elements = tfidf_matrix.nnz
        sparsity = (1 - non_zero_elements / total_elements) * 100
        print("\nğŸ•³ï¸  í¬ì†Œì„± ì •ë³´:")
        print(f"   â€¢ ì „ì²´ ì›ì†Œ ìˆ˜: {total_elements:,}")
        print(f"   â€¢ 0ì´ ì•„ë‹Œ ì›ì†Œ ìˆ˜: {non_zero_elements:,}")
        print(f"   â€¢ í¬ì†Œì„±: {sparsity:.2f}% (0ì¸ ì›ì†Œ ë¹„ìœ¨)")

        # 3. íŠ¹ì„±(ë‹¨ì–´) ì •ë³´
        feature_names = self.feature_names
        print("\nğŸ“ íŠ¹ì„±(ë‹¨ì–´) ì •ë³´:")
        print(f"   â€¢ ì²« 10ê°œ ë‹¨ì–´: {list(feature_names[:10])}")
        print(f"   â€¢ ë§ˆì§€ë§‰ 10ê°œ ë‹¨ì–´: {list(feature_names[-10:])}")

        # 4. ë¬¸ì„œë³„ í†µê³„
        print("\nğŸ“„ ë¬¸ì„œë³„ í†µê³„:")
        doc_word_counts = np.array(tfidf_matrix.sum(axis=1)).flatten()
        print(f"   â€¢ ë¬¸ì„œë‹¹ í‰ê·  TF-IDF í•©: {doc_word_counts.mean():.3f}")
        print(f"   â€¢ ìµœëŒ€ TF-IDF í•©: {doc_word_counts.max():.3f}")
        print(f"   â€¢ ìµœì†Œ TF-IDF í•©: {doc_word_counts.min():.3f}")

        # 5. ë‹¨ì–´ë³„ í†µê³„
        print("\nğŸ”¤ ë‹¨ì–´ë³„ í†µê³„:")
        word_frequencies = np.array(tfidf_matrix.sum(axis=0)).flatten()
        print(f"   â€¢ ë‹¨ì–´ë‹¹ í‰ê·  TF-IDF í•©: {word_frequencies.mean():.3f}")
        print(
            f"   â€¢ ìµœëŒ€ TF-IDF í•©ì„ ê°€ì§„ ë‹¨ì–´: {feature_names[word_frequencies.argmax()]}"
        )
        print(
            f"   â€¢ ìµœì†Œ TF-IDF í•©ì„ ê°€ì§„ ë‹¨ì–´: {feature_names[word_frequencies.argmin()]}"
        )

        # 6. ìƒìœ„ ì¤‘ìš” ë‹¨ì–´ë“¤
        top_word_indices = word_frequencies.argsort()[-10:][::-1]
        print("\nğŸ† ìƒìœ„ 10ê°œ ì¤‘ìš” ë‹¨ì–´ (TF-IDF í•© ê¸°ì¤€):")
        for i, idx in enumerate(top_word_indices, 1):
            word = feature_names[idx]
            score = word_frequencies[idx]
            print(f"   {i:2d}. {word:<15} (ì ìˆ˜: {score:.3f})")

    def get_top_keywords(self, n_keywords=20):
        """ì „ì²´ ì½”í¼ìŠ¤ì—ì„œ ìƒìœ„ í‚¤ì›Œë“œ ì¶”ì¶œ"""
        # ê° íŠ¹ì„±ì˜ í‰ê·  TF-IDF ì ìˆ˜ ê³„ì‚°
        mean_scores = np.array(self.tfidf_matrix.mean(axis=0)).flatten()

        # ìƒìœ„ í‚¤ì›Œë“œ ì¶”ì¶œ
        top_indices = mean_scores.argsort()[-n_keywords:][::-1]
        top_keywords = [(self.feature_names[i], mean_scores[i]) for i in top_indices]

        return top_keywords

    def get_document_keywords(self, doc_index, n_keywords=10):
        """íŠ¹ì • ë¬¸ì„œì˜ ìƒìœ„ í‚¤ì›Œë“œ ì¶”ì¶œ"""
        doc_scores = self.tfidf_matrix[doc_index].toarray().flatten()
        top_indices = doc_scores.argsort()[-n_keywords:][::-1]

        doc_keywords = [
            (self.feature_names[i], doc_scores[i])
            for i in top_indices
            if doc_scores[i] > 0
        ]

        return doc_keywords

    def analyze_keywords(self, df):
        """í‚¤ì›Œë“œ ë¶„ì„ ìˆ˜í–‰"""
        print("\nğŸ” TF-IDF í‚¤ì›Œë“œ ë¶„ì„")
        print("-" * 40)

        # ì „ì²´ ìƒìœ„ í‚¤ì›Œë“œ
        top_keywords = self.get_top_keywords(20)
        print("ğŸ“ˆ ì „ì²´ ìƒìœ„ í‚¤ì›Œë“œ:")
        for i, (keyword, score) in enumerate(top_keywords, 1):
            print(f"   {i:2d}. {keyword:<20} ({score:.4f})")

        # ê° ë¬¸ì„œë³„ í‚¤ì›Œë“œ
        print("\nğŸ“° ë¬¸ì„œë³„ ìƒìœ„ í‚¤ì›Œë“œ:")
        for i in range(min(3, len(df))):
            title = df.iloc[i]["Title"][:40]
            doc_keywords = self.get_document_keywords(i, 5)
            print(f"\n   ğŸ“„ {title}...")
            for keyword, score in doc_keywords:
                print(f"      â€¢ {keyword} ({score:.3f})")

    def export_keywords_to_csv(self, df, output_file="tfidf_keywords.csv"):
        """í‚¤ì›Œë“œ ë¶„ì„ ê²°ê³¼ë¥¼ CSVë¡œ ì €ì¥"""
        results = []

        for i in range(len(df)):
            doc_keywords = self.get_document_keywords(i, 10)
            keywords = [kw[0] for kw in doc_keywords]
            scores = [kw[1] for kw in doc_keywords]

            result = {
                "Document_Index": i,
                "Title": df.iloc[i]["Title"],
                "Top_Keywords": ", ".join(keywords[:5]),
                "Top_Keyword_Scores": ", ".join(
                    [f"{score:.3f}" for score in scores[:5]]
                ),
            }
            results.append(result)

        results_df = pd.DataFrame(results)
        results_df.to_csv(output_file, index=False)
        print(f"ğŸ’¾ í‚¤ì›Œë“œ ë¶„ì„ ê²°ê³¼ê°€ {output_file}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")

        return results_df


def main():
    print("ğŸš€ TF-IDF í…ìŠ¤íŠ¸ ë¶„ì„ ì‹œì‘")
    print("=" * 50)

    try:
        input_file = "it_news_articles_processed.csv"
        df = pd.read_csv(input_file)
        print(f"ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {len(df)}ê°œ ë¬¸ì„œ")

        # ë¹ˆ í…ìŠ¤íŠ¸ ì œê±°
        df = df[df["processed_content"].str.len() > 0]
        print(f"ìœ íš¨í•œ ë¬¸ì„œ: {len(df)}ê°œ")

        # TF-IDF ë¶„ì„ê¸° ì´ˆê¸°í™”
        analyzer = TFIDFAnalyzer(
            max_features=1500,  # íŠ¹ì„± ìˆ˜ ì¦ê°€
            ngram_range=(1, 3),  # 3-gramê¹Œì§€ í¬í•¨
        )

        # TF-IDF ë²¡í„°í™” ìˆ˜í–‰
        print("\nğŸ”„ TF-IDF ë²¡í„°í™” ì§„í–‰ ì¤‘...")
        analyzer.fit_transform(df["processed_content"])

        # í‚¤ì›Œë“œ ë¶„ì„ ìˆ˜í–‰
        analyzer.analyze_keywords(df)

        analyzer.print_analyzer_result()

        # ì „ì²´ ìƒìœ„ í‚¤ì›Œë“œë¥¼ DataFrameìœ¼ë¡œ ì €ì¥
        print("\nğŸ’¾ ê²°ê³¼ ì €ì¥ ì¤‘...")
        top_keywords = analyzer.get_top_keywords(50)
        keywords_df = pd.DataFrame(top_keywords, columns=["Keyword", "TF-IDF_Score"])
        keywords_df.to_csv("top_keywords.csv", index=False)
        print("âœ… ìƒìœ„ í‚¤ì›Œë“œê°€ 'top_keywords.csv'ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")

        # ë¬¸ì„œë³„ í‚¤ì›Œë“œ ë¶„ì„ ê²°ê³¼ ì €ì¥
        analyzer.export_keywords_to_csv(df, "document_keywords.csv")

    except Exception as e:
        print(f"âŒ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜ ë°œìƒ: {e}")


if __name__ == "__main__":
    main()
