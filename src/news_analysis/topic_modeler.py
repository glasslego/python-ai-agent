# flake8: noqa: E501

import warnings

import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
from sklearn.decomposition import LatentDirichletAllocation
from sklearn.feature_extraction.text import CountVectorizer
from wordcloud import WordCloud

warnings.filterwarnings("ignore")

# í•œê¸€ í°íŠ¸ ì„¤ì •
plt.rcParams["font.family"] = [
    "DejaVu Sans",
    "AppleGothic",
    "Malgun Gothic",
    "gulim",
    "Arial Unicode MS",
]
plt.rcParams["axes.unicode_minus"] = False  # ë§ˆì´ë„ˆìŠ¤ ê¸°í˜¸ ê¹¨ì§ ë°©ì§€


class TopicModeler:
    """LDA í† í”½ ëª¨ë¸ë§"""

    def __init__(self, n_topics=3, max_features=1000):
        self.n_topics = n_topics
        self.vectorizer = CountVectorizer(
            max_features=max_features, min_df=2, max_df=0.95, ngram_range=(1, 2)
        )
        self.lda = LatentDirichletAllocation(
            n_components=n_topics, random_state=42, max_iter=100
        )
        self.count_matrix = None
        self.doc_topic_probs = None

    def fit_transform(self, texts):
        """í† í”½ ëª¨ë¸ë§ ìˆ˜í–‰"""
        # CountVectorizerë¡œ ë³€í™˜
        self.count_matrix = self.vectorizer.fit_transform(texts)

        # LDA ëª¨ë¸ í•™ìŠµ
        self.lda.fit(self.count_matrix)

        # ë¬¸ì„œ-í† í”½ í™•ë¥  ë¶„í¬
        doc_topic_probs = self.lda.transform(self.count_matrix)

        print(f"âœ… LDA í† í”½ ëª¨ë¸ë§ ì™„ë£Œ ({self.n_topics}ê°œ í† í”½)")

        return doc_topic_probs

    def get_top_words_per_topic(self, n_words=10):
        """ê° í† í”½ì˜ ìƒìœ„ ë‹¨ì–´ë“¤ ì¶”ì¶œ"""
        feature_names = self.vectorizer.get_feature_names_out()
        topics = []

        for topic_idx, topic in enumerate(self.lda.components_):
            top_words_idx = topic.argsort()[-n_words:][::-1]
            top_words = [feature_names[i] for i in top_words_idx]
            topics.append(top_words)

        return topics

    def analyze_topics(self, df, doc_topic_probs):
        """í† í”½ ë¶„ì„ ê²°ê³¼ ì¶œë ¥"""
        print("\nğŸ­ í† í”½ ëª¨ë¸ë§ ê²°ê³¼")
        print("-" * 40)

        # ê° í† í”½ì˜ ìƒìœ„ ë‹¨ì–´ë“¤
        topics = self.get_top_words_per_topic()

        for i, topic_words in enumerate(topics):
            print(f"\nğŸ“š í† í”½ {i + 1}:")
            print(f"   ì£¼ìš” ë‹¨ì–´: {', '.join(topic_words[:8])}")

        # ê° ë¬¸ì„œì˜ ì£¼ìš” í† í”½ í• ë‹¹
        df_with_topics = df.copy()
        df_with_topics["main_topic"] = doc_topic_probs.argmax(axis=1)
        df_with_topics["topic_confidence"] = doc_topic_probs.max(axis=1)

        print("\nğŸ“° ë¬¸ì„œë³„ í† í”½ í• ë‹¹:")
        for _, row in df_with_topics.head(5).iterrows():
            title = row["Title"][:40]
            topic = row["main_topic"]
            confidence = row["topic_confidence"]
            print(f"   â€¢ {title}... â†’ í† í”½ {topic + 1} ({confidence:.2f})")

        return df_with_topics

    def visualize_topics(self, df_with_topics):
        """í† í”½ ì‹œê°í™”"""
        print("\nğŸ“Š í† í”½ ì‹œê°í™” ìƒì„± ì¤‘...")

        fig, axes = plt.subplots(2, 2, figsize=(16, 12))

        # 1. í† í”½ë³„ ë¬¸ì„œ ë¶„í¬
        topic_counts = df_with_topics["main_topic"].value_counts().sort_index()
        colors = ["#FF6B6B", "#4ECDC4", "#45B7D1", "#96CEB4", "#FFEAA7", "#DDA0DD"]

        axes[0, 0].pie(
            topic_counts.values,
            labels=[f"Topic {i + 1}" for i in topic_counts.index],
            autopct="%1.1f%%",
            colors=colors[: len(topic_counts)],
        )
        axes[0, 0].set_title(
            "Document Distribution by Topic", fontsize=14, fontweight="bold"
        )

        # 2. í† í”½ë³„ ì‹ ë¢°ë„ ë¶„í¬
        for topic_id in range(self.n_topics):
            topic_data = df_with_topics[df_with_topics["main_topic"] == topic_id]
            if len(topic_data) > 0:
                axes[0, 1].hist(
                    topic_data["topic_confidence"],
                    alpha=0.7,
                    label=f"Topic {topic_id + 1}",
                    color=colors[topic_id % len(colors)],
                    bins=10,
                )

        axes[0, 1].set_title(
            "Topic Confidence Distribution", fontsize=14, fontweight="bold"
        )
        axes[0, 1].set_xlabel("Confidence Score")
        axes[0, 1].set_ylabel("Frequency")
        axes[0, 1].legend()
        axes[0, 1].grid(True, alpha=0.3)

        # 3. í† í”½-ë‹¨ì–´ íˆíŠ¸ë§µ
        feature_names = self.vectorizer.get_feature_names_out()
        top_words_per_topic = []
        word_scores_per_topic = []

        for topic_idx in range(self.n_topics):
            topic = self.lda.components_[topic_idx]
            top_words_idx = topic.argsort()[-10:][::-1]
            top_words = [feature_names[i] for i in top_words_idx]
            top_scores = [topic[i] for i in top_words_idx]

            top_words_per_topic.extend(top_words)
            word_scores_per_topic.append(top_scores)

        # ì¤‘ë³µ ì œê±°í•˜ì—¬ ìƒìœ„ ë‹¨ì–´ë“¤ë§Œ ì„ íƒ
        unique_words = list(set(top_words_per_topic))[:15]
        heatmap_data = []

        for topic_idx in range(self.n_topics):
            topic = self.lda.components_[topic_idx]
            topic_scores = []
            for word in unique_words:
                if word in feature_names:
                    word_idx = list(feature_names).index(word)
                    topic_scores.append(topic[word_idx])
                else:
                    topic_scores.append(0)
            heatmap_data.append(topic_scores)

        im = axes[1, 0].imshow(heatmap_data, cmap="YlOrRd", aspect="auto")
        axes[1, 0].set_xticks(range(len(unique_words)))
        axes[1, 0].set_xticklabels(unique_words, rotation=45, ha="right")
        axes[1, 0].set_yticks(range(self.n_topics))
        axes[1, 0].set_yticklabels([f"Topic {i + 1}" for i in range(self.n_topics)])
        axes[1, 0].set_title("Topic-Word Heatmap", fontsize=14, fontweight="bold")

        # ì»¬ëŸ¬ë°” ì¶”ê°€
        plt.colorbar(im, ax=axes[1, 0], shrink=0.8)

        # 4. í† í”½ë³„ í‰ê·  ì‹ ë¢°ë„
        avg_confidence = df_with_topics.groupby("main_topic")["topic_confidence"].mean()
        bars = axes[1, 1].bar(
            range(self.n_topics), avg_confidence.values, color=colors[: self.n_topics]
        )
        axes[1, 1].set_title("Average Topic Confidence", fontsize=14, fontweight="bold")
        axes[1, 1].set_xlabel("Topic")
        axes[1, 1].set_ylabel("Average Confidence")
        axes[1, 1].set_xticks(range(self.n_topics))
        axes[1, 1].set_xticklabels([f"Topic {i + 1}" for i in range(self.n_topics)])
        axes[1, 1].grid(True, alpha=0.3)

        # ë§‰ëŒ€ ìœ„ì— ê°’ í‘œì‹œ
        for bar, value in zip(bars, avg_confidence.values):
            axes[1, 1].text(
                bar.get_x() + bar.get_width() / 2,
                bar.get_height() + 0.01,
                f"{value:.3f}",
                ha="center",
                va="bottom",
            )

        plt.tight_layout()
        plt.savefig("topic_modeling_analysis.png", dpi=300, bbox_inches="tight")
        plt.show()

        print("   ğŸ“ ì‹œê°í™” ê²°ê³¼ê°€ 'topic_modeling_analysis.png'ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")

    def generate_wordclouds(self):
        """í† í”½ë³„ ì›Œë“œí´ë¼ìš°ë“œ ìƒì„±"""
        print("\nâ˜ï¸  í† í”½ë³„ ì›Œë“œí´ë¼ìš°ë“œ ìƒì„± ì¤‘...")

        feature_names = self.vectorizer.get_feature_names_out()

        fig, axes = plt.subplots(1, self.n_topics, figsize=(5 * self.n_topics, 5))
        if self.n_topics == 1:
            axes = [axes]

        for topic_idx in range(self.n_topics):
            topic = self.lda.components_[topic_idx]

            # í† í”½ì˜ ë‹¨ì–´-í™•ë¥  ë”•ì…”ë„ˆë¦¬ ìƒì„±
            word_prob_dict = {}
            for word_idx, prob in enumerate(topic):
                if prob > 0.001:  # ìµœì†Œ ì„ê³„ê°’
                    word_prob_dict[feature_names[word_idx]] = prob

            if word_prob_dict:  # ë‹¨ì–´ê°€ ìˆëŠ” ê²½ìš°ì—ë§Œ ì›Œë“œí´ë¼ìš°ë“œ ìƒì„±
                wordcloud = WordCloud(
                    width=400,
                    height=400,
                    background_color="white",
                    max_words=50,
                    colormap="viridis",
                ).generate_from_frequencies(word_prob_dict)

                axes[topic_idx].imshow(wordcloud, interpolation="bilinear")
                axes[topic_idx].set_title(
                    f"Topic {topic_idx + 1}", fontsize=14, fontweight="bold"
                )
            else:
                axes[topic_idx].text(
                    0.5,
                    0.5,
                    "No significant words",
                    ha="center",
                    va="center",
                    transform=axes[topic_idx].transAxes,
                )
                axes[topic_idx].set_title(
                    f"Topic {topic_idx + 1}", fontsize=14, fontweight="bold"
                )

            axes[topic_idx].axis("off")

        plt.tight_layout()
        plt.savefig("topic_wordclouds.png", dpi=300, bbox_inches="tight")
        plt.show()

        print("   ğŸ“ ì›Œë“œí´ë¼ìš°ë“œê°€ 'topic_wordclouds.png'ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")

    def get_topic_coherence(self):
        """í† í”½ ì¼ê´€ì„± ì ìˆ˜ ê³„ì‚° (ê°„ë‹¨í•œ ë²„ì „)"""
        topics = self.get_top_words_per_topic(10)
        feature_names = self.vectorizer.get_feature_names_out()

        coherence_scores = []
        for topic_words in topics:
            # í† í”½ ë‚´ ë‹¨ì–´ë“¤ì˜ ë™ì‹œ ì¶œí˜„ ë¹ˆë„ ê³„ì‚°
            word_indices = [
                list(feature_names).index(word)
                for word in topic_words
                if word in feature_names
            ]

            if len(word_indices) < 2:
                coherence_scores.append(0)
                continue

            # ê°„ë‹¨í•œ ì¼ê´€ì„± ì ìˆ˜ (ì‹¤ì œë¡œëŠ” ë” ë³µì¡í•œ ê³„ì‚° í•„ìš”)
            cooccurrence_sum = 0
            count = 0

            for i in range(len(word_indices)):
                for j in range(i + 1, len(word_indices)):
                    word1_counts = (
                        self.count_matrix[:, word_indices[i]].toarray().flatten()
                    )
                    word2_counts = (
                        self.count_matrix[:, word_indices[j]].toarray().flatten()
                    )

                    # ë‘ ë‹¨ì–´ê°€ ëª¨ë‘ ë‚˜íƒ€ë‚˜ëŠ” ë¬¸ì„œ ìˆ˜
                    cooccurrence = np.sum((word1_counts > 0) & (word2_counts > 0))
                    cooccurrence_sum += cooccurrence
                    count += 1

            coherence_scores.append(cooccurrence_sum / count if count > 0 else 0)

        return coherence_scores


def find_optimal_topics(texts, max_topics=10):
    """ìµœì  í† í”½ ìˆ˜ ì°¾ê¸°"""
    print("\nğŸ” ìµœì  í† í”½ ìˆ˜ íƒìƒ‰ ì¤‘...")

    # ë°ì´í„° ìˆ˜ì— ë”°ë¼ ìµœëŒ€ í† í”½ ìˆ˜ ì¡°ì •
    max_topics = min(max_topics, len(texts) // 3)

    if max_topics < 2:
        print("   âš ï¸  ë°ì´í„°ê°€ ë„ˆë¬´ ì ì–´ í† í”½ ìˆ˜ ìµœì í™”ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.")
        return 2

    perplexities = []
    topic_range = range(2, max_topics + 1)

    for n_topics in topic_range:
        vectorizer = CountVectorizer(
            max_features=1000, min_df=2, max_df=0.95, ngram_range=(1, 2)
        )
        count_matrix = vectorizer.fit_transform(texts)

        lda = LatentDirichletAllocation(
            n_components=n_topics, random_state=42, max_iter=50
        )
        lda.fit(count_matrix)

        perplexity = lda.perplexity(count_matrix)
        perplexities.append(perplexity)

        print(f"   í† í”½ ìˆ˜ {n_topics}: Perplexity = {perplexity:.2f}")

    # ê°€ì¥ ë‚®ì€ perplexityë¥¼ ê°€ì§„ í† í”½ ìˆ˜ ì„ íƒ
    optimal_idx = np.argmin(perplexities)
    optimal_topics = topic_range[optimal_idx]

    print(
        f"\nâœ… ìµœì  í† í”½ ìˆ˜: {optimal_topics} (Perplexity: {perplexities[optimal_idx]:.2f})"
    )

    return optimal_topics


def save_topic_results(df_with_topics, topic_modeler):
    """í† í”½ ëª¨ë¸ë§ ê²°ê³¼ ì €ì¥"""
    print("\nğŸ’¾ ê²°ê³¼ ì €ì¥ ì¤‘...")

    # 1. ë¬¸ì„œë³„ í† í”½ ê²°ê³¼ ì €ì¥
    df_with_topics.to_csv(
        "topic_modeling_results.csv", index=False, encoding="utf-8-sig"
    )
    print("   ğŸ“„ í† í”½ ëª¨ë¸ë§ ê²°ê³¼: topic_modeling_results.csv")

    # 2. í† í”½ë³„ ìƒìœ„ ë‹¨ì–´ ì €ì¥
    topics = topic_modeler.get_top_words_per_topic(15)
    topic_words_data = []

    for i, topic_words in enumerate(topics):
        topic_words_data.append(
            {
                "topic_id": i + 1,
                "top_words": ", ".join(topic_words),
                "word_list": topic_words,
            }
        )

    topic_df = pd.DataFrame(topic_words_data)
    topic_df.to_csv("topic_words.csv", index=False, encoding="utf-8-sig")
    print("   ğŸ”¤ í† í”½ë³„ ë‹¨ì–´: topic_words.csv")

    # 3. í† í”½ë³„ ë¬¸ì„œ ìš”ì•½ ì €ì¥
    topic_summary = []
    for topic_id in range(topic_modeler.n_topics):
        topic_docs = df_with_topics[df_with_topics["main_topic"] == topic_id]

        summary = {
            "topic_id": topic_id + 1,
            "document_count": len(topic_docs),
            "avg_confidence": (
                topic_docs["topic_confidence"].mean() if len(topic_docs) > 0 else 0
            ),
            "sample_titles": (
                topic_docs["Title"].head(3).tolist() if len(topic_docs) > 0 else []
            ),
            "top_words": ", ".join(topics[topic_id][:5]),
        }
        topic_summary.append(summary)

    summary_df = pd.DataFrame(topic_summary)
    summary_df.to_csv("topic_summary.csv", index=False, encoding="utf-8-sig")
    print("   ğŸ“Š í† í”½ ìš”ì•½: topic_summary.csv")

    # 4. ë¬¸ì„œ-í† í”½ í™•ë¥  í–‰ë ¬ ì €ì¥
    prob_df = pd.DataFrame(
        topic_modeler.doc_topic_probs,
        columns=[f"Topic_{i + 1}_Prob" for i in range(topic_modeler.n_topics)],
    )
    prob_df["Title"] = df_with_topics["Title"]
    prob_df.to_csv(
        "document_topic_probabilities.csv", index=False, encoding="utf-8-sig"
    )
    print("   ğŸ“ˆ ë¬¸ì„œ-í† í”½ í™•ë¥ : document_topic_probabilities.csv")


def main():
    print("ğŸ­ LDA í† í”½ ëª¨ë¸ë§ ì‹œìŠ¤í…œì„ ì‹œì‘í•©ë‹ˆë‹¤!")
    print("=" * 50)

    # ë°ì´í„° ë¡œë“œ ë˜ëŠ” ìƒì„±
    print("\nğŸ“‚ ë°ì´í„° ë¡œë“œ ì¤‘...")
    df = pd.read_csv("it_news_articles_processed.csv")

    # ìµœì  í† í”½ ìˆ˜ ì°¾ê¸°
    # if len(df) > 6:  # í† í”½ ìˆ˜ ìµœì í™”ëŠ” ë°ì´í„°ê°€ ì¶©ë¶„í•  ë•Œë§Œ
    #     optimal_topics = find_optimal_topics(df["processed_content"])
    # else:
    #     optimal_topics = min(3, len(df) - 1)
    #     print(f"\nâš ï¸  ë°ì´í„° ìˆ˜ê°€ ì ì–´ í† í”½ ìˆ˜ë¥¼ {optimal_topics}ê°œë¡œ ì„¤ì •í•©ë‹ˆë‹¤.")
    #
    # # í† í”½ ëª¨ë¸ë§ ìˆ˜í–‰
    # print(f"\nğŸ¯ LDA í† í”½ ëª¨ë¸ë§ ìˆ˜í–‰ ì¤‘... (í† í”½ ìˆ˜: {optimal_topics})")
    optimal_topics = 5

    topic_modeler = TopicModeler(n_topics=optimal_topics, max_features=1000)
    doc_topic_probs = topic_modeler.fit_transform(df["processed_content"])

    # í† í”½ ë¶„ì„
    df_with_topics = topic_modeler.analyze_topics(df, doc_topic_probs)

    # í† í”½ ì¼ê´€ì„± ì ìˆ˜ ê³„ì‚°
    print("\nğŸ“ í† í”½ ì¼ê´€ì„± í‰ê°€ ì¤‘...")
    coherence_scores = topic_modeler.get_topic_coherence()

    print("   í† í”½ë³„ ì¼ê´€ì„± ì ìˆ˜:")
    for i, score in enumerate(coherence_scores):
        print(f"   â€¢ í† í”½ {i + 1}: {score:.3f}")

    avg_coherence = np.mean(coherence_scores)
    print(f"   í‰ê·  ì¼ê´€ì„± ì ìˆ˜: {avg_coherence:.3f}")

    # ì‹œê°í™”
    if len(df) > 2:  # ì‹œê°í™”ëŠ” ìµœì†Œ 3ê°œ ì´ìƒì˜ ë°ì´í„°ê°€ ìˆì„ ë•Œ
        topic_modeler.visualize_topics(df_with_topics)

        # ì›Œë“œí´ë¼ìš°ë“œ ìƒì„± (ì˜µì…˜)
        try:
            topic_modeler.generate_wordclouds()
        except ImportError:
            print(
                "   âš ï¸  WordCloud ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•„ ì›Œë“œí´ë¼ìš°ë“œë¥¼ ìƒëµí•©ë‹ˆë‹¤."
            )
            print("   ğŸ’¡ 'pip install wordcloud' ëª…ë ¹ì–´ë¡œ ì„¤ì¹˜ í›„ ë‹¤ì‹œ ì‹¤í–‰í•´ë³´ì„¸ìš”.")
    else:
        print("\nâš ï¸  ë°ì´í„° ìˆ˜ê°€ ì ì–´ ì‹œê°í™”ë¥¼ ìƒëµí•©ë‹ˆë‹¤.")

    # ê²°ê³¼ ì €ì¥
    save_topic_results(df_with_topics, topic_modeler)

    # ìƒì„¸ í† í”½ ë¶„ì„
    print("\nğŸ” ìƒì„¸ í† í”½ ë¶„ì„")
    print("=" * 30)

    topics = topic_modeler.get_top_words_per_topic(10)
    for i, topic_words in enumerate(topics):
        topic_docs = df_with_topics[df_with_topics["main_topic"] == i]

        print(f"\nğŸ“š í† í”½ {i + 1} ìƒì„¸ ë¶„ì„:")
        print(f"   â€¢ ë¬¸ì„œ ìˆ˜: {len(topic_docs)}ê°œ")
        print(f"   â€¢ í‰ê·  ì‹ ë¢°ë„: {topic_docs['topic_confidence'].mean():.3f}")
        print(f"   â€¢ ì£¼ìš” í‚¤ì›Œë“œ: {', '.join(topic_words[:5])}")
        print(f"   â€¢ ì¼ê´€ì„± ì ìˆ˜: {coherence_scores[i]:.3f}")

        if len(topic_docs) > 0:
            print("   â€¢ ëŒ€í‘œ ê¸°ì‚¬:")
            top_confident = topic_docs.nlargest(2, "topic_confidence")
            for _, article in top_confident.iterrows():
                print(
                    f"     - {article['Title'][:50]}... ({article['topic_confidence']:.3f})"
                )

    # í† í”½ íŠ¸ë Œë“œ ë¶„ì„ (ë‚ ì§œê°€ ìˆëŠ” ê²½ìš°)
    if "Date" in df.columns:
        print("\nğŸ“ˆ í† í”½ íŠ¸ë Œë“œ ë¶„ì„")
        try:
            df_with_topics["Date"] = pd.to_datetime(df_with_topics["Date"])
            df_with_topics["Month"] = df_with_topics["Date"].dt.to_period("M")

            trend_data = (
                df_with_topics.groupby(["Month", "main_topic"])
                .size()
                .unstack(fill_value=0)
            )
            trend_data.to_csv("topic_trends.csv", encoding="utf-8-sig")
            print("   ğŸ“Š í† í”½ íŠ¸ë Œë“œ ë°ì´í„°: topic_trends.csv")
        except Exception as e:
            print("error during trend analysis:", e)

    # ìµœì¢… ìš”ì•½ ë¦¬í¬íŠ¸
    print("\nğŸ“Š ìµœì¢… ê²°ê³¼ ìš”ì•½")
    print("=" * 30)
    print(f"â€¢ ì´ ê¸°ì‚¬ ìˆ˜: {len(df)}")
    print(f"â€¢ ì¶”ì¶œëœ í† í”½ ìˆ˜: {optimal_topics}")
    print(f"â€¢ í‰ê·  í† í”½ ì¼ê´€ì„±: {avg_coherence:.3f}")
    print("â€¢ í† í”½ë³„ ë¬¸ì„œ ë¶„í¬:")

    for topic_id in range(optimal_topics):
        count = len(df_with_topics[df_with_topics["main_topic"] == topic_id])
        percentage = (count / len(df)) * 100
        print(f"  - í† í”½ {topic_id + 1}: {count}ê°œ ({percentage:.1f}%)")

    print("\nğŸ“ ìƒì„±ëœ íŒŒì¼:")
    print("  â€¢ topic_modeling_results.csv - ë¬¸ì„œë³„ í† í”½ í• ë‹¹ ê²°ê³¼")
    print("  â€¢ topic_words.csv - í† í”½ë³„ ì£¼ìš” ë‹¨ì–´")
    print("  â€¢ topic_summary.csv - í† í”½ë³„ ìš”ì•½ ì •ë³´")
    # TODO ìˆ˜ì •í•˜ê¸°
    print("  â€¢ document_topic_probabilities.csv - ë¬¸ì„œ-í† í”½ í™•ë¥  í–‰ë ¬")
    if len(df) > 2:
        print("  â€¢ topic_modeling_analysis.png - í† í”½ ë¶„ì„ ì‹œê°í™”")
        try:
            print("  â€¢ topic_wordclouds.png - í† í”½ë³„ ì›Œë“œí´ë¼ìš°ë“œ")
        except ImportError:
            pass

    # í† í”½ ë¼ë²¨ë§ ì œì•ˆ
    print("\nğŸ·ï¸  í† í”½ ë¼ë²¨ë§ ì œì•ˆ")
    print("-" * 20)

    topic_labels = []
    for i, topic_words in enumerate(topics):
        # ìƒìœ„ 3ê°œ ë‹¨ì–´ë¥¼ ì¡°í•©í•˜ì—¬ ë¼ë²¨ ì œì•ˆ
        label = " & ".join(topic_words[:3]).title()
        topic_labels.append(label)
        print(f'   í† í”½ {i + 1}: "{label}"')

    # ë¼ë²¨ì´ ì ìš©ëœ ê²°ê³¼ ì €ì¥
    df_labeled = df_with_topics.copy()
    df_labeled["topic_label"] = df_labeled["main_topic"].map(
        {i: topic_labels[i] for i in range(len(topic_labels))}
    )
    df_labeled.to_csv("topic_modeling_labeled.csv", index=False, encoding="utf-8-sig")
    print("  â€¢ topic_modeling_labeled.csv - ë¼ë²¨ì´ ì ìš©ëœ ê²°ê³¼")

    print("\nğŸ‰ í† í”½ ëª¨ë¸ë§ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")

    return df_with_topics, topic_modeler, topic_labels


def export_to_json(
    df_with_topics, topic_modeler, filename="topic_modeling_export.json"
):
    """JSON í˜•ì‹ìœ¼ë¡œ ê²°ê³¼ ë‚´ë³´ë‚´ê¸°"""
    import json

    topics = topic_modeler.get_top_words_per_topic(15)

    export_data = {
        "model_info": {
            "n_topics": topic_modeler.n_topics,
            "n_documents": len(df_with_topics),
            "timestamp": pd.Timestamp.now().isoformat(),
        },
        "topics": [],
        "documents": [],
    }

    # í† í”½ ì •ë³´
    for i, topic_words in enumerate(topics):
        topic_docs = df_with_topics[df_with_topics["main_topic"] == i]

        export_data["topics"].append(
            {
                "id": i,
                "words": topic_words,
                "document_count": len(topic_docs),
                "avg_confidence": (
                    float(topic_docs["topic_confidence"].mean())
                    if len(topic_docs) > 0
                    else 0
                ),
            }
        )

    # ë¬¸ì„œ ì •ë³´
    for idx, row in df_with_topics.iterrows():
        doc_info = {
            "id": idx,
            "title": row["Title"],
            "main_topic": int(row["main_topic"]),
            "topic_confidence": float(row["topic_confidence"]),
            "topic_probabilities": (
                topic_modeler.doc_topic_probs[idx].tolist()
                if topic_modeler.doc_topic_probs is not None
                else []
            ),
        }
        export_data["documents"].append(doc_info)

    with open(filename, "w", encoding="utf-8") as f:
        json.dump(export_data, f, ensure_ascii=False, indent=2)

    print(f"   ğŸ“„ JSON ë‚´ë³´ë‚´ê¸° ì™„ë£Œ: {filename}")


if __name__ == "__main__":
    # ì‹¤í–‰
    results = main()

    # ê²°ê³¼ ì‚¬ìš© ì˜ˆì‹œ
    df_with_topics, topic_modeler, topic_labels = results

    # JSONìœ¼ë¡œ ë‚´ë³´ë‚´ê¸°
    export_to_json(df_with_topics, topic_modeler)
