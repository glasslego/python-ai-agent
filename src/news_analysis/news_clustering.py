# flake8: noqa: E501
import warnings

import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
from sklearn.cluster import KMeans
from sklearn.decomposition import PCA
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics import silhouette_score

warnings.filterwarnings("ignore")


class NewsClustering:
    """ë‰´ìŠ¤ ê¸°ì‚¬ í´ëŸ¬ìŠ¤í„°ë§"""

    def __init__(self, n_clusters=3):
        self.n_clusters = n_clusters
        self.kmeans = KMeans(n_clusters=n_clusters, random_state=42)
        self.labels = None

    def fit_predict(self, tfidf_matrix):
        """í´ëŸ¬ìŠ¤í„°ë§ ìˆ˜í–‰"""
        self.labels = self.kmeans.fit_predict(tfidf_matrix)

        print(f"âœ… K-Means í´ëŸ¬ìŠ¤í„°ë§ ì™„ë£Œ ({self.n_clusters}ê°œ í´ëŸ¬ìŠ¤í„°)")

        return self.labels

    def analyze_clusters(self, df, feature_names, tfidf_matrix):
        """í´ëŸ¬ìŠ¤í„° ë¶„ì„"""
        print("\nğŸ¯ í´ëŸ¬ìŠ¤í„° ë¶„ì„ ê²°ê³¼")
        print("-" * 40)

        df_with_clusters = df.copy()
        df_with_clusters["cluster"] = self.labels

        for cluster_id in range(self.n_clusters):
            cluster_docs = df_with_clusters[df_with_clusters["cluster"] == cluster_id]
            print(f"\nğŸ“Š í´ëŸ¬ìŠ¤í„° {cluster_id} ({len(cluster_docs)}ê°œ ê¸°ì‚¬):")

            # í´ëŸ¬ìŠ¤í„° ë‚´ ê¸°ì‚¬ ì œëª©ë“¤
            for title in cluster_docs["Title"].head(3):
                print(f"   â€¢ {title[:50]}...")

            # í´ëŸ¬ìŠ¤í„° ì¤‘ì‹¬ í‚¤ì›Œë“œ
            cluster_center = self.kmeans.cluster_centers_[cluster_id]
            top_indices = cluster_center.argsort()[-10:][::-1]

            print("   ğŸ”‘ ì£¼ìš” í‚¤ì›Œë“œ:")
            for i in top_indices:
                if cluster_center[i] > 0.01:
                    print(f"      - {feature_names[i]} ({cluster_center[i]:.3f})")

        return df_with_clusters

    def visualize_clusters(self, tfidf_matrix, df_with_clusters):
        """í´ëŸ¬ìŠ¤í„° ì‹œê°í™”"""
        print("\nğŸ“ˆ í´ëŸ¬ìŠ¤í„° ì‹œê°í™” ìƒì„± ì¤‘...")

        # PCAë¡œ ì°¨ì› ì¶•ì†Œ (2D)
        pca = PCA(n_components=2, random_state=42)
        coords_2d = pca.fit_transform(tfidf_matrix.toarray())

        # ê·¸ë˜í”„ ì„¤ì •
        plt.figure(figsize=(15, 5))

        # 1. í´ëŸ¬ìŠ¤í„° ë¶„í¬ ì‹œê°í™”
        plt.subplot(1, 3, 1)
        colors = ["red", "blue", "green", "purple", "orange", "brown", "pink", "gray"]

        for i in range(self.n_clusters):
            cluster_points = coords_2d[self.labels == i]
            plt.scatter(
                cluster_points[:, 0],
                cluster_points[:, 1],
                c=colors[i % len(colors)],
                label=f"Cluster {i}",
                alpha=0.6,
            )

        plt.title("News Clusters (PCA 2D)")
        plt.xlabel("PC1")
        plt.ylabel("PC2")
        plt.legend()
        plt.grid(True, alpha=0.3)

        # 2. í´ëŸ¬ìŠ¤í„° í¬ê¸° ë¶„í¬
        plt.subplot(1, 3, 2)
        cluster_counts = df_with_clusters["cluster"].value_counts().sort_index()
        plt.bar(
            range(self.n_clusters),
            cluster_counts.values,
            color=[colors[i % len(colors)] for i in range(self.n_clusters)],
        )
        plt.title("Cluster Size Distribution")
        plt.xlabel("Cluster ID")
        plt.ylabel("Number of Articles")
        plt.grid(True, alpha=0.3)

        # ê° ë§‰ëŒ€ ìœ„ì— ìˆ«ì í‘œì‹œ
        for i, count in enumerate(cluster_counts.values):
            plt.text(i, count + 0.5, str(count), ha="center", va="bottom")

        # 3. ì‹¤ë£¨ì—£ ì ìˆ˜ (í´ëŸ¬ìŠ¤í„° í’ˆì§ˆ í‰ê°€)
        plt.subplot(1, 3, 3)
        silhouette_avg = silhouette_score(tfidf_matrix, self.labels)

        # í´ëŸ¬ìŠ¤í„° ê°œìˆ˜ë³„ ì‹¤ë£¨ì—£ ì ìˆ˜ ê³„ì‚°
        cluster_range = range(2, min(8, len(df_with_clusters) // 2))
        silhouette_scores = []

        for n in cluster_range:
            kmeans_temp = KMeans(n_clusters=n, random_state=42)
            temp_labels = kmeans_temp.fit_predict(tfidf_matrix)
            score = silhouette_score(tfidf_matrix, temp_labels)
            silhouette_scores.append(score)

        plt.plot(cluster_range, silhouette_scores, "bo-")
        plt.axvline(
            x=self.n_clusters,
            color="red",
            linestyle="--",
            label=f"Current ({self.n_clusters} clusters)",
        )
        plt.title("Silhouette Score by Cluster Count")
        plt.xlabel("Number of Clusters")
        plt.ylabel("Silhouette Score")
        plt.legend()
        plt.grid(True, alpha=0.3)

        plt.tight_layout()
        plt.savefig("news_clustering_analysis.png", dpi=300, bbox_inches="tight")
        plt.show()

        print(f"   í˜„ì¬ í´ëŸ¬ìŠ¤í„°ë§ ì‹¤ë£¨ì—£ ì ìˆ˜: {silhouette_avg:.3f}")
        print("   ğŸ“ ì‹œê°í™” ê²°ê³¼ê°€ 'news_clustering_analysis.png'ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")


def find_optimal_clusters(tfidf_matrix, max_clusters=8):
    """ìµœì  í´ëŸ¬ìŠ¤í„° ìˆ˜ ì°¾ê¸°"""
    print("\nğŸ” ìµœì  í´ëŸ¬ìŠ¤í„° ìˆ˜ íƒìƒ‰ ì¤‘...")

    silhouette_scores = []
    inertias = []
    cluster_range = range(2, min(max_clusters + 1, len(tfidf_matrix.toarray()) // 2))

    for n_clusters in cluster_range:
        kmeans = KMeans(n_clusters=n_clusters, random_state=42)
        labels = kmeans.fit_predict(tfidf_matrix)

        silhouette_avg = silhouette_score(tfidf_matrix, labels)
        silhouette_scores.append(silhouette_avg)
        inertias.append(kmeans.inertia_)

        print(f"   í´ëŸ¬ìŠ¤í„° ìˆ˜ {n_clusters}: ì‹¤ë£¨ì—£ ì ìˆ˜ = {silhouette_avg:.3f}")

    # ìµœì  í´ëŸ¬ìŠ¤í„° ìˆ˜ (ì‹¤ë£¨ì—£ ì ìˆ˜ ê¸°ì¤€)
    optimal_idx = np.argmax(silhouette_scores)
    optimal_clusters = cluster_range[optimal_idx]

    print(
        f"\nâœ… ìµœì  í´ëŸ¬ìŠ¤í„° ìˆ˜: {optimal_clusters} (ì‹¤ë£¨ì—£ ì ìˆ˜: {silhouette_scores[optimal_idx]:.3f})"
    )

    return optimal_clusters, silhouette_scores, inertias


def save_results(df_with_clusters, feature_names, clustering_model):
    """ê²°ê³¼ ì €ì¥"""
    print("\nğŸ’¾ ê²°ê³¼ ì €ì¥ ì¤‘...")

    # 1. í´ëŸ¬ìŠ¤í„°ë§ ê²°ê³¼ CSV ì €ì¥
    df_with_clusters.to_csv(
        "news_clustering_results.csv", index=False, encoding="utf-8-sig"
    )
    print("   ğŸ“„ í´ëŸ¬ìŠ¤í„°ë§ ê²°ê³¼: news_clustering_results.csv")

    # 2. í´ëŸ¬ìŠ¤í„°ë³„ í‚¤ì›Œë“œ ì €ì¥
    cluster_keywords = {}
    for cluster_id in range(clustering_model.n_clusters):
        cluster_center = clustering_model.kmeans.cluster_centers_[cluster_id]
        top_indices = cluster_center.argsort()[-10:][::-1]

        keywords = []
        for i in top_indices:
            if cluster_center[i] > 0.01:
                keywords.append(
                    {"keyword": feature_names[i], "score": cluster_center[i]}
                )

        cluster_keywords[f"cluster_{cluster_id}"] = keywords

    # í‚¤ì›Œë“œ ê²°ê³¼ë¥¼ JSONìœ¼ë¡œ ì €ì¥
    import json

    with open("cluster_keywords.json", "w", encoding="utf-8") as f:
        json.dump(cluster_keywords, f, ensure_ascii=False, indent=2)
    print("   ğŸ”‘ í´ëŸ¬ìŠ¤í„° í‚¤ì›Œë“œ: cluster_keywords.json")

    # 3. í´ëŸ¬ìŠ¤í„°ë³„ ê¸°ì‚¬ ìš”ì•½ ì €ì¥
    cluster_summary = []
    for cluster_id in range(clustering_model.n_clusters):
        cluster_docs = df_with_clusters[df_with_clusters["cluster"] == cluster_id]

        summary = {
            "cluster_id": cluster_id,
            "article_count": len(cluster_docs),
            "sample_titles": cluster_docs["Title"].head(5).tolist(),
            "top_keywords": [
                kw["keyword"] for kw in cluster_keywords[f"cluster_{cluster_id}"][:5]
            ],
        }
        cluster_summary.append(summary)

    summary_df = pd.DataFrame(cluster_summary)
    summary_df.to_csv("cluster_summary.csv", index=False, encoding="utf-8-sig")
    print("   ğŸ“Š í´ëŸ¬ìŠ¤í„° ìš”ì•½: cluster_summary.csv")

    return cluster_keywords


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    print("ğŸ“° ë‰´ìŠ¤ í´ëŸ¬ìŠ¤í„°ë§ ì‹œìŠ¤í…œì„ ì‹œì‘í•©ë‹ˆë‹¤!")
    print("=" * 50)

    df = pd.read_csv("it_news_articles_processed.csv")

    vectorizer = TfidfVectorizer(
        max_features=1000,  # ìµœëŒ€ íŠ¹ì„± ìˆ˜
        stop_words="english",  # ì˜ì–´ ë¶ˆìš©ì–´ ì œê±°
        ngram_range=(1, 2),  # 1-gramê³¼ 2-gram ì‚¬ìš©
        min_df=2,  # ìµœì†Œ ë¬¸ì„œ ë¹ˆë„
        max_df=0.8,  # ìµœëŒ€ ë¬¸ì„œ ë¹ˆë„
    )

    tfidf_matrix = vectorizer.fit_transform(df["processed_content"])
    feature_names = vectorizer.get_feature_names_out()

    print("   âœ… TF-IDF ë²¡í„°í™” ì™„ë£Œ")
    print(f"   â€¢ íŠ¹ì„± ìˆ˜: {len(feature_names)}")
    print(f"   â€¢ í–‰ë ¬ í¬ê¸°: {tfidf_matrix.shape}")

    # ìµœì  í´ëŸ¬ìŠ¤í„° ìˆ˜ ì°¾ê¸°
    if len(df) > 4:  # í´ëŸ¬ìŠ¤í„° ìˆ˜ ìµœì í™”ëŠ” ë°ì´í„°ê°€ ì¶©ë¶„í•  ë•Œë§Œ
        optimal_k, silhouette_scores, inertias = find_optimal_clusters(tfidf_matrix)
    else:
        optimal_k = min(3, len(df) - 1)
        print(f"\nâš ï¸  ë°ì´í„° ìˆ˜ê°€ ì ì–´ í´ëŸ¬ìŠ¤í„° ìˆ˜ë¥¼ {optimal_k}ê°œë¡œ ì„¤ì •í•©ë‹ˆë‹¤.")

    # í´ëŸ¬ìŠ¤í„°ë§ ìˆ˜í–‰
    print(f"\nğŸ¯ K-Means í´ëŸ¬ìŠ¤í„°ë§ ìˆ˜í–‰ ì¤‘... (k={optimal_k})")

    clustering = NewsClustering(n_clusters=optimal_k)
    labels = clustering.fit_predict(tfidf_matrix)

    # í´ëŸ¬ìŠ¤í„° ë¶„ì„
    df_with_clusters = clustering.analyze_clusters(df, feature_names, tfidf_matrix)

    # ì‹œê°í™”
    if len(df) > 2:  # ì‹œê°í™”ëŠ” ìµœì†Œ 3ê°œ ì´ìƒì˜ ë°ì´í„°ê°€ ìˆì„ ë•Œ
        clustering.visualize_clusters(tfidf_matrix, df_with_clusters)
    else:
        print("\nâš ï¸  ë°ì´í„° ìˆ˜ê°€ ì ì–´ ì‹œê°í™”ë¥¼ ìƒëµí•©ë‹ˆë‹¤.")

    # ê²°ê³¼ ì €ì¥
    cluster_keywords = save_results(df_with_clusters, feature_names, clustering)

    # ìš”ì•½ ë¦¬í¬íŠ¸
    print("\nğŸ“Š ìµœì¢… ê²°ê³¼ ìš”ì•½")
    print("=" * 30)
    print(f"â€¢ ì´ ê¸°ì‚¬ ìˆ˜: {len(df)}")
    print(f"â€¢ í´ëŸ¬ìŠ¤í„° ìˆ˜: {optimal_k}")

    if len(df) > 4:
        silhouette_avg = silhouette_score(tfidf_matrix, labels)
        print(f"â€¢ ì‹¤ë£¨ì—£ ì ìˆ˜: {silhouette_avg:.3f}")

    print("â€¢ í´ëŸ¬ìŠ¤í„°ë³„ ê¸°ì‚¬ ìˆ˜:")
    for i in range(optimal_k):
        count = sum(labels == i)
        print(f"  - í´ëŸ¬ìŠ¤í„° {i}: {count}ê°œ")

    print("\nğŸ“ ìƒì„±ëœ íŒŒì¼:")
    print("  â€¢ news_clustering_results.csv")
    print("  â€¢ cluster_keywords.json")
    print("  â€¢ cluster_summary.csv")
    if len(df) > 2:
        print("  â€¢ news_clustering_analysis.png")

    print("\nğŸ‰ ë‰´ìŠ¤ í´ëŸ¬ìŠ¤í„°ë§ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")

    return df_with_clusters, clustering, cluster_keywords


if __name__ == "__main__":
    # ì‹¤í–‰
    results = main()

    # ê²°ê³¼ ì‚¬ìš© ì˜ˆì‹œ
    df_with_clusters, clustering_model, keywords = results

    # íŠ¹ì • í´ëŸ¬ìŠ¤í„°ì˜ ê¸°ì‚¬ í™•ì¸
    print("\nğŸ” í´ëŸ¬ìŠ¤í„° 0ì˜ ê¸°ì‚¬ë“¤:")
    cluster_0_articles = df_with_clusters[df_with_clusters["cluster"] == 0]
    for idx, article in cluster_0_articles.iterrows():
        print(f"  â€¢ {article['Title']}")
