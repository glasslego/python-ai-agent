# https://newsapi.org/sources
# https://newsapi.org/docs/client-libraries/python
# flake8: noqa: E501

import time

import pandas as pd
import requests

from src.news_analysis.parse_url import parse_article_content

NEWS_API_KEY = "9268c1b56a644f2fa03cc5979fbfcb75"


def get_latest_it_news_json(num_articles=10):
    """
    NewsAPIë¥¼ ì‚¬ìš©í•˜ì—¬ ìµœì‹  IT ë‰´ìŠ¤ ê¸°ì‚¬ë¥¼ JSON í˜•íƒœë¡œ ê°€ì ¸ì˜µë‹ˆë‹¤.

    Args:
        num_articles (int): ê°€ì ¸ì˜¬ ê¸°ì‚¬ ìˆ˜ (ê¸°ë³¸ê°’: 10)

    Returns:
        dict: ê¸°ì‚¬ ì œëª©ê³¼ URLì´ ë‹´ê¸´ JSON ê°ì²´.
              ì˜¤ë¥˜ ë°œìƒ ì‹œ ë¹ˆ ë”•ì…”ë„ˆë¦¬ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    api_key = NEWS_API_KEY
    if not api_key:
        print("âŒ ì—ëŸ¬: NewsAPI í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. .env íŒŒì¼ì„ í™•ì¸í•˜ì„¸ìš”.")
        return []

    url = "https://newsapi.org/v2/top-headlines"

    # API ìš”ì²­ì— í•„ìš”í•œ íŒŒë¼ë¯¸í„° ì„¤ì •
    params = {
        "apiKey": api_key,
        "category": "technology",  # IT(ê¸°ìˆ ) ì¹´í…Œê³ ë¦¬
        "country": "us",  # ì£¼ìš” IT ë‰´ìŠ¤ê°€ ë§ì€ ë¯¸êµ­ìœ¼ë¡œ ì„¤ì • (í•œêµ­ 'kr'ë„ ê°€ëŠ¥)
    }

    try:
        response = requests.get(url, params=params)
        # HTTP ìƒíƒœ ì½”ë“œê°€ ì—ëŸ¬ë¥¼ ë‚˜íƒ€ë‚´ë©´ ìë™ìœ¼ë¡œ ì˜ˆì™¸ ë°œìƒ ì‹œí‚¤ëŠ” Response ê°ì²´ ë©”ì†Œë“œ
        response.raise_for_status()

        # ì‘ë‹µ ë°ì´í„°ë¥¼ JSON í˜•íƒœë¡œ íŒŒì‹±
        data = response.json()

        # for debugging
        print(data)

        # ê¸°ì‚¬ ëª©ë¡ ì¶”ì¶œ
        articles = data.get("articles")

        if not articles:
            print("ğŸ’¡ ìƒˆë¡œìš´ IT ë‰´ìŠ¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return []

        return articles

    except requests.exceptions.RequestException as e:
        print(f"âŒ API ìš”ì²­ ì¤‘ ì—ëŸ¬ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
        return []
    except Exception as e:
        print(f"âŒ ì•Œ ìˆ˜ ì—†ëŠ” ì—ëŸ¬ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
        return []


def get_latest_it_news_formatted(num_articles=10):
    """
    NewsAPIë¥¼ ì‚¬ìš©í•˜ì—¬ ìµœì‹  IT ë‰´ìŠ¤ ê¸°ì‚¬ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.

    Args:
        num_articles (int): ê°€ì ¸ì˜¬ ê¸°ì‚¬ ìˆ˜ (ê¸°ë³¸ê°’: 10)

    Returns:
        list: ê¸°ì‚¬ ì œëª©ê³¼ URLì´ ë‹´ê¸´ ë¬¸ìì—´ ë¦¬ìŠ¤íŠ¸.
              ì˜¤ë¥˜ ë°œìƒ ì‹œ ë¹ˆ ë¦¬ìŠ¤íŠ¸ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    articles = get_latest_it_news_json(num_articles)

    formatted_news = []
    for i, article in enumerate(articles):
        title = article.get("title")
        url = article.get("url")
        formatted_news.append(f"{i + 1}. {title}\n   - ë§í¬: {url}")

    return formatted_news


def get_latest_it_news_dataframe(num_articles=10) -> pd.DataFrame:
    articles = get_latest_it_news_json(num_articles)
    sources = []
    authors = []
    titles = []
    descriptions = []
    urls = []
    published_ats = []
    contents = []

    for article in articles:
        sources.append(article["source"]["name"])
        authors.append(article["author"])
        titles.append(article["title"])
        descriptions.append(article["description"])
        urls.append(article["url"])
        published_ats.append(article["publishedAt"])
        contents.append(article["content"])

    new_article_data = {
        "Source": sources,
        "Author": authors,
        "Title": titles,
        "Description": descriptions,
        "URL": urls,
        "Published At": published_ats,
        "Content": contents,
    }

    # pandas DataFrameìœ¼ë¡œ ë³€í™˜
    df = pd.DataFrame(new_article_data)

    print("\n DataFrame ë³€í™˜ ì™„ë£Œ!")
    print(f"DataFrame í¬ê¸°: {df.shape} (í–‰: {df.shape[0]}, ì—´: {df.shape[1]})")

    print("\n DataFrame ì •ë³´:")
    print(df.info())

    print("\n DataFrame ë¯¸ë¦¬ë³´ê¸° (ì²˜ìŒ 3ê°œ í–‰):")
    print(df.head(3))

    # url íŒŒì‹± ê²°ê³¼ë¥¼ DataFrameì— ì¶”ê°€
    df_with_content = add_parsed_content_to_dataframe(df)
    print("\n DataFrame ë¯¸ë¦¬ë³´ê¸° (ì²˜ìŒ 3ê°œ í–‰):")
    print(df_with_content.head(3))

    return df_with_content


def get_news_content(url: str):
    """
    ì£¼ì–´ì§„ URLì—ì„œ ë‰´ìŠ¤ ê¸°ì‚¬ì˜ ë‚´ìš©ì„ ê°€ì ¸ì˜µë‹ˆë‹¤.

    Args:
        url (str): ë‰´ìŠ¤ ê¸°ì‚¬ URL

    Returns:
        str: ë‰´ìŠ¤ ê¸°ì‚¬ì˜ íŒŒì‹±ëœ ë‚´ìš©ê³¼ ë©”íƒ€ ì •ë³´ê°€ í¬í•¨ëœ ë”•ì…”ë„ˆë¦¬.
    """

    try:
        parsed_data = parse_article_content(url)
        print(parsed_data)

        res = {
            "Parsed Content": parsed_data.get("content", ""),
            "Parsed Author": parsed_data.get("author", ""),
            "Meta Description": parsed_data.get("meta_description", ""),
            "Parsed Publish Date": parsed_data.get("publish_date", ""),
            "Tags": ", ".join(parsed_data.get("tags", [])),
            "Word Count": parsed_data.get("word_count", 0),
            "Reading Time (min)": parsed_data.get("reading_time", 0),
            "Parse Status": parsed_data.get("status", "not_attempted"),
        }

        return res

    except Exception as e:
        print(f"âŒ ë‰´ìŠ¤ ê¸°ì‚¬ ë‚´ìš©ì„ ê°€ì ¸ì˜¤ëŠ” ì¤‘ ì—ëŸ¬ ë°œìƒ: {e}")
        return {
            "Parsed Content": "",
            "Parsed Author": "",
            "Meta Description": "",
            "Parsed Publish Date": "",
            "Tags": "",
            "Word Count": 0,
            "Reading Time (min)": 0,
            "Parse Status": "error",
        }


def add_parsed_content_to_dataframe(df: pd.DataFrame) -> pd.DataFrame:
    """ê¸°ì¡´ DataFrameì— URL íŒŒì‹± ê²°ê³¼ ì¶”ê°€"""

    df_copy = df.copy()

    # ìƒˆ ì»¬ëŸ¼ë“¤ ì´ˆê¸°í™”
    df_copy["Parsed Content"] = ""
    df_copy["Parsed Author"] = ""
    df_copy["Meta Description"] = ""
    df_copy["Parsed Publish Date"] = ""
    df_copy["Tags"] = ""
    df_copy["Word Count"] = 0
    df_copy["Reading Time (min)"] = 0
    df_copy["Parse Status"] = "pending"

    # iterrowsë¡œ ê° URL íŒŒì‹±
    for index, row in df_copy.iterrows():
        print(f"ğŸ” [{index + 1}/{len(df_copy)}] {row['Title'][:40]}...")

        url = row["URL"]
        parsed_data = get_news_content(url)  # ê¸°ì¡´ í•¨ìˆ˜ ì‚¬ìš©

        # íŒŒì‹± ê²°ê³¼ë¥¼ DataFrameì— ì§ì ‘ í• ë‹¹
        df_copy.at[index, "Parsed Content"] = parsed_data["Parsed Content"]
        df_copy.at[index, "Parsed Author"] = parsed_data["Parsed Author"]
        df_copy.at[index, "Meta Description"] = parsed_data["Meta Description"]
        df_copy.at[index, "Parsed Publish Date"] = parsed_data["Parsed Publish Date"]
        df_copy.at[index, "Tags"] = parsed_data["Tags"]
        df_copy.at[index, "Word Count"] = parsed_data["Word Count"]
        df_copy.at[index, "Reading Time (min)"] = parsed_data["Reading Time (min)"]
        df_copy.at[index, "Parse Status"] = parsed_data["Parse Status"]

        # ì„œë²„ ë¶€í•˜ë¥¼ ìœ„í•´ì„œ 1ì´ˆ ëŒ€ê¸° (optional)
        time.sleep(1)

    return df_copy


if __name__ == "__main__":
    print("ğŸš€ ìµœì‹  IT ë‰´ìŠ¤ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤...")
    latest_it_news = get_latest_it_news_json(100)
    print(latest_it_news)

    formatted_news = get_latest_it_news_formatted(100)
    for news in formatted_news:
        print(news)

    df = get_latest_it_news_dataframe(100)
    df.to_csv("it_news_articles.csv", index=False)

    # res = get_news_content("https://nintendoeverything.com/nintendo-announces-my-mario-series-of-products-including-switch-app/")

    # latest_news = get_latest_it_news(10)
    #
    # if latest_news:
    #     print("\n--- ğŸ“° ìµœì‹  IT ë‰´ìŠ¤ Top 10---")
    #     for news_item in latest_news:
    #         print(news_item)
    #     print("--------------------------\n")
