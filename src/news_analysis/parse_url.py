# https://newsapi.org/sources
# flake8: noqa: E501
import re

import requests
from bs4 import BeautifulSoup


def parse_article_content(url, timeout=10):
    """
    BeautifulSoupì„ ì‚¬ìš©í•˜ì—¬ ë‰´ìŠ¤ ê¸°ì‚¬ URLì—ì„œ ì „ì²´ ë‚´ìš©ì„ íŒŒì‹±í•©ë‹ˆë‹¤.

    Args:
        url (str): íŒŒì‹±í•  ê¸°ì‚¬ URL
        timeout (int): ìš”ì²­ íƒ€ì„ì•„ì›ƒ (ì´ˆ)

    Returns:
        dict: íŒŒì‹±ëœ ê¸°ì‚¬ ë‚´ìš© (ì œëª©, ë³¸ë¬¸, ë©”íƒ€ë°ì´í„° ë“±)
    """

    print(f"ğŸ” URL íŒŒì‹± ì¤‘: {url}")

    # HTTP í—¤ë” ì„¤ì • (ë´‡ ì°¨ë‹¨ ë°©ì§€)
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36",
        "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8",
        "Accept-Language": "en-US,en;q=0.5",
        "Accept-Encoding": "gzip, deflate",
        "DNT": "1",
        "Connection": "keep-alive",
        "Upgrade-Insecure-Requests": "1",
    }

    try:
        response = requests.get(url, headers=headers, timeout=timeout)
        response.raise_for_status()

        # BeautifulSoupìœ¼ë¡œ HTML íŒŒì‹±
        soup = BeautifulSoup(response.content, "html.parser")

        # íŒŒì‹± ê²°ê³¼ ì €ì¥
        parsed_data = {
            "url": url,
            "status": "success",
            "title": extract_title(soup),
            "content": extract_content(soup),
            "meta_description": extract_meta_description(soup),
            "publish_date": extract_publish_date(soup),
            "author": extract_author(soup),
            "tags": extract_tags(soup),
            "word_count": 0,
            "reading_time": 0,
        }

        # ë‹¨ì–´ ìˆ˜ ë° ì½ê¸° ì‹œê°„ ê³„ì‚°
        if parsed_data["content"]:
            word_count = len(parsed_data["content"].split())
            parsed_data["word_count"] = word_count
            parsed_data["reading_time"] = max(1, word_count // 200)  # ë¶„ë‹¹ 200ë‹¨ì–´ ê¸°ì¤€

        print(
            f"íŒŒì‹± ì„±ê³µ: {parsed_data['word_count']}ë‹¨ì–´, ì˜ˆìƒ ì½ê¸°ì‹œê°„ {parsed_data['reading_time']}ë¶„"
        )

        return parsed_data

    except requests.exceptions.Timeout:
        print(f" íƒ€ì„ì•„ì›ƒ: {url}")
        return {"url": url, "status": "timeout", "error": "Request timeout"}

    except requests.exceptions.RequestException as e:
        print(f"ë„¤íŠ¸ì›Œí¬ ì˜¤ë¥˜: {e}")
        return {"url": url, "status": "network_error", "error": str(e)}

    except Exception as e:
        print(f"íŒŒì‹± ì˜¤ë¥˜: {e}")
        return {"url": url, "status": "parse_error", "error": str(e)}


def extract_title(soup):
    """HTMLì—ì„œ ì œëª© ì¶”ì¶œ"""

    # ë‹¤ì–‘í•œ ì œëª© ì„ íƒì ì‹œë„
    title_selectors = [
        "h1.entry-title",
        "h1.article-title",
        "h1.post-title",
        "h1.headline",
        "h1",
        "title",
        '[property="og:title"]',
        '[name="twitter:title"]',
    ]

    for selector in title_selectors:
        element = soup.select_one(selector)
        if element:
            title = (
                element.get_text(strip=True)
                if element.name != "meta"
                else element.get("content", "")
            )
            if title and len(title) > 5:  # ë„ˆë¬´ ì§§ì€ ì œëª© ì œì™¸
                return clean_text(title)

    return "ì œëª©ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ"


def extract_content(soup):
    """HTMLì—ì„œ ë³¸ë¬¸ ë‚´ìš© ì¶”ì¶œ"""

    # ë¶ˆí•„ìš”í•œ íƒœê·¸ë“¤ ì œê±°
    for tag in soup(
        [
            "script",
            "style",
            "nav",
            "header",
            "footer",
            "aside",
            "advertisement",
            "ads",
            "social-share",
            "comments",
        ]
    ):
        tag.decompose()

    # ë³¸ë¬¸ ì¶”ì¶œì„ ìœ„í•œ ì„ íƒìë“¤ (ì‚¬ì´íŠ¸ë³„ë¡œ ë‹¤ë¦„)
    content_selectors = [
        "article .entry-content",
        "article .post-content",
        "article .article-body",
        "article .content",
        ".story-body",
        ".article-content",
        ".post-body",
        ".entry-content",
        "article",
        ".main-content article",
        '[role="main"] article',
    ]

    content = ""

    for selector in content_selectors:
        elements = soup.select(selector)
        if elements:
            # ê°€ì¥ ê¸´ í…ìŠ¤íŠ¸ë¥¼ ê°€ì§„ ìš”ì†Œ ì„ íƒ
            for element in elements:
                text = element.get_text(separator="\n", strip=True)
                if len(text) > len(content):
                    content = text
            if content:
                break

    # ì„ íƒìë¡œ ëª» ì°¾ìœ¼ë©´ p íƒœê·¸ë“¤ë¡œ ì‹œë„
    if len(content) < 100:
        paragraphs = soup.find_all("p")
        paragraph_texts = []
        for p in paragraphs:
            text = p.get_text(strip=True)
            if len(text) > 30:  # ë„ˆë¬´ ì§§ì€ ë¬¸ë‹¨ ì œì™¸
                paragraph_texts.append(text)

        content = "\n\n".join(paragraph_texts)

    return clean_text(content) if content else "ë³¸ë¬¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ"


def extract_meta_description(soup):
    """ë©”íƒ€ ì„¤ëª… ì¶”ì¶œ"""

    meta_selectors = [
        'meta[name="description"]',
        'meta[property="og:description"]',
        'meta[name="twitter:description"]',
    ]

    for selector in meta_selectors:
        meta = soup.select_one(selector)
        if meta and meta.get("content"):
            return clean_text(meta.get("content"))

    return ""


def extract_publish_date(soup):
    """ë°œí–‰ì¼ ì¶”ì¶œ"""

    date_selectors = [
        "time[datetime]",
        '[property="article:published_time"]',
        '[name="publish_date"]',
        ".publish-date",
        ".article-date",
        ".post-date",
    ]

    for selector in date_selectors:
        element = soup.select_one(selector)
        if element:
            date_text = (
                element.get("datetime")
                or element.get("content")
                or element.get_text(strip=True)
            )
            if date_text:
                return date_text

    return ""


def extract_author(soup):
    """ì €ì ì¶”ì¶œ"""

    author_selectors = [
        '[rel="author"]',
        ".author-name",
        ".byline-author",
        '[property="article:author"]',
        ".article-author",
    ]

    for selector in author_selectors:
        element = soup.select_one(selector)
        if element:
            author = element.get("content") or element.get_text(strip=True)
            if author:
                return clean_text(author)

    return ""


def extract_tags(soup):
    """íƒœê·¸/í‚¤ì›Œë“œ ì¶”ì¶œ"""

    tags = []

    # ë©”íƒ€ í‚¤ì›Œë“œ
    keywords_meta = soup.select_one('meta[name="keywords"]')
    if keywords_meta and keywords_meta.get("content"):
        tags.extend([tag.strip() for tag in keywords_meta.get("content").split(",")])

    # ê¸°ì‚¬ íƒœê·¸ ì„¹ì…˜
    tag_selectors = [".tags a", ".article-tags a", ".post-tags a", '[rel="tag"]']

    for selector in tag_selectors:
        tag_elements = soup.select(selector)
        for tag_el in tag_elements:
            tag_text = tag_el.get_text(strip=True)
            if tag_text and tag_text not in tags:
                tags.append(tag_text)

    return tags[:10]  # ìµœëŒ€ 10ê°œë§Œ ë°˜í™˜


def clean_text(text):
    """í…ìŠ¤íŠ¸ ì •ë¦¬ (ê³µë°±, íŠ¹ìˆ˜ë¬¸ì ë“±)"""
    if not text:
        return ""

    # ì—°ì†ëœ ê³µë°±, íƒ­, ì¤„ë°”ê¿ˆ ì •ë¦¬
    text = re.sub(r"\s+", " ", text)
    text = re.sub(r"\n+", "\n", text)

    # ì•ë’¤ ê³µë°± ì œê±°
    text = text.strip()

    return text


if __name__ == "__main__":
    url = "https://nintendoeverything.com/nintendo-announces-my-mario-series-of-products-including-switch-app/"
    parsed_data = parse_article_content(url)
    print(parsed_data)
